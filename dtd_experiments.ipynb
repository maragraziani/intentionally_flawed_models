{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "configtf = tf.ConfigProto()\n",
    "configtf.gpu_options.allow_growth = True\n",
    "configtf.gpu_options.visible_device_list = \"0\"\n",
    "set_session(tf.Session(config=configtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import keras.datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from memorization_utils import *\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "import sys \n",
    "sys.path.append('../rcvs_fexps/')\n",
    "sys.path.append('../rcvs_fexps/iMIMIC-RCVs/')\n",
    "sys.path.append('../rcvs_fexps/iMIMIC-RCVs/scripts/')\n",
    "sys.path.append('../rcvs_fexps/iMIMIC-RCVs/scripts/keras_vis_rcv/')\n",
    "#from rcv_utils import *\n",
    "from mnist_utils import *\n",
    "import rcv_utils\n",
    "import PIL\n",
    "\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source='/mnt/nas2/results/IntermediateResults/Mara/probes'\n",
    "folders = os.listdir('{}/dtd/images/'.format(source))\n",
    "textures = os.listdir('{}/dtd/images/'.format(source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading split no. 1\n",
      "778 900\n",
      "(1880, 778, 900, 4)\n",
      "(1880, 299, 299, 3)\n",
      "(1880, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_split(1, source, textures)\n",
    "print x_train.shape\n",
    "print x_val.shape\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del models\n",
    "import models\n",
    "reload(models)\n",
    "from models import *\n",
    "\n",
    "#del models\n",
    "import dataset_utils\n",
    "reload(dataset_utils)\n",
    "from dataset_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "inceptionv3 = InceptionV3(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 47)           96303       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,899,087\n",
      "Trainable params: 21,864,655\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inceptionv3.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print dataset\n",
    "except:\n",
    "    train_data = (x_train, y_train)\n",
    "    val_data= (x_val, y_val) \n",
    "    test_data = (x_test, y_test)\n",
    "\n",
    "    #dataset = Dataset(train_data, val_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 218.157158852\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,1,2048,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_86/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=9620299, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_86/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'conv2d_86/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/mara/venv/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-dcd33dcf53ff>\", line 4, in <module>\n    inceptionv3 = InceptionV3(batch_size=64)\n  File \"models.py\", line 39, in __init__\n    classes=n_classes)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/applications/inception_v3.py\", line 326, in InceptionV3\n    branch1x1 = conv2d_bn(x, 320, 1, 1)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/applications/inception_v3.py\", line 81, in conv2d_bn\n    name=conv_name)(x)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 413, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/initializers.py\", line 217, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3838, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 674, in random_uniform\n    name=name)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,1,2048,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_86/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=9620299, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_86/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dcd33dcf53ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minceptionv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_corrupt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m'''REP 0 with label corruption at 0.0'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'random_fix_0.0lcp_rep0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.py\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mend_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mval_batch_no\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtot_val_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                         \u001b[0mouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_idxs_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                         \u001b[0;31m# Global Average Pooling the activations : saves space and removes pixel dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.py\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(self, inputs, layer)\u001b[0m\n\u001b[1;32m     60\u001b[0m         get_layer_output = K.function([self.model.layers[0].input],\n\u001b[1;32m     61\u001b[0m                               [self.model.get_layer(layer).output])\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2478\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2480\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2482\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,1,2048,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_86/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=9620299, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_86/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op u'conv2d_86/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/mara/venv/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/ioloop.py\", line 1064, in start\n    handler_func(fd_obj, events)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2714, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-14-dcd33dcf53ff>\", line 4, in <module>\n    inceptionv3 = InceptionV3(batch_size=64)\n  File \"models.py\", line 39, in __init__\n    classes=n_classes)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/applications/inception_v3.py\", line 326, in InceptionV3\n    branch1x1 = conv2d_bn(x, 320, 1, 1)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/applications/inception_v3.py\", line 81, in conv2d_bn\n    name=conv_name)(x)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 592, in __call__\n    self.build(input_shapes[0])\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/layers/convolutional.py\", line 138, in build\n    constraint=self.kernel_constraint)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/topology.py\", line 413, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/initializers.py\", line 217, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 3838, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/random_ops.py\", line 242, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 674, in random_uniform\n    name=name)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,1,2048,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv2d_86/random_uniform/RandomUniform = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=9620299, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_86/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "### New experiment to check that the performance at random is correct \n",
    "#  hopefully it'll be lower than the initial results we got!!!!\n",
    "K.clear_session()\n",
    "inceptionv3 = InceptionV3(batch_size=64)\n",
    "dataset = Dataset(train_data, val_data, test_data, label_corrupt_p = 0.0, random_seed=0) \n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6'])\n",
    "'''REP 0 with label corruption at 0.0'''\n",
    "inceptionv3.save('random_fix_0.0lcp_rep0', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERS\n",
      "[41 41 41 41 41 41 41 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n",
      " 26 26 12 12 12 12 12 12 12 12 12 12 12 12  5  5  5  5  5  5  5  5  5  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 35 35 35 35 35 35 35 35 35 35 35 35  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 28 28\n",
      " 28 28 28 28 28 28 28 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n",
      " 38 38 38 38 38 38 38 38 38 38 38 15 15 15 15 15 15 15 15 15 15 15 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 45 45 45 45 45 45 45 45 45 45 27 27 27\n",
      " 27 27 27 27 27 27 27 27 19 19 19 19 19 19 19 19 19 19 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11  9  9  9  9  9  9  9  9  9  9  9  9  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 32 32 32 32 32 32 32 32 32  6  6  6  6  6  6  6  6  6  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 20 20 20 20 20 20 20 20 20 20 20 37 37 37 37 37\n",
      " 37 37 37 37 37 37 37 37 37 37 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 42 42 42 42 42 42 42 42 42 42 42 42 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 31 31 31 31 31 31 31 31 31 31 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 29 29 29 29 29 29 10 10 10 10 10 10 10 10 10 10 10 10  1  1  1\n",
      "  1  1  1  1  1  1  1  1 21 21 21 21 21 21 21 21 21 21 34 34 34 34 34 34\n",
      " 34 34 34 34 34 34 34 34 34 34 34  4  4  4  4  4  4  4  3  3  3  3  3  3\n",
      "  3  3  3  3  3 14 14 14 14 14 14 14 14 44 44 44 44 44 44 44 44 44 44 44\n",
      " 46 46 46 46 46 46 46 46 46 46 25 25 25 25 25 25 25 25 25 25 25 25 30 30\n",
      " 30 30 30 30 30 30 30 30 30 30 30 30 30 17 17 17 17 17 17 17 17 17 17 17\n",
      " 17 17 17 17 17 17 16 16 16 16 16 16 16 16 16 16 16 16 13 13 13 13 13 13\n",
      " 13 13 36 36 36 36 36 36 36 36 36 36 36 36 36]\n",
      "(567,)\n",
      "[15 39 20 34 45 46  2  0 38 33  3 46 15 23 31 42 26 16 26  0  1  2 17 17\n",
      " 40 10 43 24  7 42 23 36  8 43  7 43  3  0 44 38 24 11 11 18  1 23 16 26\n",
      " 24 30  0 22 14  8 37 38 34 30 18  5 13  7 30 38  6 19 25 27 30 43 35 23\n",
      " 27 19 25  7 21 36 12 30 15 14  9  5 46 25 34 39 13 21 18 44  2 17 30 35\n",
      " 32 33  6 27 10 16 38 22 28  1 19 41 12 26 33 17 19 33 13  5 34  9 43 44\n",
      " 23 19 46 37 12 45  0 14 33  8 44 42 17 31 22 46  2 21 10  5 25 31 26 11\n",
      " 36 13  1 26 23 22 24 25 42  5 14 25 22  7 28 17 32 21  3 11  2 16 19 36\n",
      " 26 27 40 17 17 41 44 15 17 20 43 39 30 17 11  8  1  2 34 21  5 25 37 37\n",
      " 37  6 16 41 11 28  8  3 33  7 41 27 26  9 25 32  3 26 12 23 25 44 18 22\n",
      " 37 17 24 26 35 13 23 43 24 18  2 40 36 39 40 37 32 36 45 34 31  9 40 35\n",
      " 21  1 30 12 23  2 35 35 17 30 45 42 11 37  2  4 33  9 33 37 22 24 22 23\n",
      " 34 32 26 15 34 20 44 39 39  8 33  9 45 18 19 29 19  8 34 33 11 34  2 30\n",
      " 20  4 45 35 11 46 34 26 16 43 20 34 33 39 40 27 12 25 41  2 18  0 39 10\n",
      "  0 43 23 27  4 21 10 32  7 39 44 42 31 11 15 37 30 32 41  7  8 22 11 26\n",
      "  6  7 45  1 18 43 25 43  7 40 38 37  4  1 29 36 35 46 22 39  7 13  0 12\n",
      " 28 36 11 32 10  1 19 22 18 20 35 26  6  4 15 28  6 33 16 31 16 31  6 37\n",
      "  5 45 34 12 18 14 10 43 38 31 28 42 24  8  9 27 36 11 10 26 19 24 39 36\n",
      " 43  6  0 45 22 29 12 32 31  7 17 33 24 11 24 18 20 33 36  7 23 18 44  9\n",
      " 17 27 41  2 42 15  3 35  8 34 22  7 40 20  3  9 38 42 22 40 12 40  2 42\n",
      "  9 12 42  5  0 12 34 39 15 13 46  9 11 24 28 16 14 24 11  6 20 44 27 24\n",
      " 33 20 17 26 36  4 23 10 44 16 16  8 46 42 25 46 10  3 26 39  8 14 30 16\n",
      "  3  9  8 18 38 37 17 20 34 40 21 15 13 29 29 24 11  2 14 33 30  0  4 34\n",
      " 21 17 38 37 31 24  3 26 30 37 29 28  0 33  7 15  7  5 40 43  3 21 35 43\n",
      "  0  1 26 28 27 10 40 38 10 35 30 36 45  1 39]\n",
      "(567,)\n"
     ]
    }
   ],
   "source": [
    "## Adding label corruption\n",
    "# NEW VERS\n",
    "'''REP 1 : seed = 1\n",
    "[41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 41 26 26\n",
    " 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 12 12 12 12 12\n",
    " 12 \n",
    " [16 27 32 29 31 33 23 38 13  8  7 29 17 28 42 16 45 25  2 22 37 17  3 11\n",
    "  3 26 34 33 40 33 11 28 23 13 40 32 36 20 29 20 14 16 28  0 21 29 18 15\n",
    " 33  6 22  3 33 34 19 24  3 19 11 41  2  7 16 32 25 14 35  0 41 32 27  1\n",
    " REP 2 : seed = 2\n",
    "[ 1  7 35 17 19 35  7 37 19 23 41 22 30 30 45 30 25 15 13 41 14 10 35 45\n",
    "  0 34 14 29 25  6  2 41  0  2 21 27 26  8 12  1  3 40 30 22 38  8 23 38\n",
    " 26 36 30  7 20 12 10  9 25 40 18 42 25 18 43 17 17  6 40  7 19  7 19 45\n",
    " REP 3 : seed = 3\n",
    "[29  2 24 40 43 10  7 41 40 27  4  1  4 28 18  7 21 41 12  5 40 18 17 40\n",
    " 10 24 16 10 37 26  3 11 18 25 36  3 28 24  5  2  6 16  4 28 16  0 44 240\n",
    " 34 32 40 38 37 44 33 33 11 46 30 41 28 22 31 18 20 15  0 10 32 20  1 30\n",
    "'''\n",
    "dataset = Dataset(train_data, val_data, test_data, label_corrupt_p = 0.3, random_seed=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERS\n",
      "[41 41 41 41 41 41 41 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26 26\n",
      " 26 26 12 12 12 12 12 12 12 12 12 12 12 12  5  5  5  5  5  5  5  5  5  2\n",
      "  2  2  2  2  2  2  2  2  2  2  2  2  2 22 22 22 22 22 22 22 22 22 22 22\n",
      " 22 22 22 35 35 35 35 35 35 35 35 35 35 35 35  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7 43 43 43 43 43 43 43 43 43 43 43 43 43 43 43 28 28\n",
      " 28 28 28 28 28 28 28 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n",
      " 38 38 38 38 38 38 38 38 38 38 38 15 15 15 15 15 15 15 15 15 15 15 23 23\n",
      " 23 23 23 23 23 23 23 23 23 23 23 45 45 45 45 45 45 45 45 45 45 27 27 27\n",
      " 27 27 27 27 27 27 27 27 19 19 19 19 19 19 19 19 19 19 11 11 11 11 11 11\n",
      " 11 11 11 11 11 11 11 11 11 11 11  9  9  9  9  9  9  9  9  9  9  9  9  8\n",
      "  8  8  8  8  8  8  8  8  8  8  8  8 18 18 18 18 18 18 18 18 18 18 18 18\n",
      " 18 32 32 32 32 32 32 32 32 32  6  6  6  6  6  6  6  6  6  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0 20 20 20 20 20 20 20 20 20 20 20 37 37 37 37 37\n",
      " 37 37 37 37 37 37 37 37 37 37 24 24 24 24 24 24 24 24 24 24 24 24 24 24\n",
      " 24 24 42 42 42 42 42 42 42 42 42 42 42 42 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 31 31 31 31 31 31 31 31 31 31 39 39 39 39 39 39 39 39 39 39 39\n",
      " 39 39 39 29 29 29 29 29 29 10 10 10 10 10 10 10 10 10 10 10 10  1  1  1\n",
      "  1  1  1  1  1  1  1  1 21 21 21 21 21 21 21 21 21 21 34 34 34 34 34 34\n",
      " 34 34 34 34 34 34 34 34 34 34 34  4  4  4  4  4  4  4  3  3  3  3  3  3\n",
      "  3  3  3  3  3 14 14 14 14 14 14 14 14 44 44 44 44 44 44 44 44 44 44 44\n",
      " 46 46 46 46 46 46 46 46 46 46 25 25 25 25 25 25 25 25 25 25 25 25 30 30\n",
      " 30 30 30 30 30 30 30 30 30 30 30 30 30 17 17 17 17 17 17 17 17 17 17 17\n",
      " 17 17 17 17 17 17 16 16 16 16 16 16 16 16 16 16 16 16 13 13 13 13 13 13\n",
      " 13 13 36 36 36 36 36 36 36 36 36 36 36 36 36]\n",
      "(567,)\n",
      "[15 39 20 34 45 46  2  0 38 33  3 46 15 23 31 42 26 16 26  0  1  2 17 17\n",
      " 40 10 43 24  7 42 23 36  8 43  7 43  3  0 44 38 24 11 11 18  1 23 16 26\n",
      " 24 30  0 22 14  8 37 38 34 30 18  5 13  7 30 38  6 19 25 27 30 43 35 23\n",
      " 27 19 25  7 21 36 12 30 15 14  9  5 46 25 34 39 13 21 18 44  2 17 30 35\n",
      " 32 33  6 27 10 16 38 22 28  1 19 41 12 26 33 17 19 33 13  5 34  9 43 44\n",
      " 23 19 46 37 12 45  0 14 33  8 44 42 17 31 22 46  2 21 10  5 25 31 26 11\n",
      " 36 13  1 26 23 22 24 25 42  5 14 25 22  7 28 17 32 21  3 11  2 16 19 36\n",
      " 26 27 40 17 17 41 44 15 17 20 43 39 30 17 11  8  1  2 34 21  5 25 37 37\n",
      " 37  6 16 41 11 28  8  3 33  7 41 27 26  9 25 32  3 26 12 23 25 44 18 22\n",
      " 37 17 24 26 35 13 23 43 24 18  2 40 36 39 40 37 32 36 45 34 31  9 40 35\n",
      " 21  1 30 12 23  2 35 35 17 30 45 42 11 37  2  4 33  9 33 37 22 24 22 23\n",
      " 34 32 26 15 34 20 44 39 39  8 33  9 45 18 19 29 19  8 34 33 11 34  2 30\n",
      " 20  4 45 35 11 46 34 26 16 43 20 34 33 39 40 27 12 25 41  2 18  0 39 10\n",
      "  0 43 23 27  4 21 10 32  7 39 44 42 31 11 15 37 30 32 41  7  8 22 11 26\n",
      "  6  7 45  1 18 43 25 43  7 40 38 37  4  1 29 36 35 46 22 39  7 13  0 12\n",
      " 28 36 11 32 10  1 19 22 18 20 35 26  6  4 15 28  6 33 16 31 16 31  6 37\n",
      "  5 45 34 12 18 14 10 43 38 31 28 42 24  8  9 27 36 11 10 26 19 24 39 36\n",
      " 43  6  0 45 22 29 12 32 31  7 17 33 24 11 24 18 20 33 36  7 23 18 44  9\n",
      " 17 27 41  2 42 15  3 35  8 34 22  7 40 20  3  9 38 42 22 40 12 40  2 42\n",
      "  9 12 42  5  0 12 34 39 15 13 46  9 11 24 28 16 14 24 11  6 20 44 27 24\n",
      " 33 20 17 26 36  4 23 10 44 16 16  8 46 42 25 46 10  3 26 39  8 14 30 16\n",
      "  3  9  8 18 38 37 17 20 34 40 21 15 13 29 29 24 11  2 14 33 30  0  4 34\n",
      " 21 17 38 37 31 24  3 26 30 37 29 28  0 33  7 15  7  5 40 43  3 21 35 43\n",
      "  0  1 26 28 27 10 40 38 10 35 30 36 45  1 39]\n",
      "(567,)\n",
      "Train generator ready, time elapsed: 20.6401159763\n",
      "Epoch: 0, loss: 4.22684049606, acc: 0.0129310349002\n",
      "Val: 0.0199353452772\n",
      "Epoch: 1, loss: 4.9069852829, acc: 0.0226293094456\n",
      "Epoch: 2, loss: 4.1786198616, acc: 0.0177801717073\n",
      "Epoch: 3, loss: 4.13805770874, acc: 0.0269396547228\n",
      "Epoch: 4, loss: 4.11896800995, acc: 0.0242456905544\n",
      "Epoch: 5, loss: 4.40095043182, acc: 0.0226293094456\n",
      "Epoch: 6, loss: 4.17088985443, acc: 0.0264008622617\n",
      "Epoch: 7, loss: 4.07515907288, acc: 0.0269396547228\n",
      "Epoch: 8, loss: 4.0785946846, acc: 0.0296336207539\n",
      "Epoch: 9, loss: 4.1046295166, acc: 0.0258620698005\n",
      "Epoch: 10, loss: 4.25077962875, acc: 0.03125\n",
      "Val: 0.0237068962306\n",
      "Epoch: 11, loss: 4.52910280228, acc: 0.0231681037694\n",
      "Epoch: 12, loss: 4.05312919617, acc: 0.0269396547228\n",
      "Epoch: 13, loss: 4.10416030884, acc: 0.03125\n",
      "Epoch: 14, loss: 4.02935600281, acc: 0.03125\n",
      "Epoch: 15, loss: 4.02144432068, acc: 0.03125\n",
      "Epoch: 16, loss: 4.0103354454, acc: 0.0274784490466\n",
      "Epoch: 17, loss: 3.85003328323, acc: 0.0280172415078\n",
      "Epoch: 18, loss: 3.81901311874, acc: 0.0398706905544\n",
      "Epoch: 19, loss: 3.81202936172, acc: 0.036099139601\n",
      "Epoch: 20, loss: 3.81654882431, acc: 0.0350215509534\n",
      "Val: 0.0242456905544\n",
      "Epoch: 21, loss: 3.81125116348, acc: 0.0404094830155\n",
      "Epoch: 22, loss: 3.81472301483, acc: 0.0328663811088\n",
      "Epoch: 23, loss: 3.8049621582, acc: 0.0414870679379\n",
      "Epoch: 24, loss: 3.79684185982, acc: 0.042025860399\n",
      "Epoch: 25, loss: 3.80130624771, acc: 0.0404094830155\n",
      "Epoch: 26, loss: 3.79728221893, acc: 0.0447198264301\n",
      "Epoch: 27, loss: 3.78231072426, acc: 0.0404094830155\n",
      "Epoch: 28, loss: 3.77402949333, acc: 0.0457974150777\n",
      "Epoch: 29, loss: 3.75995874405, acc: 0.0522629320621\n",
      "Epoch: 30, loss: 3.7672290802, acc: 0.0474137924612\n",
      "Val: 0.0382543094456\n",
      "Epoch: 31, loss: 3.75434470177, acc: 0.0528017245233\n",
      "Epoch: 32, loss: 3.75289392471, acc: 0.0549568980932\n",
      "Epoch: 33, loss: 3.73541426659, acc: 0.0565732754767\n",
      "Epoch: 34, loss: 3.78330659866, acc: 0.0495689660311\n",
      "Epoch: 35, loss: 3.75484752655, acc: 0.0528017245233\n",
      "Epoch: 36, loss: 3.72741913795, acc: 0.0581896565855\n",
      "Epoch: 37, loss: 3.74068713188, acc: 0.0587284490466\n",
      "Epoch: 38, loss: 3.73020935059, acc: 0.0587284490466\n",
      "Epoch: 39, loss: 3.7325694561, acc: 0.0565732754767\n",
      "Epoch: 40, loss: 3.72270298004, acc: 0.0608836188912\n",
      "Val: 0.0355603434145\n",
      "Epoch: 41, loss: 3.72771501541, acc: 0.0614224150777\n",
      "Epoch: 42, loss: 3.72876310349, acc: 0.051724139601\n",
      "Epoch: 43, loss: 3.70325684547, acc: 0.0565732754767\n",
      "Epoch: 44, loss: 3.71803450584, acc: 0.0560344830155\n",
      "Epoch: 45, loss: 3.69925475121, acc: 0.0598060339689\n",
      "Epoch: 46, loss: 3.69729590416, acc: 0.0614224150777\n",
      "Epoch: 47, loss: 3.68910455704, acc: 0.0668103471398\n",
      "Epoch: 48, loss: 3.67920589447, acc: 0.0635775849223\n",
      "Epoch: 49, loss: 3.67967391014, acc: 0.0668103471398\n",
      "Epoch: 50, loss: 3.67958569527, acc: 0.0668103471398\n",
      "Val: 0.0290948282927\n",
      "Epoch: 51, loss: 3.6789162159, acc: 0.0727370679379\n",
      "Epoch: 52, loss: 3.67767119408, acc: 0.0705818980932\n",
      "Epoch: 53, loss: 3.67472147942, acc: 0.0732758641243\n",
      "Epoch: 54, loss: 3.6621670723, acc: 0.0662715509534\n",
      "Epoch: 55, loss: 3.65914940834, acc: 0.072198279202\n",
      "Epoch: 56, loss: 3.64955377579, acc: 0.0716594830155\n",
      "Epoch: 57, loss: 3.64491963387, acc: 0.068426720798\n",
      "Epoch: 58, loss: 3.652998209, acc: 0.0727370679379\n",
      "Epoch: 59, loss: 3.64595007896, acc: 0.0716594830155\n",
      "Epoch: 60, loss: 3.64121007919, acc: 0.068426720798\n",
      "Val: 0.0436422415078\n",
      "Epoch: 61, loss: 3.63193011284, acc: 0.0765086188912\n",
      "Epoch: 62, loss: 3.62788677216, acc: 0.0802801698446\n",
      "Epoch: 63, loss: 3.62410759926, acc: 0.078125\n",
      "Epoch: 64, loss: 3.60656023026, acc: 0.078125\n",
      "Epoch: 65, loss: 3.58484363556, acc: 0.084051720798\n",
      "Epoch: 66, loss: 3.59766530991, acc: 0.0775862038136\n",
      "Epoch: 67, loss: 3.58765435219, acc: 0.0797413811088\n",
      "Epoch: 68, loss: 3.58878993988, acc: 0.0818965509534\n",
      "Epoch: 69, loss: 3.58231520653, acc: 0.0818965509534\n",
      "Epoch: 70, loss: 3.58032059669, acc: 0.0889008641243\n",
      "Val: 0.0393318980932\n",
      "Epoch: 71, loss: 3.57481861115, acc: 0.0802801698446\n",
      "Epoch: 72, loss: 3.55630087852, acc: 0.0851293131709\n",
      "Epoch: 73, loss: 3.55570483208, acc: 0.0915948301554\n",
      "Epoch: 74, loss: 3.54708242416, acc: 0.0991379320621\n",
      "Epoch: 75, loss: 3.5341591835, acc: 0.0905172377825\n",
      "Epoch: 76, loss: 3.5323548317, acc: 0.099676720798\n",
      "Epoch: 77, loss: 3.52330350876, acc: 0.100754313171\n",
      "Epoch: 78, loss: 3.51419472694, acc: 0.105603449047\n",
      "Epoch: 79, loss: 3.49470281601, acc: 0.099676720798\n",
      "Epoch: 80, loss: 3.49499344826, acc: 0.100215516984\n",
      "Val: 0.0533405169845\n",
      "Epoch: 81, loss: 3.4818649292, acc: 0.106681033969\n",
      "Epoch: 82, loss: 3.47082662582, acc: 0.103448279202\n",
      "Epoch: 83, loss: 3.46832203865, acc: 0.114762932062\n",
      "Epoch: 84, loss: 3.45098829269, acc: 0.119612067938\n",
      "Epoch: 85, loss: 3.43294382095, acc: 0.116918101907\n",
      "Epoch: 86, loss: 3.41263985634, acc: 0.127155169845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87, loss: 3.41958212852, acc: 0.125\n",
      "Epoch: 88, loss: 3.41405725479, acc: 0.119612067938\n",
      "Epoch: 89, loss: 3.39634799957, acc: 0.130387932062\n",
      "Epoch: 90, loss: 3.37813663483, acc: 0.123383618891\n",
      "Val: 0.0479525849223\n",
      "Epoch: 91, loss: 3.38998651505, acc: 0.128232762218\n",
      "Epoch: 92, loss: 3.34696483612, acc: 0.136314660311\n",
      "Epoch: 93, loss: 3.3101952076, acc: 0.147090524435\n",
      "Epoch: 94, loss: 3.33262705803, acc: 0.135775864124\n",
      "Epoch: 95, loss: 3.3115644455, acc: 0.142780169845\n",
      "Epoch: 96, loss: 3.28395104408, acc: 0.143857762218\n",
      "Epoch: 97, loss: 3.28199052811, acc: 0.137931033969\n",
      "Epoch: 98, loss: 3.27277684212, acc: 0.143857762218\n",
      "Epoch: 99, loss: 3.29027104378, acc: 0.140086203814\n",
      "Epoch: 100, loss: 3.26408934593, acc: 0.150323271751\n",
      "Val: 0.051724139601\n",
      "Epoch: 101, loss: 3.2711186409, acc: 0.153556033969\n",
      "Epoch: 102, loss: 3.24578142166, acc: 0.150323271751\n",
      "Epoch: 103, loss: 3.24844837189, acc: 0.149784475565\n",
      "Epoch: 104, loss: 3.19867920876, acc: 0.158943966031\n",
      "Epoch: 105, loss: 3.22717094421, acc: 0.155711203814\n",
      "Epoch: 106, loss: 3.19433784485, acc: 0.153556033969\n",
      "Epoch: 107, loss: 3.16414237022, acc: 0.173491373658\n",
      "Epoch: 108, loss: 3.13743686676, acc: 0.175646558404\n",
      "Epoch: 109, loss: 3.12226247787, acc: 0.185344830155\n",
      "Epoch: 110, loss: 3.12791466713, acc: 0.176185339689\n",
      "Val: 0.0662715509534\n",
      "Epoch: 111, loss: 3.10374808311, acc: 0.175646558404\n",
      "Epoch: 112, loss: 3.08437156677, acc: 0.186422407627\n",
      "Epoch: 113, loss: 3.08078813553, acc: 0.190732762218\n",
      "Epoch: 114, loss: 3.04749321938, acc: 0.193426728249\n",
      "Epoch: 115, loss: 3.07947874069, acc: 0.182112067938\n",
      "Epoch: 116, loss: 3.02713513374, acc: 0.193426728249\n",
      "Epoch: 117, loss: 3.0316593647, acc: 0.196659475565\n",
      "Epoch: 118, loss: 3.02121424675, acc: 0.195581898093\n",
      "Epoch: 119, loss: 3.00274157524, acc: 0.204741373658\n",
      "Epoch: 120, loss: 3.02072191238, acc: 0.195581898093\n",
      "Val: 0.0538793094456\n",
      "Epoch: 121, loss: 2.99679207802, acc: 0.207974135876\n",
      "Epoch: 122, loss: 2.98619246483, acc: 0.200431033969\n",
      "Epoch: 123, loss: 2.96969532967, acc: 0.213362067938\n",
      "Epoch: 124, loss: 2.94925117493, acc: 0.206357762218\n",
      "Epoch: 125, loss: 2.92286157608, acc: 0.214439660311\n",
      "Epoch: 126, loss: 2.88718914986, acc: 0.233297407627\n",
      "Epoch: 127, loss: 2.89625716209, acc: 0.226293101907\n",
      "Epoch: 128, loss: 2.88038110733, acc: 0.225215524435\n",
      "Epoch: 129, loss: 2.86572766304, acc: 0.224137932062\n",
      "Epoch: 130, loss: 2.86295390129, acc: 0.232758626342\n",
      "Val: 0.0646551698446\n",
      "Epoch: 131, loss: 2.86484122276, acc: 0.222521558404\n",
      "Epoch: 132, loss: 2.77559733391, acc: 0.269396543503\n",
      "Epoch: 133, loss: 2.80114006996, acc: 0.251077592373\n",
      "Epoch: 134, loss: 2.78125810623, acc: 0.251616388559\n",
      "Epoch: 135, loss: 2.7448656559, acc: 0.270474135876\n",
      "Epoch: 136, loss: 2.76749277115, acc: 0.244612067938\n",
      "Epoch: 137, loss: 2.75202345848, acc: 0.249461203814\n",
      "Epoch: 138, loss: 2.71404790878, acc: 0.272629320621\n",
      "Epoch: 139, loss: 2.65834331512, acc: 0.275862067938\n",
      "Epoch: 140, loss: 2.67789292336, acc: 0.277478456497\n",
      "Val: 0.0732758641243\n",
      "Epoch: 141, loss: 2.66864967346, acc: 0.280711203814\n",
      "Epoch: 142, loss: 2.6484708786, acc: 0.277478456497\n",
      "Epoch: 143, loss: 2.64568114281, acc: 0.296875\n",
      "Epoch: 144, loss: 2.63796663284, acc: 0.28125\n",
      "Epoch: 145, loss: 2.5899913311, acc: 0.293103456497\n",
      "Epoch: 146, loss: 2.64006733894, acc: 0.286637932062\n",
      "Epoch: 147, loss: 2.58401918411, acc: 0.303879320621\n",
      "Epoch: 148, loss: 2.55101299286, acc: 0.303340524435\n",
      "Epoch: 149, loss: 2.50528144836, acc: 0.323814660311\n",
      "Epoch: 150, loss: 2.52940702438, acc: 0.311422407627\n",
      "Val: 0.0635775849223\n",
      "Epoch: 151, loss: 2.50125932693, acc: 0.310344815254\n",
      "Epoch: 152, loss: 2.47722530365, acc: 0.327047407627\n",
      "Epoch: 153, loss: 2.47301054001, acc: 0.330280184746\n",
      "Epoch: 154, loss: 2.41044092178, acc: 0.34644395113\n",
      "Epoch: 155, loss: 2.44384217262, acc: 0.330280184746\n",
      "Epoch: 156, loss: 2.39001274109, acc: 0.34375\n",
      "Epoch: 157, loss: 2.36387658119, acc: 0.353448271751\n",
      "Epoch: 158, loss: 2.36989808083, acc: 0.345905184746\n",
      "Epoch: 159, loss: 2.38066506386, acc: 0.342672407627\n",
      "Epoch: 160, loss: 2.35118246078, acc: 0.355603456497\n",
      "Val: 0.0765086188912\n",
      "Epoch: 161, loss: 2.3182747364, acc: 0.360991388559\n",
      "Epoch: 162, loss: 2.27056312561, acc: 0.370689660311\n",
      "Epoch: 163, loss: 2.21622776985, acc: 0.382543116808\n",
      "Epoch: 164, loss: 2.23490333557, acc: 0.383081883192\n",
      "Epoch: 165, loss: 2.17549300194, acc: 0.397629320621\n",
      "Epoch: 166, loss: 2.15779304504, acc: 0.399784475565\n",
      "Epoch: 167, loss: 2.22811460495, acc: 0.378771543503\n",
      "Epoch: 168, loss: 2.14100265503, acc: 0.405711203814\n",
      "Epoch: 169, loss: 2.16906452179, acc: 0.404633611441\n",
      "Epoch: 170, loss: 2.12813711166, acc: 0.412715524435\n",
      "Val: 0.068426720798\n",
      "Epoch: 171, loss: 2.12524151802, acc: 0.412715524435\n",
      "Epoch: 172, loss: 2.13695693016, acc: 0.404094815254\n",
      "Epoch: 173, loss: 2.09352898598, acc: 0.418103456497\n",
      "Epoch: 174, loss: 2.0776052475, acc: 0.418103456497\n",
      "Epoch: 175, loss: 2.11460065842, acc: 0.425646543503\n",
      "Epoch: 176, loss: 2.04347705841, acc: 0.438038796186\n",
      "Epoch: 177, loss: 2.08012199402, acc: 0.422413796186\n",
      "Epoch: 178, loss: 1.99871695042, acc: 0.438577592373\n",
      "Epoch: 179, loss: 2.01714348793, acc: 0.445581883192\n",
      "Epoch: 180, loss: 1.9462351799, acc: 0.470905184746\n",
      "Val: 0.0598060339689\n",
      "Epoch: 181, loss: 1.94311308861, acc: 0.457435339689\n",
      "Epoch: 182, loss: 1.94389462471, acc: 0.457974135876\n",
      "Epoch: 183, loss: 1.87323093414, acc: 0.477909475565\n",
      "Epoch: 184, loss: 1.96639704704, acc: 0.454741388559\n",
      "Epoch: 185, loss: 1.81208848953, acc: 0.511853456497\n",
      "Epoch: 186, loss: 1.81225693226, acc: 0.491918116808\n",
      "Epoch: 187, loss: 1.81651985645, acc: 0.489224135876\n",
      "Epoch: 188, loss: 1.84531748295, acc: 0.475215524435\n",
      "Epoch: 189, loss: 1.9132604599, acc: 0.467672407627\n",
      "Epoch: 190, loss: 1.77012646198, acc: 0.491379320621\n",
      "Val: 0.0716594830155\n",
      "Epoch: 191, loss: 1.78322196007, acc: 0.49730604887\n",
      "Epoch: 192, loss: 1.76043331623, acc: 0.511853456497\n",
      "Epoch: 193, loss: 1.77487134933, acc: 0.498383611441\n",
      "Epoch: 194, loss: 1.71535468102, acc: 0.515625\n",
      "Epoch: 195, loss: 1.74369633198, acc: 0.511853456497\n",
      "Epoch: 196, loss: 1.69706463814, acc: 0.524245679379\n",
      "Epoch: 197, loss: 1.68604755402, acc: 0.52855604887\n",
      "Epoch: 198, loss: 1.70080006123, acc: 0.531788766384\n",
      "Epoch: 199, loss: 1.6581094265, acc: 0.537176728249\n",
      "Epoch: 200, loss: 1.64741814137, acc: 0.542025864124\n",
      "Val: 0.0554956905544\n",
      "Epoch: 201, loss: 1.55759823322, acc: 0.564116358757\n",
      "Epoch: 202, loss: 1.57575201988, acc: 0.556573271751\n",
      "Epoch: 203, loss: 1.53332149982, acc: 0.559267222881\n",
      "Epoch: 204, loss: 1.55421614647, acc: 0.567349135876\n",
      "Epoch: 205, loss: 1.55295157433, acc: 0.58081895113\n",
      "Epoch: 206, loss: 1.57332241535, acc: 0.558189630508\n",
      "Epoch: 207, loss: 1.50155639648, acc: 0.58351290226\n",
      "Epoch: 208, loss: 1.46596348286, acc: 0.594827592373\n",
      "Epoch: 209, loss: 1.46256232262, acc: 0.579202592373\n",
      "Epoch: 210, loss: 1.447296381, acc: 0.582974135876\n",
      "Val: 0.0538793094456\n",
      "Epoch: 211, loss: 1.51872622967, acc: 0.563577592373\n",
      "Epoch: 212, loss: 1.45094072819, acc: 0.588900864124\n",
      "Epoch: 213, loss: 1.42855060101, acc: 0.594288766384\n",
      "Epoch: 214, loss: 1.40825760365, acc: 0.606142222881\n",
      "Epoch: 215, loss: 1.34672272205, acc: 0.620150864124\n",
      "Epoch: 216, loss: 1.39565825462, acc: 0.59644395113\n",
      "Epoch: 217, loss: 1.41777634621, acc: 0.591594815254\n",
      "Epoch: 218, loss: 1.31876885891, acc: 0.625538766384\n",
      "Epoch: 219, loss: 1.29172229767, acc: 0.634159505367\n",
      "Epoch: 220, loss: 1.26663947105, acc: 0.637392222881\n",
      "Val: 0.0727370679379\n",
      "Epoch: 221, loss: 1.2687766552, acc: 0.644396543503\n",
      "Epoch: 222, loss: 1.34977221489, acc: 0.615301728249\n",
      "Epoch: 223, loss: 1.25478816032, acc: 0.63793104887\n",
      "Epoch: 224, loss: 1.32272720337, acc: 0.621767222881\n",
      "Epoch: 225, loss: 1.26688492298, acc: 0.637392222881\n",
      "Epoch: 226, loss: 1.21195137501, acc: 0.654094815254\n",
      "Epoch: 227, loss: 1.13664150238, acc: 0.68480604887\n",
      "Epoch: 228, loss: 1.18442428112, acc: 0.65894395113\n",
      "Epoch: 229, loss: 1.14395189285, acc: 0.676185369492\n",
      "Epoch: 230, loss: 1.16238093376, acc: 0.664870679379\n",
      "Val: 0.0603448264301\n",
      "Epoch: 231, loss: 1.17430830002, acc: 0.660560369492\n",
      "Epoch: 232, loss: 1.10785579681, acc: 0.69019395113\n",
      "Epoch: 233, loss: 1.18443250656, acc: 0.676185369492\n",
      "Epoch: 234, loss: 1.09671902657, acc: 0.686961233616\n",
      "Epoch: 235, loss: 1.13070750237, acc: 0.67456895113\n",
      "Epoch: 236, loss: 1.04943287373, acc: 0.688038766384\n",
      "Epoch: 237, loss: 1.04105436802, acc: 0.69773709774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 238, loss: 1.10428535938, acc: 0.673491358757\n",
      "Epoch: 239, loss: 1.05763447285, acc: 0.699353456497\n",
      "Epoch: 240, loss: 1.06412577629, acc: 0.681034505367\n",
      "Val: 0.0619612075388\n",
      "Epoch: 241, loss: 1.00904166698, acc: 0.699892222881\n",
      "Epoch: 242, loss: 0.988991379738, acc: 0.696120679379\n",
      "Epoch: 243, loss: 1.03186488152, acc: 0.701508641243\n",
      "Epoch: 244, loss: 0.946721255779, acc: 0.726293087006\n",
      "Epoch: 245, loss: 0.94472771883, acc: 0.72413790226\n",
      "Epoch: 246, loss: 1.04973018169, acc: 0.704741358757\n",
      "Epoch: 247, loss: 0.930662989616, acc: 0.727909505367\n",
      "Epoch: 248, loss: 0.888588547707, acc: 0.740301728249\n",
      "Epoch: 249, loss: 0.838137209415, acc: 0.747844815254\n",
      "Epoch: 250, loss: 0.933674991131, acc: 0.729525864124\n",
      "Val: 0.0571120679379\n",
      "Epoch: 251, loss: 0.888713479042, acc: 0.737607777119\n",
      "Epoch: 252, loss: 0.826658308506, acc: 0.755926728249\n",
      "Epoch: 253, loss: 0.84598916769, acc: 0.75\n",
      "Epoch: 254, loss: 0.854865133762, acc: 0.748383641243\n",
      "Epoch: 255, loss: 0.879616320133, acc: 0.751616358757\n",
      "Epoch: 256, loss: 0.887146472931, acc: 0.740301728249\n",
      "Epoch: 257, loss: 0.85894548893, acc: 0.743534505367\n",
      "Epoch: 258, loss: 0.831113636494, acc: 0.77101290226\n",
      "Epoch: 259, loss: 0.934734404087, acc: 0.72898709774\n",
      "Epoch: 260, loss: 0.948606967926, acc: 0.724676728249\n",
      "Val: 0.0635775849223\n",
      "Epoch: 261, loss: 0.909723043442, acc: 0.733297407627\n",
      "Epoch: 262, loss: 0.780863046646, acc: 0.772629320621\n",
      "Epoch: 263, loss: 0.826099336147, acc: 0.761314630508\n",
      "Epoch: 264, loss: 0.795430898666, acc: 0.761853456497\n",
      "Epoch: 265, loss: 0.833201229572, acc: 0.751616358757\n",
      "Epoch: 266, loss: 0.714557170868, acc: 0.778017222881\n",
      "Epoch: 267, loss: 0.799848794937, acc: 0.769396543503\n",
      "Epoch: 268, loss: 0.807109296322, acc: 0.757004320621\n",
      "Epoch: 269, loss: 0.730186700821, acc: 0.779633641243\n",
      "Epoch: 270, loss: 0.769058465958, acc: 0.765625\n",
      "Val: 0.0651939660311\n",
      "Epoch: 271, loss: 0.743721485138, acc: 0.787715494633\n",
      "Epoch: 272, loss: 0.663451910019, acc: 0.79956895113\n",
      "Epoch: 273, loss: 0.691539108753, acc: 0.79148709774\n",
      "Epoch: 274, loss: 0.681771755219, acc: 0.804418087006\n",
      "Epoch: 275, loss: 0.682981133461, acc: 0.792025864124\n",
      "Epoch: 276, loss: 0.729530513287, acc: 0.785021543503\n",
      "Epoch: 277, loss: 0.743007898331, acc: 0.77101290226\n",
      "Epoch: 278, loss: 0.67716217041, acc: 0.794719815254\n",
      "Epoch: 279, loss: 0.637033581734, acc: 0.818426728249\n",
      "Epoch: 280, loss: 0.617180526257, acc: 0.814116358757\n",
      "Val: 0.0646551698446\n",
      "Epoch: 281, loss: 0.642326176167, acc: 0.810883641243\n",
      "Epoch: 282, loss: 0.6486774683, acc: 0.80711209774\n",
      "Epoch: 283, loss: 0.639788746834, acc: 0.81519395113\n",
      "Epoch: 284, loss: 0.600734353065, acc: 0.8125\n",
      "Epoch: 285, loss: 0.635583877563, acc: 0.806573271751\n",
      "Epoch: 286, loss: 0.657748758793, acc: 0.806034505367\n",
      "Epoch: 287, loss: 0.582328796387, acc: 0.831896543503\n",
      "Epoch: 288, loss: 0.611229598522, acc: 0.813038766384\n",
      "Epoch: 289, loss: 0.611776590347, acc: 0.818965494633\n",
      "Epoch: 290, loss: 0.590338110924, acc: 0.82273709774\n",
      "Val: 0.0668103471398\n",
      "Epoch: 291, loss: 0.589996218681, acc: 0.811422407627\n",
      "Epoch: 292, loss: 0.59382557869, acc: 0.835668087006\n",
      "Epoch: 293, loss: 0.557952821255, acc: 0.829202592373\n",
      "Epoch: 294, loss: 0.579957842827, acc: 0.827586233616\n",
      "Epoch: 295, loss: 0.637642085552, acc: 0.816271543503\n",
      "Epoch: 296, loss: 0.641162276268, acc: 0.820043087006\n",
      "Epoch: 297, loss: 0.634638428688, acc: 0.823275864124\n",
      "Epoch: 298, loss: 0.545110166073, acc: 0.831896543503\n",
      "Epoch: 299, loss: 0.601633489132, acc: 0.818426728249\n",
      "Epoch: 300, loss: 0.548412263393, acc: 0.834051728249\n",
      "Val: 0.0678879320621\n",
      "Epoch: 301, loss: 0.510976374149, acc: 0.853448271751\n",
      "Epoch: 302, loss: 0.500887572765, acc: 0.850215494633\n",
      "Epoch: 303, loss: 0.578036606312, acc: 0.824892222881\n",
      "Epoch: 304, loss: 0.475887656212, acc: 0.870689630508\n",
      "Epoch: 305, loss: 0.54051554203, acc: 0.83351290226\n",
      "Epoch: 306, loss: 0.521659255028, acc: 0.845366358757\n",
      "Epoch: 307, loss: 0.553443789482, acc: 0.840517222881\n",
      "Epoch: 308, loss: 0.550787627697, acc: 0.835129320621\n",
      "Epoch: 309, loss: 0.476465672255, acc: 0.859375\n",
      "Epoch: 310, loss: 0.554725885391, acc: 0.848599135876\n",
      "Val: 0.0560344830155\n",
      "Epoch: 311, loss: 0.47471678257, acc: 0.857219815254\n",
      "Epoch: 312, loss: 0.482982367277, acc: 0.848599135876\n",
      "Epoch: 313, loss: 0.485359847546, acc: 0.858297407627\n",
      "Epoch: 314, loss: 0.52091473341, acc: 0.850215494633\n",
      "Epoch: 315, loss: 0.521264851093, acc: 0.844288766384\n",
      "Epoch: 316, loss: 0.494122385979, acc: 0.855064630508\n",
      "Epoch: 317, loss: 0.453586548567, acc: 0.870689630508\n",
      "Epoch: 318, loss: 0.506905674934, acc: 0.850215494633\n",
      "Epoch: 319, loss: 0.47537907958, acc: 0.864224135876\n",
      "Epoch: 320, loss: 0.452226668596, acc: 0.862607777119\n",
      "Val: 0.0651939660311\n",
      "Epoch: 321, loss: 0.430367380381, acc: 0.867995679379\n",
      "Epoch: 322, loss: 0.534855723381, acc: 0.844288766384\n",
      "Epoch: 323, loss: 0.454660385847, acc: 0.87230604887\n",
      "Epoch: 324, loss: 0.449991613626, acc: 0.87769395113\n",
      "Epoch: 325, loss: 0.443156212568, acc: 0.859913766384\n",
      "Epoch: 326, loss: 0.480974733829, acc: 0.85398709774\n",
      "Epoch: 327, loss: 0.411462068558, acc: 0.883081912994\n",
      "Epoch: 328, loss: 0.376231223345, acc: 0.882543087006\n",
      "Epoch: 329, loss: 0.390314757824, acc: 0.877155184746\n",
      "Epoch: 330, loss: 0.405784547329, acc: 0.880926728249\n",
      "Val: 0.0554956905544\n",
      "Epoch: 331, loss: 0.399667710066, acc: 0.871228456497\n",
      "Epoch: 332, loss: 0.488050043583, acc: 0.850215494633\n",
      "Epoch: 333, loss: 0.425592035055, acc: 0.870689630508\n",
      "Epoch: 334, loss: 0.387363195419, acc: 0.88038790226\n",
      "Epoch: 335, loss: 0.413917809725, acc: 0.866379320621\n",
      "Epoch: 336, loss: 0.354833424091, acc: 0.895474135876\n",
      "Epoch: 337, loss: 0.359377890825, acc: 0.899784505367\n",
      "Epoch: 338, loss: 0.386187314987, acc: 0.886853456497\n",
      "Epoch: 339, loss: 0.446856737137, acc: 0.865301728249\n",
      "Epoch: 340, loss: 0.390263676643, acc: 0.886314630508\n",
      "Val: 0.0651939660311\n",
      "Epoch: 341, loss: 0.404971688986, acc: 0.87230604887\n",
      "Epoch: 342, loss: 0.399209022522, acc: 0.882004320621\n",
      "Epoch: 343, loss: 0.370996922255, acc: 0.891163766384\n",
      "Epoch: 344, loss: 0.342691987753, acc: 0.894935369492\n",
      "Epoch: 345, loss: 0.357775866985, acc: 0.890625\n",
      "Epoch: 346, loss: 0.446266114712, acc: 0.863146543503\n",
      "Epoch: 347, loss: 0.365362763405, acc: 0.88793104887\n",
      "Epoch: 348, loss: 0.351125389338, acc: 0.885775864124\n",
      "Epoch: 349, loss: 0.367254525423, acc: 0.893857777119\n",
      "Epoch: 350, loss: 0.350172221661, acc: 0.896551728249\n",
      "Val: 0.068426720798\n",
      "Epoch: 351, loss: 0.355598986149, acc: 0.897090494633\n",
      "Epoch: 352, loss: 0.358178317547, acc: 0.894396543503\n",
      "Epoch: 353, loss: 0.377575457096, acc: 0.881465494633\n",
      "Epoch: 354, loss: 0.35450232029, acc: 0.893857777119\n",
      "Epoch: 355, loss: 0.301129639149, acc: 0.909482777119\n",
      "Epoch: 356, loss: 0.354807823896, acc: 0.889008641243\n",
      "Epoch: 357, loss: 0.374767035246, acc: 0.884698271751\n",
      "Epoch: 358, loss: 0.343133568764, acc: 0.894396543503\n",
      "Epoch: 359, loss: 0.326265305281, acc: 0.901939630508\n",
      "Epoch: 360, loss: 0.355813235044, acc: 0.892780184746\n",
      "Val: 0.0630387961864\n",
      "Epoch: 361, loss: 0.379496604204, acc: 0.88793104887\n",
      "Epoch: 362, loss: 0.367077708244, acc: 0.894396543503\n",
      "Epoch: 363, loss: 0.351918190718, acc: 0.892780184746\n",
      "Epoch: 364, loss: 0.41317614913, acc: 0.878771543503\n",
      "Epoch: 365, loss: 0.287319630384, acc: 0.913793087006\n",
      "Epoch: 366, loss: 0.301019459963, acc: 0.90894395113\n",
      "Epoch: 367, loss: 0.356265038252, acc: 0.897629320621\n",
      "Epoch: 368, loss: 0.314370065928, acc: 0.910560369492\n",
      "Epoch: 369, loss: 0.310609519482, acc: 0.905172407627\n",
      "Epoch: 370, loss: 0.34274315834, acc: 0.893857777119\n",
      "Val: 0.0732758641243\n",
      "Epoch: 371, loss: 0.320058494806, acc: 0.902478456497\n",
      "Epoch: 372, loss: 0.326118052006, acc: 0.905711233616\n",
      "Epoch: 373, loss: 0.301297932863, acc: 0.906788766384\n",
      "Epoch: 374, loss: 0.314471453428, acc: 0.90355604887\n",
      "Epoch: 375, loss: 0.317875564098, acc: 0.913793087006\n",
      "Epoch: 376, loss: 0.308985322714, acc: 0.90355604887\n",
      "Epoch: 377, loss: 0.298657655716, acc: 0.91163790226\n",
      "Epoch: 378, loss: 0.290104210377, acc: 0.912715494633\n",
      "Epoch: 379, loss: 0.319387108088, acc: 0.900323271751\n",
      "Epoch: 380, loss: 0.28275641799, acc: 0.914331912994\n",
      "Val: 0.0678879320621\n",
      "Epoch: 381, loss: 0.256555050611, acc: 0.91918104887\n",
      "Epoch: 382, loss: 0.288603782654, acc: 0.918103456497\n",
      "Epoch: 383, loss: 0.317695617676, acc: 0.908405184746\n",
      "Epoch: 384, loss: 0.241948738694, acc: 0.92456895113\n",
      "Epoch: 385, loss: 0.253607213497, acc: 0.92456895113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 386, loss: 0.268183141947, acc: 0.921875\n",
      "Epoch: 387, loss: 0.279777407646, acc: 0.914870679379\n",
      "Epoch: 388, loss: 0.221298515797, acc: 0.931573271751\n",
      "Epoch: 389, loss: 0.288922488689, acc: 0.917025864124\n",
      "Epoch: 390, loss: 0.266585558653, acc: 0.922952592373\n",
      "Val: 0.0625\n",
      "Epoch: 391, loss: 0.2779687047, acc: 0.91648709774\n",
      "Epoch: 392, loss: 0.263970404863, acc: 0.92456895113\n",
      "Epoch: 393, loss: 0.274520754814, acc: 0.915948271751\n",
      "Epoch: 394, loss: 0.254317164421, acc: 0.925107777119\n",
      "Epoch: 395, loss: 0.308562248945, acc: 0.90894395113\n",
      "Epoch: 396, loss: 0.289128810167, acc: 0.915409505367\n",
      "Epoch: 397, loss: 0.262554943562, acc: 0.920258641243\n",
      "Epoch: 398, loss: 0.272805631161, acc: 0.913254320621\n",
      "Epoch: 399, loss: 0.274929314852, acc: 0.915409505367\n",
      "Epoch: 400, loss: 0.326645106077, acc: 0.899784505367\n",
      "Val: 0.0711206868291\n",
      "Epoch: 401, loss: 0.266286849976, acc: 0.917025864124\n",
      "Epoch: 402, loss: 0.245829924941, acc: 0.922952592373\n",
      "Epoch: 403, loss: 0.246857762337, acc: 0.934267222881\n",
      "Epoch: 404, loss: 0.24962516129, acc: 0.925646543503\n",
      "Epoch: 405, loss: 0.222667723894, acc: 0.929418087006\n",
      "Epoch: 406, loss: 0.240122959018, acc: 0.92456895113\n",
      "Epoch: 407, loss: 0.255408704281, acc: 0.923491358757\n",
      "Epoch: 408, loss: 0.274482309818, acc: 0.91648709774\n",
      "Epoch: 409, loss: 0.312821954489, acc: 0.908405184746\n",
      "Epoch: 410, loss: 0.265494912863, acc: 0.91648709774\n",
      "Val: 0.0619612075388\n",
      "Epoch: 411, loss: 0.253589481115, acc: 0.92456895113\n",
      "Epoch: 412, loss: 0.254488021135, acc: 0.926185369492\n",
      "Epoch: 413, loss: 0.237017124891, acc: 0.929956912994\n",
      "Epoch: 414, loss: 0.255507916212, acc: 0.918642222881\n",
      "Epoch: 415, loss: 0.243718326092, acc: 0.921875\n",
      "Epoch: 416, loss: 0.233873620629, acc: 0.928340494633\n",
      "Epoch: 417, loss: 0.188640058041, acc: 0.943426728249\n",
      "Epoch: 418, loss: 0.296312987804, acc: 0.912715494633\n",
      "Epoch: 419, loss: 0.25381308794, acc: 0.927801728249\n",
      "Epoch: 420, loss: 0.262500703335, acc: 0.91918104887\n",
      "Val: 0.0587284490466\n",
      "Epoch: 421, loss: 0.214402377605, acc: 0.930495679379\n",
      "Epoch: 422, loss: 0.252438515425, acc: 0.926185369492\n",
      "Epoch: 423, loss: 0.23343898356, acc: 0.929418087006\n",
      "Epoch: 424, loss: 0.2646933496, acc: 0.91648709774\n",
      "Epoch: 425, loss: 0.220520034432, acc: 0.927801728249\n",
      "Epoch: 426, loss: 0.234365850687, acc: 0.926185369492\n",
      "Epoch: 427, loss: 0.221677467227, acc: 0.931034505367\n",
      "Epoch: 428, loss: 0.264851242304, acc: 0.921875\n",
      "Epoch: 429, loss: 0.266214191914, acc: 0.922952592373\n",
      "Epoch: 430, loss: 0.244984805584, acc: 0.924030184746\n",
      "Val: 0.0592672415078\n",
      "Epoch: 431, loss: 0.207206711173, acc: 0.93211209774\n",
      "Epoch: 432, loss: 0.198008686304, acc: 0.9375\n",
      "Epoch: 433, loss: 0.282328039408, acc: 0.908405184746\n",
      "Epoch: 434, loss: 0.213133752346, acc: 0.93480604887\n",
      "Epoch: 435, loss: 0.178993448615, acc: 0.94288790226\n",
      "Epoch: 436, loss: 0.214819937944, acc: 0.931034505367\n",
      "Epoch: 437, loss: 0.272921800613, acc: 0.922413766384\n",
      "Epoch: 438, loss: 0.243916600943, acc: 0.921875\n",
      "Epoch: 439, loss: 0.268401473761, acc: 0.924030184746\n",
      "Epoch: 440, loss: 0.253163903952, acc: 0.926185369492\n",
      "Val: 0.0765086188912\n",
      "Epoch: 441, loss: 0.253655701876, acc: 0.930495679379\n",
      "Epoch: 442, loss: 0.247126355767, acc: 0.930495679379\n",
      "Epoch: 443, loss: 0.211003035307, acc: 0.935883641243\n",
      "Epoch: 444, loss: 0.18371912837, acc: 0.941271543503\n",
      "Epoch: 445, loss: 0.224131926894, acc: 0.930495679379\n",
      "Epoch: 446, loss: 0.218916296959, acc: 0.933189630508\n",
      "Epoch: 447, loss: 0.238727256656, acc: 0.931034505367\n",
      "Epoch: 448, loss: 0.219734057784, acc: 0.93211209774\n",
      "Epoch: 449, loss: 0.17971894145, acc: 0.947198271751\n",
      "Epoch: 450, loss: 0.172313675284, acc: 0.949353456497\n",
      "Val: 0.0625\n",
      "Epoch: 451, loss: 0.201523691416, acc: 0.938038766384\n",
      "Epoch: 452, loss: 0.263393819332, acc: 0.933189630508\n",
      "Epoch: 453, loss: 0.239518269897, acc: 0.926724135876\n",
      "Epoch: 454, loss: 0.221424937248, acc: 0.932650864124\n",
      "Epoch: 455, loss: 0.212764650583, acc: 0.934267222881\n",
      "Epoch: 456, loss: 0.200575441122, acc: 0.940732777119\n",
      "Epoch: 457, loss: 0.196709424257, acc: 0.94019395113\n",
      "Epoch: 458, loss: 0.209354534745, acc: 0.938577592373\n",
      "Epoch: 459, loss: 0.236628174782, acc: 0.933189630508\n",
      "Epoch: 460, loss: 0.191889330745, acc: 0.933189630508\n",
      "Val: 0.0738146528602\n",
      "Epoch: 461, loss: 0.220998287201, acc: 0.93480604887\n",
      "Epoch: 462, loss: 0.186924815178, acc: 0.943965494633\n",
      "Epoch: 463, loss: 0.184792309999, acc: 0.942349135876\n",
      "Epoch: 464, loss: 0.212491393089, acc: 0.934267222881\n",
      "Epoch: 465, loss: 0.209300383925, acc: 0.9375\n",
      "Epoch: 466, loss: 0.204322680831, acc: 0.9375\n",
      "Epoch: 467, loss: 0.178410977125, acc: 0.939116358757\n",
      "Epoch: 468, loss: 0.220168873668, acc: 0.935883641243\n",
      "Epoch: 469, loss: 0.215625613928, acc: 0.933189630508\n",
      "Epoch: 470, loss: 0.183512747288, acc: 0.94019395113\n",
      "Val: 0.0625\n",
      "Epoch: 471, loss: 0.204033121467, acc: 0.938577592373\n",
      "Epoch: 472, loss: 0.210413992405, acc: 0.938577592373\n",
      "Epoch: 473, loss: 0.159256130457, acc: 0.951508641243\n",
      "Epoch: 474, loss: 0.153145983815, acc: 0.948814630508\n",
      "Epoch: 475, loss: 0.188585788012, acc: 0.94288790226\n",
      "Epoch: 476, loss: 0.244586035609, acc: 0.930495679379\n",
      "Epoch: 477, loss: 0.206036657095, acc: 0.938577592373\n",
      "Epoch: 478, loss: 0.179707273841, acc: 0.942349135876\n",
      "Epoch: 479, loss: 0.216752365232, acc: 0.929956912994\n",
      "Epoch: 480, loss: 0.191834911704, acc: 0.945581912994\n",
      "Val: 0.0614224150777\n",
      "Epoch: 481, loss: 0.178357407451, acc: 0.949353456497\n",
      "Epoch: 482, loss: 0.193928182125, acc: 0.943965494633\n",
      "Epoch: 483, loss: 0.165253296494, acc: 0.955280184746\n",
      "Epoch: 484, loss: 0.152868330479, acc: 0.953125\n",
      "Epoch: 485, loss: 0.186630934477, acc: 0.939116358757\n",
      "Epoch: 486, loss: 0.190454781055, acc: 0.940732777119\n",
      "Epoch: 487, loss: 0.192581176758, acc: 0.939116358757\n",
      "Epoch: 488, loss: 0.171338260174, acc: 0.945043087006\n",
      "Epoch: 489, loss: 0.171232268214, acc: 0.946659505367\n",
      "Epoch: 490, loss: 0.186829268932, acc: 0.944504320621\n",
      "Val: 0.0738146528602\n",
      "Epoch: 491, loss: 0.184839934111, acc: 0.943426728249\n",
      "Epoch: 492, loss: 0.20091354847, acc: 0.931034505367\n",
      "Epoch: 493, loss: 0.2203361094, acc: 0.942349135876\n",
      "Epoch: 494, loss: 0.166893869638, acc: 0.949892222881\n",
      "Epoch: 495, loss: 0.185982882977, acc: 0.944504320621\n",
      "Epoch: 496, loss: 0.277763545513, acc: 0.922413766384\n",
      "Epoch: 497, loss: 0.173423469067, acc: 0.948814630508\n",
      "Epoch: 498, loss: 0.183221146464, acc: 0.945581912994\n",
      "Epoch: 499, loss: 0.163034707308, acc: 0.950969815254\n",
      "Epoch: 500, loss: 0.166963264346, acc: 0.950969815254\n",
      "Val: 0.072198279202\n",
      "Epoch: 501, loss: 0.172780349851, acc: 0.944504320621\n",
      "Epoch: 502, loss: 0.158008769155, acc: 0.956896543503\n",
      "Epoch: 503, loss: 0.145888313651, acc: 0.95851290226\n",
      "Epoch: 504, loss: 0.172046810389, acc: 0.944504320621\n",
      "Epoch: 505, loss: 0.214501187205, acc: 0.94019395113\n",
      "Epoch: 506, loss: 0.161205098033, acc: 0.947198271751\n",
      "Epoch: 507, loss: 0.155910655856, acc: 0.950969815254\n",
      "Epoch: 508, loss: 0.174350082874, acc: 0.95043104887\n",
      "Epoch: 509, loss: 0.161798343062, acc: 0.953125\n",
      "Epoch: 510, loss: 0.16283544898, acc: 0.947198271751\n",
      "Val: 0.0754310339689\n",
      "Epoch: 511, loss: 0.161995470524, acc: 0.953125\n",
      "Epoch: 512, loss: 0.14237973094, acc: 0.959051728249\n",
      "Epoch: 513, loss: 0.164473637938, acc: 0.949353456497\n",
      "Epoch: 514, loss: 0.164518997073, acc: 0.950969815254\n",
      "Epoch: 515, loss: 0.142618611455, acc: 0.959051728249\n",
      "Epoch: 516, loss: 0.181433096528, acc: 0.947198271751\n",
      "Epoch: 517, loss: 0.195896685123, acc: 0.941810369492\n",
      "Epoch: 518, loss: 0.1494166255, acc: 0.954202592373\n",
      "Epoch: 519, loss: 0.211723640561, acc: 0.94019395113\n",
      "Epoch: 520, loss: 0.157315790653, acc: 0.951508641243\n",
      "Val: 0.0668103471398\n",
      "Epoch: 521, loss: 0.162084266543, acc: 0.949892222881\n",
      "Epoch: 522, loss: 0.152792751789, acc: 0.954202592373\n",
      "Epoch: 523, loss: 0.118920579553, acc: 0.960129320621\n",
      "Epoch: 524, loss: 0.139544993639, acc: 0.952047407627\n",
      "Epoch: 525, loss: 0.147664904594, acc: 0.955280184746\n",
      "Epoch: 526, loss: 0.120880030096, acc: 0.956357777119\n",
      "Epoch: 527, loss: 0.159171357751, acc: 0.95043104887\n",
      "Epoch: 528, loss: 0.165076360106, acc: 0.942349135876\n",
      "Epoch: 529, loss: 0.147658407688, acc: 0.953125\n",
      "Epoch: 530, loss: 0.156704887748, acc: 0.953125\n",
      "Val: 0.0716594830155\n",
      "Epoch: 531, loss: 0.143804192543, acc: 0.95581895113\n",
      "Epoch: 532, loss: 0.138214498758, acc: 0.956896543503\n",
      "Epoch: 533, loss: 0.140802696347, acc: 0.956357777119\n",
      "Epoch: 534, loss: 0.144214615226, acc: 0.954202592373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 535, loss: 0.133504524827, acc: 0.957435369492\n",
      "Epoch: 536, loss: 0.190077573061, acc: 0.953663766384\n",
      "Epoch: 537, loss: 0.134833842516, acc: 0.96336209774\n",
      "Epoch: 538, loss: 0.13228276372, acc: 0.95851290226\n",
      "Epoch: 539, loss: 0.140276089311, acc: 0.957435369492\n",
      "Epoch: 540, loss: 0.150265157223, acc: 0.952047407627\n",
      "Val: 0.068426720798\n",
      "Epoch: 541, loss: 0.127257362008, acc: 0.956896543503\n",
      "Epoch: 542, loss: 0.14311209321, acc: 0.960129320621\n",
      "Epoch: 543, loss: 0.164721205831, acc: 0.952047407627\n",
      "Epoch: 544, loss: 0.147948682308, acc: 0.955280184746\n",
      "Epoch: 545, loss: 0.146632164717, acc: 0.962284505367\n",
      "Epoch: 546, loss: 0.149263694882, acc: 0.954741358757\n",
      "Epoch: 547, loss: 0.141085192561, acc: 0.953663766384\n",
      "Epoch: 548, loss: 0.13353869319, acc: 0.959590494633\n",
      "Epoch: 549, loss: 0.161786586046, acc: 0.954741358757\n",
      "Epoch: 550, loss: 0.166468262672, acc: 0.951508641243\n",
      "Val: 0.0797413811088\n",
      "Epoch: 551, loss: 0.155513375998, acc: 0.951508641243\n",
      "Epoch: 552, loss: 0.140603795648, acc: 0.956896543503\n",
      "Epoch: 553, loss: 0.186919361353, acc: 0.949353456497\n",
      "Epoch: 554, loss: 0.154642790556, acc: 0.948814630508\n",
      "Epoch: 555, loss: 0.136693105102, acc: 0.961745679379\n",
      "Epoch: 556, loss: 0.173073247075, acc: 0.956357777119\n",
      "Epoch: 557, loss: 0.154530838132, acc: 0.957974135876\n",
      "Epoch: 558, loss: 0.138754010201, acc: 0.957974135876\n",
      "Epoch: 559, loss: 0.147164136171, acc: 0.955280184746\n",
      "Epoch: 560, loss: 0.101330675185, acc: 0.965517222881\n",
      "Val: 0.0635775849223\n",
      "Epoch: 561, loss: 0.117094680667, acc: 0.960668087006\n",
      "Epoch: 562, loss: 0.135993152857, acc: 0.956357777119\n",
      "Epoch: 563, loss: 0.144223377109, acc: 0.957435369492\n",
      "Epoch: 564, loss: 0.137817010283, acc: 0.964439630508\n",
      "Epoch: 565, loss: 0.136417329311, acc: 0.960129320621\n",
      "Epoch: 566, loss: 0.142539471388, acc: 0.952047407627\n",
      "Epoch: 567, loss: 0.147325828671, acc: 0.957435369492\n",
      "Epoch: 568, loss: 0.179178997874, acc: 0.946659505367\n",
      "Epoch: 569, loss: 0.136510848999, acc: 0.957974135876\n",
      "Epoch: 570, loss: 0.104570791125, acc: 0.966594815254\n",
      "Val: 0.0646551698446\n",
      "Epoch: 571, loss: 0.112740799785, acc: 0.96336209774\n",
      "Epoch: 572, loss: 0.155420899391, acc: 0.954202592373\n",
      "Epoch: 573, loss: 0.159210503101, acc: 0.95043104887\n",
      "Epoch: 574, loss: 0.100348614156, acc: 0.967672407627\n",
      "Epoch: 575, loss: 0.146248951554, acc: 0.956896543503\n",
      "Epoch: 576, loss: 0.156443074346, acc: 0.956357777119\n",
      "Epoch: 577, loss: 0.172418639064, acc: 0.952586233616\n",
      "Epoch: 578, loss: 0.109532818198, acc: 0.966594815254\n",
      "Epoch: 579, loss: 0.109030023217, acc: 0.96605604887\n",
      "Epoch: 580, loss: 0.133477464318, acc: 0.959590494633\n",
      "Val: 0.072198279202\n",
      "Epoch: 581, loss: 0.193465858698, acc: 0.945581912994\n",
      "Epoch: 582, loss: 0.126549690962, acc: 0.964978456497\n",
      "Epoch: 583, loss: 0.119246304035, acc: 0.96605604887\n",
      "Epoch: 584, loss: 0.131264224648, acc: 0.960668087006\n",
      "Epoch: 585, loss: 0.126573234797, acc: 0.959051728249\n",
      "Epoch: 586, loss: 0.117850765586, acc: 0.959051728249\n",
      "Epoch: 587, loss: 0.114010237157, acc: 0.96336209774\n",
      "Epoch: 588, loss: 0.11918271333, acc: 0.964439630508\n",
      "Epoch: 589, loss: 0.121612481773, acc: 0.959590494633\n",
      "Epoch: 590, loss: 0.128767490387, acc: 0.962284505367\n",
      "Val: 0.0678879320621\n",
      "Epoch: 591, loss: 0.117522589862, acc: 0.96875\n",
      "Epoch: 592, loss: 0.131687581539, acc: 0.961206912994\n",
      "Epoch: 593, loss: 0.120037771761, acc: 0.962284505367\n",
      "Epoch: 594, loss: 0.128275603056, acc: 0.961745679379\n",
      "Epoch: 595, loss: 0.142703086138, acc: 0.957435369492\n",
      "Epoch: 596, loss: 0.106919020414, acc: 0.967133641243\n",
      "ERR. something not working\n",
      "(778, 900, 4)\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_row' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1e1f0ef472c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minceptionv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_corrupt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m'''REP 0 with label corruption at 0.3'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.3lcp_rep0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.pyc\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             for x_batch, y_batch in datagen.flow(x_train[shuffle_idxs_train], \n\u001b[1;32m    213\u001b[0m                                                  \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_idxs_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                                                  batch_size=batch_size):\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m             \u001b[0;31m#removing standardize cause I already did it in my code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;31m#x = self.image_data_generator.standardize(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36mrandom_transform\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m#left_crop_size = (patch_size + 1) / 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mmin_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mmax_row\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m             \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m#print(\"min_row: {}, max_row: {}\".format(min_row, max_row))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'max_row' referenced before assignment"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "inceptionv3 = InceptionV3(batch_size=64)\n",
    "dataset = Dataset(train_data, val_data, test_data, label_corrupt_p = 0.3, random_seed=0) \n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "'''REP 0 with label corruption at 0.3'''\n",
    "inceptionv3.save('0.3lcp_rep0', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "History not saved\n"
     ]
    }
   ],
   "source": [
    "inceptionv3.save('0.3lcp_rep0', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW VERS\n",
      "[41 41 41 ... 36 36 36]\n",
      "(1476,)\n",
      "[15 32  1 ... 14 15 16]\n",
      "(1476,)\n",
      "Train generator ready, time elapsed: 17.996776104\n",
      "Epoch: 0, loss: 4.31330776215, acc: 0.015625\n",
      "Val: 0.0210129301995\n",
      "Epoch: 1, loss: 4.1668844223, acc: 0.016702586785\n",
      "Epoch: 2, loss: 4.11418962479, acc: 0.0193965509534\n",
      "Epoch: 3, loss: 4.26196575165, acc: 0.0183189660311\n",
      "Epoch: 4, loss: 4.12186288834, acc: 0.015625\n",
      "Epoch: 5, loss: 4.10747671127, acc: 0.0188577584922\n",
      "Epoch: 6, loss: 4.29759311676, acc: 0.0231681037694\n",
      "Epoch: 7, loss: 4.20042514801, acc: 0.0193965509534\n",
      "Epoch: 8, loss: 4.09047794342, acc: 0.0183189660311\n",
      "Epoch: 9, loss: 4.05858659744, acc: 0.0177801717073\n",
      "Epoch: 10, loss: 4.06033468246, acc: 0.0161637924612\n",
      "Val: 0.0188577584922\n",
      "Epoch: 11, loss: 4.04693984985, acc: 0.0188577584922\n",
      "Epoch: 12, loss: 4.07635211945, acc: 0.0204741377383\n",
      "Epoch: 13, loss: 4.08995723724, acc: 0.0193965509534\n",
      "Epoch: 14, loss: 4.06286430359, acc: 0.0129310349002\n",
      "Epoch: 15, loss: 4.05800485611, acc: 0.0188577584922\n",
      "Epoch: 16, loss: 4.064204216, acc: 0.016702586785\n",
      "Epoch: 17, loss: 4.08325242996, acc: 0.0145474141464\n",
      "Epoch: 18, loss: 4.12670898438, acc: 0.0188577584922\n",
      "Epoch: 19, loss: 4.08723831177, acc: 0.0210129301995\n",
      "Epoch: 20, loss: 3.89142751694, acc: 0.0172413792461\n",
      "Val: 0.0134698273614\n",
      "Epoch: 21, loss: 3.85611796379, acc: 0.0215517245233\n",
      "Epoch: 22, loss: 3.85204076767, acc: 0.0204741377383\n",
      "Epoch: 23, loss: 3.84632945061, acc: 0.0215517245233\n",
      "Epoch: 24, loss: 3.84498333931, acc: 0.0193965509534\n",
      "Epoch: 25, loss: 3.84356284142, acc: 0.0264008622617\n",
      "Epoch: 26, loss: 3.83843421936, acc: 0.0258620698005\n",
      "Epoch: 27, loss: 3.84382128716, acc: 0.0290948282927\n",
      "Epoch: 28, loss: 3.83999156952, acc: 0.0285560339689\n",
      "Epoch: 29, loss: 3.83670496941, acc: 0.0264008622617\n",
      "Epoch: 30, loss: 3.8378071785, acc: 0.030172413215\n",
      "Val: 0.0247844830155\n",
      "Epoch: 31, loss: 3.83481550217, acc: 0.0285560339689\n",
      "Epoch: 32, loss: 3.83378767967, acc: 0.0323275849223\n",
      "Epoch: 33, loss: 3.83097910881, acc: 0.0307112075388\n",
      "Epoch: 34, loss: 3.82334470749, acc: 0.0264008622617\n",
      "Epoch: 35, loss: 3.82133698463, acc: 0.0344827584922\n",
      "Epoch: 36, loss: 3.82605552673, acc: 0.0269396547228\n",
      "Epoch: 37, loss: 3.8195130825, acc: 0.03125\n",
      "Epoch: 38, loss: 3.8191473484, acc: 0.0290948282927\n",
      "Epoch: 39, loss: 3.81582665443, acc: 0.03125\n",
      "Epoch: 40, loss: 3.82415604591, acc: 0.0317887924612\n",
      "Val: 0.0280172415078\n",
      "Epoch: 41, loss: 3.82049250603, acc: 0.0350215509534\n",
      "Epoch: 42, loss: 3.81591653824, acc: 0.036099139601\n",
      "Epoch: 43, loss: 3.81544184685, acc: 0.0339439660311\n",
      "Epoch: 44, loss: 3.81778812408, acc: 0.0355603434145\n",
      "Epoch: 45, loss: 3.81351232529, acc: 0.0393318980932\n",
      "Epoch: 46, loss: 3.81707215309, acc: 0.0344827584922\n",
      "Epoch: 47, loss: 3.81221842766, acc: 0.0334051735699\n",
      "Epoch: 48, loss: 3.82172465324, acc: 0.0334051735699\n",
      "Epoch: 49, loss: 3.80813789368, acc: 0.0371767245233\n",
      "Epoch: 50, loss: 3.81327867508, acc: 0.036099139601\n",
      "Val: 0.0204741377383\n",
      "Epoch: 51, loss: 3.80849695206, acc: 0.0377155169845\n",
      "Epoch: 52, loss: 3.80301332474, acc: 0.0371767245233\n",
      "Epoch: 53, loss: 3.80863666534, acc: 0.0350215509534\n",
      "Epoch: 54, loss: 3.79718375206, acc: 0.0366379320621\n",
      "Epoch: 55, loss: 3.7952401638, acc: 0.0371767245233\n",
      "Epoch: 56, loss: 3.78963279724, acc: 0.0382543094456\n",
      "Epoch: 57, loss: 3.79535388947, acc: 0.0382543094456\n",
      "Epoch: 58, loss: 3.78764939308, acc: 0.036099139601\n",
      "Epoch: 59, loss: 3.78487801552, acc: 0.0339439660311\n",
      "Epoch: 60, loss: 3.77667450905, acc: 0.0393318980932\n",
      "Val: 0.03125\n",
      "Epoch: 61, loss: 3.78752851486, acc: 0.0350215509534\n",
      "Epoch: 62, loss: 3.77566194534, acc: 0.0436422415078\n",
      "Epoch: 63, loss: 3.77313542366, acc: 0.0447198264301\n",
      "Epoch: 64, loss: 3.75835490227, acc: 0.0463362075388\n",
      "Epoch: 65, loss: 3.77451467514, acc: 0.0414870679379\n",
      "Epoch: 66, loss: 3.75750756264, acc: 0.0490301735699\n",
      "Epoch: 67, loss: 3.77170801163, acc: 0.0371767245233\n",
      "Epoch: 68, loss: 3.75451231003, acc: 0.0457974150777\n",
      "Epoch: 69, loss: 3.75029468536, acc: 0.0431034490466\n",
      "Epoch: 70, loss: 3.74936699867, acc: 0.0425646565855\n",
      "Val: 0.0366379320621\n",
      "Epoch: 71, loss: 3.75573515892, acc: 0.0431034490466\n",
      "Epoch: 72, loss: 3.75051212311, acc: 0.0479525849223\n",
      "Epoch: 73, loss: 3.73880624771, acc: 0.0414870679379\n",
      "Epoch: 74, loss: 3.74827003479, acc: 0.042025860399\n",
      "Epoch: 75, loss: 3.73648786545, acc: 0.0452586188912\n",
      "Epoch: 76, loss: 3.73725700378, acc: 0.0414870679379\n",
      "Epoch: 77, loss: 3.73085832596, acc: 0.0474137924612\n",
      "Epoch: 78, loss: 3.74599957466, acc: 0.0457974150777\n",
      "Epoch: 79, loss: 3.74378943443, acc: 0.0474137924612\n",
      "Epoch: 80, loss: 3.73348045349, acc: 0.0452586188912\n",
      "Val: 0.0264008622617\n",
      "Epoch: 81, loss: 3.72862863541, acc: 0.0511853434145\n",
      "Epoch: 82, loss: 3.72251462936, acc: 0.0447198264301\n",
      "Epoch: 83, loss: 3.71841835976, acc: 0.0495689660311\n",
      "Epoch: 84, loss: 3.7046482563, acc: 0.0457974150777\n",
      "Epoch: 85, loss: 3.70135331154, acc: 0.0608836188912\n",
      "Epoch: 86, loss: 3.70746040344, acc: 0.0506465509534\n",
      "Epoch: 87, loss: 3.6953754425, acc: 0.0544181019068\n",
      "Epoch: 88, loss: 3.69373321533, acc: 0.0511853434145\n",
      "Epoch: 89, loss: 3.6858985424, acc: 0.0641163811088\n",
      "Epoch: 90, loss: 3.69337058067, acc: 0.0592672415078\n",
      "Val: 0.0231681037694\n",
      "Epoch: 91, loss: 3.67646765709, acc: 0.0608836188912\n",
      "Epoch: 92, loss: 3.67676544189, acc: 0.057650860399\n",
      "Epoch: 93, loss: 3.67036724091, acc: 0.0587284490466\n",
      "Epoch: 94, loss: 3.66310858727, acc: 0.0554956905544\n",
      "Epoch: 95, loss: 3.66303873062, acc: 0.0641163811088\n",
      "Epoch: 96, loss: 3.66095900536, acc: 0.0619612075388\n",
      "Epoch: 97, loss: 3.66407060623, acc: 0.0565732754767\n",
      "Epoch: 98, loss: 3.66684412956, acc: 0.0630387961864\n",
      "Epoch: 99, loss: 3.64914298058, acc: 0.0630387961864\n",
      "Epoch: 100, loss: 3.64346218109, acc: 0.0678879320621\n",
      "Val: 0.0183189660311\n",
      "Epoch: 101, loss: 3.62199044228, acc: 0.0668103471398\n",
      "Epoch: 102, loss: 3.61601519585, acc: 0.0678879320621\n",
      "Epoch: 103, loss: 3.61887836456, acc: 0.0700431019068\n",
      "Epoch: 104, loss: 3.61589503288, acc: 0.0792025849223\n",
      "Epoch: 105, loss: 3.62712717056, acc: 0.0662715509534\n",
      "Epoch: 106, loss: 3.59713220596, acc: 0.0743534490466\n",
      "Epoch: 107, loss: 3.60702991486, acc: 0.0673491358757\n",
      "Epoch: 108, loss: 3.59692645073, acc: 0.0775862038136\n",
      "Epoch: 109, loss: 3.58915925026, acc: 0.0786637961864\n",
      "Epoch: 110, loss: 3.58832454681, acc: 0.0748922377825\n",
      "Val: 0.0237068962306\n",
      "Epoch: 111, loss: 3.57874727249, acc: 0.0808189660311\n",
      "Epoch: 112, loss: 3.54340195656, acc: 0.0813577622175\n",
      "Epoch: 113, loss: 3.5549197197, acc: 0.0786637961864\n",
      "Epoch: 114, loss: 3.54756736755, acc: 0.0824353471398\n",
      "Epoch: 115, loss: 3.57494306564, acc: 0.0716594830155\n",
      "Epoch: 116, loss: 3.56848430634, acc: 0.0770474150777\n",
      "Epoch: 117, loss: 3.53940749168, acc: 0.0711206868291\n",
      "Epoch: 118, loss: 3.54478001595, acc: 0.084051720798\n",
      "Epoch: 119, loss: 3.55599522591, acc: 0.0824353471398\n",
      "Epoch: 120, loss: 3.52141499519, acc: 0.0845905169845\n",
      "Val: 0.0247844830155\n",
      "Epoch: 121, loss: 3.50404977798, acc: 0.0905172377825\n",
      "Epoch: 122, loss: 3.50262928009, acc: 0.0975215509534\n",
      "Epoch: 123, loss: 3.50106453896, acc: 0.0980603471398\n",
      "Epoch: 124, loss: 3.48748779297, acc: 0.0926724150777\n",
      "Epoch: 125, loss: 3.49128937721, acc: 0.0862068980932\n",
      "Epoch: 126, loss: 3.49585962296, acc: 0.0921336188912\n",
      "Epoch: 127, loss: 3.48710846901, acc: 0.0872844830155\n",
      "Epoch: 128, loss: 3.45624136925, acc: 0.105603449047\n",
      "Epoch: 129, loss: 3.42494797707, acc: 0.102909483016\n",
      "Epoch: 130, loss: 3.4231595993, acc: 0.105603449047\n",
      "Val: 0.0204741377383\n",
      "Epoch: 131, loss: 3.4154958725, acc: 0.103448279202\n",
      "Epoch: 132, loss: 3.42555999756, acc: 0.102909483016\n",
      "Epoch: 133, loss: 3.43070960045, acc: 0.0964439660311\n",
      "Epoch: 134, loss: 3.41182923317, acc: 0.11368534714\n",
      "Epoch: 135, loss: 3.41138124466, acc: 0.112068966031\n",
      "Epoch: 136, loss: 3.38655567169, acc: 0.111530169845\n",
      "Epoch: 137, loss: 3.39502167702, acc: 0.111530169845\n",
      "Epoch: 138, loss: 3.37582063675, acc: 0.12068965286\n",
      "Epoch: 139, loss: 3.34398317337, acc: 0.123922415078\n",
      "Epoch: 140, loss: 3.31053471565, acc: 0.12068965286\n",
      "Val: 0.0307112075388\n",
      "Epoch: 141, loss: 3.32714128494, acc: 0.133081898093\n",
      "Epoch: 142, loss: 3.36499714851, acc: 0.117456898093\n",
      "Epoch: 143, loss: 3.27506017685, acc: 0.14762930572\n",
      "Epoch: 144, loss: 3.30188703537, acc: 0.140625\n",
      "Epoch: 145, loss: 3.2979362011, acc: 0.150862067938\n",
      "Epoch: 146, loss: 3.25691771507, acc: 0.137931033969\n",
      "Epoch: 147, loss: 3.23691868782, acc: 0.150323271751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 148, loss: 3.21480536461, acc: 0.151939660311\n",
      "Epoch: 149, loss: 3.26065897942, acc: 0.142780169845\n",
      "Epoch: 150, loss: 3.22731971741, acc: 0.14762930572\n",
      "Val: 0.0226293094456\n",
      "Epoch: 151, loss: 3.20280170441, acc: 0.161099135876\n",
      "Epoch: 152, loss: 3.18925285339, acc: 0.170258626342\n",
      "Epoch: 153, loss: 3.16928935051, acc: 0.175646558404\n",
      "Epoch: 154, loss: 3.1974594593, acc: 0.165409475565\n",
      "Epoch: 155, loss: 3.16455888748, acc: 0.157866373658\n",
      "Epoch: 156, loss: 3.15656328201, acc: 0.175646558404\n",
      "Epoch: 157, loss: 3.16433238983, acc: 0.16325430572\n",
      "Epoch: 158, loss: 3.11254858971, acc: 0.197198271751\n",
      "Epoch: 159, loss: 3.12961101532, acc: 0.179956898093\n",
      "Epoch: 160, loss: 3.08786582947, acc: 0.184806033969\n",
      "Val: 0.0231681037694\n",
      "Epoch: 161, loss: 3.07542824745, acc: 0.17887930572\n",
      "Epoch: 162, loss: 3.05347299576, acc: 0.198814660311\n",
      "Epoch: 163, loss: 3.05527925491, acc: 0.191271558404\n",
      "Epoch: 164, loss: 3.00047445297, acc: 0.211206898093\n",
      "Epoch: 165, loss: 3.03032469749, acc: 0.19450430572\n",
      "Epoch: 166, loss: 3.04261374474, acc: 0.201508626342\n",
      "Epoch: 167, loss: 3.02460694313, acc: 0.203125\n",
      "Epoch: 168, loss: 2.96883320808, acc: 0.224676728249\n",
      "Epoch: 169, loss: 2.95335555077, acc: 0.219288796186\n",
      "Epoch: 170, loss: 2.93596816063, acc: 0.227909475565\n",
      "Val: 0.0204741377383\n",
      "Epoch: 171, loss: 2.88833856583, acc: 0.232219830155\n",
      "Epoch: 172, loss: 2.88092851639, acc: 0.229525864124\n",
      "Epoch: 173, loss: 2.84845209122, acc: 0.248383626342\n",
      "Epoch: 174, loss: 2.87446403503, acc: 0.237607762218\n",
      "Epoch: 175, loss: 2.79122328758, acc: 0.266702592373\n",
      "Epoch: 176, loss: 2.83103227615, acc: 0.244073271751\n",
      "Epoch: 177, loss: 2.81889009476, acc: 0.251077592373\n",
      "Epoch: 178, loss: 2.80552768707, acc: 0.255926728249\n",
      "Epoch: 179, loss: 2.78242492676, acc: 0.26293104887\n",
      "Epoch: 180, loss: 2.75759720802, acc: 0.27855604887\n",
      "Val: 0.0269396547228\n",
      "Epoch: 181, loss: 2.73129630089, acc: 0.275323271751\n",
      "Epoch: 182, loss: 2.70338368416, acc: 0.282327592373\n",
      "Epoch: 183, loss: 2.70255947113, acc: 0.289331883192\n",
      "Epoch: 184, loss: 2.69058680534, acc: 0.287715524435\n",
      "Epoch: 185, loss: 2.66111850739, acc: 0.285560339689\n",
      "Epoch: 186, loss: 2.68304729462, acc: 0.308189660311\n",
      "Epoch: 187, loss: 2.67580628395, acc: 0.300107747316\n",
      "Epoch: 188, loss: 2.58251881599, acc: 0.323814660311\n",
      "Epoch: 189, loss: 2.54676771164, acc: 0.33081895113\n",
      "Epoch: 190, loss: 2.50353574753, acc: 0.332974135876\n",
      "Val: 0.0296336207539\n",
      "Epoch: 191, loss: 2.51791238785, acc: 0.335129320621\n",
      "Epoch: 192, loss: 2.53699040413, acc: 0.344827592373\n",
      "Epoch: 193, loss: 2.52589583397, acc: 0.335129320621\n",
      "Epoch: 194, loss: 2.46799921989, acc: 0.339439660311\n",
      "Epoch: 195, loss: 2.45306539536, acc: 0.342672407627\n",
      "Epoch: 196, loss: 2.43350291252, acc: 0.36206895113\n",
      "Epoch: 197, loss: 2.47936701775, acc: 0.336206883192\n",
      "Epoch: 198, loss: 2.42809534073, acc: 0.353987067938\n",
      "Epoch: 199, loss: 2.39020252228, acc: 0.365840524435\n",
      "Epoch: 200, loss: 2.32556009293, acc: 0.371228456497\n",
      "Val: 0.0204741377383\n",
      "Epoch: 201, loss: 2.41674304008, acc: 0.35668104887\n",
      "Epoch: 202, loss: 2.31058716774, acc: 0.379310339689\n",
      "Epoch: 203, loss: 2.3181283474, acc: 0.384159475565\n",
      "Epoch: 204, loss: 2.27342700958, acc: 0.389547407627\n",
      "Epoch: 205, loss: 2.22304677963, acc: 0.410560339689\n",
      "Epoch: 206, loss: 2.24746346474, acc: 0.397629320621\n",
      "Epoch: 207, loss: 2.25850868225, acc: 0.404633611441\n",
      "Epoch: 208, loss: 2.19536519051, acc: 0.406788796186\n",
      "Epoch: 209, loss: 2.16573381424, acc: 0.422952592373\n",
      "Epoch: 210, loss: 2.17292332649, acc: 0.40894395113\n",
      "Val: 0.0269396547228\n",
      "Epoch: 211, loss: 2.14019370079, acc: 0.438038796186\n",
      "Epoch: 212, loss: 2.19988894463, acc: 0.417025864124\n",
      "Epoch: 213, loss: 2.05849003792, acc: 0.4375\n",
      "Epoch: 214, loss: 2.10036921501, acc: 0.448814660311\n",
      "Epoch: 215, loss: 2.07931804657, acc: 0.448275864124\n",
      "Epoch: 216, loss: 2.01597356796, acc: 0.462284475565\n",
      "Epoch: 217, loss: 2.07137465477, acc: 0.452586203814\n",
      "Epoch: 218, loss: 2.01977777481, acc: 0.456896543503\n",
      "Epoch: 219, loss: 2.02473187447, acc: 0.45581895113\n",
      "Epoch: 220, loss: 1.93787097931, acc: 0.48168104887\n",
      "Val: 0.0226293094456\n",
      "Epoch: 221, loss: 1.91709506512, acc: 0.480603456497\n",
      "Epoch: 222, loss: 1.86621248722, acc: 0.501616358757\n",
      "Epoch: 223, loss: 1.87565839291, acc: 0.496767252684\n",
      "Epoch: 224, loss: 1.85238933563, acc: 0.515625\n",
      "Epoch: 225, loss: 1.81357705593, acc: 0.515086233616\n",
      "Epoch: 226, loss: 1.74319732189, acc: 0.515086233616\n",
      "Epoch: 227, loss: 1.86090695858, acc: 0.50538790226\n",
      "Epoch: 228, loss: 1.81583166122, acc: 0.51023709774\n",
      "Epoch: 229, loss: 1.76813101768, acc: 0.515086233616\n",
      "Epoch: 230, loss: 1.76735162735, acc: 0.522090494633\n",
      "Val: 0.030172413215\n",
      "Epoch: 231, loss: 1.63733160496, acc: 0.556573271751\n",
      "Epoch: 232, loss: 1.69966804981, acc: 0.54418104887\n",
      "Epoch: 233, loss: 1.66180980206, acc: 0.54956895113\n",
      "Epoch: 234, loss: 1.69124293327, acc: 0.543642222881\n",
      "Epoch: 235, loss: 1.66451919079, acc: 0.556034505367\n",
      "Epoch: 236, loss: 1.59168410301, acc: 0.572198271751\n",
      "Epoch: 237, loss: 1.57182264328, acc: 0.567349135876\n",
      "Epoch: 238, loss: 1.63276839256, acc: 0.558728456497\n",
      "Epoch: 239, loss: 1.61843848228, acc: 0.570581912994\n",
      "Epoch: 240, loss: 1.55148041248, acc: 0.572198271751\n",
      "Val: 0.0247844830155\n",
      "Epoch: 241, loss: 1.47195231915, acc: 0.597521543503\n",
      "Epoch: 242, loss: 1.40028095245, acc: 0.622844815254\n",
      "Epoch: 243, loss: 1.42112696171, acc: 0.609375\n",
      "Epoch: 244, loss: 1.40607905388, acc: 0.602370679379\n",
      "Epoch: 245, loss: 1.45885431767, acc: 0.610991358757\n",
      "Epoch: 246, loss: 1.50294852257, acc: 0.587284505367\n",
      "Epoch: 247, loss: 1.47192871571, acc: 0.607219815254\n",
      "Epoch: 248, loss: 1.34063267708, acc: 0.632004320621\n",
      "Epoch: 249, loss: 1.35808444023, acc: 0.642780184746\n",
      "Epoch: 250, loss: 1.30812990665, acc: 0.641702592373\n",
      "Val: 0.0177801717073\n",
      "Epoch: 251, loss: 1.36598992348, acc: 0.630926728249\n",
      "Epoch: 252, loss: 1.30304765701, acc: 0.644396543503\n",
      "Epoch: 253, loss: 1.31288135052, acc: 0.628771543503\n",
      "Epoch: 254, loss: 1.29781854153, acc: 0.651939630508\n",
      "Epoch: 255, loss: 1.19808983803, acc: 0.665409505367\n",
      "Epoch: 256, loss: 1.23762774467, acc: 0.659482777119\n",
      "Epoch: 257, loss: 1.18128108978, acc: 0.678340494633\n",
      "Epoch: 258, loss: 1.2229950428, acc: 0.670797407627\n",
      "Epoch: 259, loss: 1.21516573429, acc: 0.657327592373\n",
      "Epoch: 260, loss: 1.22352004051, acc: 0.652478456497\n",
      "Val: 0.0258620698005\n",
      "Epoch: 261, loss: 1.14694583416, acc: 0.67726290226\n",
      "Epoch: 262, loss: 1.16864359379, acc: 0.67726290226\n",
      "Epoch: 263, loss: 1.15071952343, acc: 0.686422407627\n",
      "Epoch: 264, loss: 1.0840845108, acc: 0.702047407627\n",
      "Epoch: 265, loss: 1.09416794777, acc: 0.686961233616\n",
      "Epoch: 266, loss: 1.11063981056, acc: 0.699892222881\n",
      "Epoch: 267, loss: 1.03525173664, acc: 0.706357777119\n",
      "Epoch: 268, loss: 1.07708573341, acc: 0.69019395113\n",
      "Epoch: 269, loss: 1.03456246853, acc: 0.704741358757\n",
      "Epoch: 270, loss: 1.00374758244, acc: 0.723599135876\n",
      "Val: 0.0220905169845\n",
      "Epoch: 271, loss: 1.09092795849, acc: 0.693965494633\n",
      "Epoch: 272, loss: 0.989828944206, acc: 0.72144395113\n",
      "Epoch: 273, loss: 0.998578846455, acc: 0.734913766384\n",
      "Epoch: 274, loss: 0.973963916302, acc: 0.709590494633\n",
      "Epoch: 275, loss: 0.936603605747, acc: 0.734913766384\n",
      "Epoch: 276, loss: 0.980925261974, acc: 0.71875\n",
      "Epoch: 277, loss: 0.908828139305, acc: 0.738146543503\n",
      "Epoch: 278, loss: 0.958166182041, acc: 0.717133641243\n",
      "Epoch: 279, loss: 0.931919157505, acc: 0.733836233616\n",
      "Epoch: 280, loss: 0.889988958836, acc: 0.746228456497\n",
      "Val: 0.0253232754767\n",
      "Epoch: 281, loss: 0.921167492867, acc: 0.727909505367\n",
      "Epoch: 282, loss: 0.852295100689, acc: 0.748922407627\n",
      "Epoch: 283, loss: 0.86108648777, acc: 0.759159505367\n",
      "Epoch: 284, loss: 0.863395810127, acc: 0.760775864124\n",
      "Epoch: 285, loss: 0.847781538963, acc: 0.766702592373\n",
      "Epoch: 286, loss: 0.894655823708, acc: 0.754849135876\n",
      "Epoch: 287, loss: 0.865074813366, acc: 0.760775864124\n",
      "Epoch: 288, loss: 0.819261610508, acc: 0.761314630508\n",
      "Epoch: 289, loss: 0.811258375645, acc: 0.770474135876\n",
      "Epoch: 290, loss: 0.831426620483, acc: 0.772090494633\n",
      "Val: 0.0258620698005\n",
      "Epoch: 291, loss: 0.798188984394, acc: 0.77586209774\n",
      "Epoch: 292, loss: 0.802560031414, acc: 0.776939630508\n",
      "Epoch: 293, loss: 0.745256662369, acc: 0.790409505367\n",
      "Epoch: 294, loss: 0.728694975376, acc: 0.795258641243\n",
      "Epoch: 295, loss: 0.751011371613, acc: 0.786099135876\n",
      "Epoch: 296, loss: 0.738418698311, acc: 0.793642222881\n",
      "Epoch: 297, loss: 0.698938012123, acc: 0.799030184746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 298, loss: 0.833765864372, acc: 0.77101290226\n",
      "Epoch: 299, loss: 0.699335455894, acc: 0.79956895113\n",
      "Epoch: 300, loss: 0.692457020283, acc: 0.786099135876\n",
      "Val: 0.0328663811088\n",
      "Epoch: 301, loss: 0.649897933006, acc: 0.808728456497\n",
      "Epoch: 302, loss: 0.654918313026, acc: 0.810883641243\n",
      "Epoch: 303, loss: 0.668532788754, acc: 0.811422407627\n",
      "Epoch: 304, loss: 0.692501544952, acc: 0.810344815254\n",
      "Epoch: 305, loss: 0.651425778866, acc: 0.825969815254\n",
      "Epoch: 306, loss: 0.694044649601, acc: 0.80226290226\n",
      "Epoch: 307, loss: 0.638267695904, acc: 0.823275864124\n",
      "Epoch: 308, loss: 0.60791438818, acc: 0.824353456497\n",
      "Epoch: 309, loss: 0.646574139595, acc: 0.81519395113\n",
      "Epoch: 310, loss: 0.654594004154, acc: 0.813577592373\n",
      "Val: 0.0323275849223\n",
      "Epoch: 311, loss: 0.599943876266, acc: 0.821659505367\n",
      "Epoch: 312, loss: 0.59032022953, acc: 0.836745679379\n",
      "Epoch: 313, loss: 0.600162923336, acc: 0.829202592373\n",
      "Epoch: 314, loss: 0.62873762846, acc: 0.820581912994\n",
      "Epoch: 315, loss: 0.533407390118, acc: 0.850754320621\n",
      "Epoch: 316, loss: 0.6004576087, acc: 0.829741358757\n",
      "Epoch: 317, loss: 0.570299863815, acc: 0.837823271751\n",
      "Epoch: 318, loss: 0.624392151833, acc: 0.820043087006\n",
      "Epoch: 319, loss: 0.544881939888, acc: 0.845905184746\n",
      "Epoch: 320, loss: 0.569508969784, acc: 0.844288766384\n",
      "Val: 0.0226293094456\n",
      "Epoch: 321, loss: 0.583655297756, acc: 0.820581912994\n",
      "Epoch: 322, loss: 0.520248055458, acc: 0.857758641243\n",
      "Epoch: 323, loss: 0.528837919235, acc: 0.848599135876\n",
      "Epoch: 324, loss: 0.533508837223, acc: 0.84105604887\n",
      "Epoch: 325, loss: 0.587952077389, acc: 0.830280184746\n",
      "Epoch: 326, loss: 0.520415127277, acc: 0.847521543503\n",
      "Epoch: 327, loss: 0.481017410755, acc: 0.858836233616\n",
      "Epoch: 328, loss: 0.549329280853, acc: 0.83836209774\n",
      "Epoch: 329, loss: 0.487723588943, acc: 0.848060369492\n",
      "Epoch: 330, loss: 0.536574602127, acc: 0.84375\n",
      "Val: 0.0323275849223\n",
      "Epoch: 331, loss: 0.461490124464, acc: 0.85668104887\n",
      "Epoch: 332, loss: 0.475581943989, acc: 0.86476290226\n",
      "Epoch: 333, loss: 0.443205833435, acc: 0.873383641243\n",
      "Epoch: 334, loss: 0.479101598263, acc: 0.862607777119\n",
      "Epoch: 335, loss: 0.439868807793, acc: 0.878232777119\n",
      "Epoch: 336, loss: 0.445515990257, acc: 0.86961209774\n",
      "Epoch: 337, loss: 0.446257352829, acc: 0.870689630508\n",
      "Epoch: 338, loss: 0.450108498335, acc: 0.870150864124\n",
      "Epoch: 339, loss: 0.506164073944, acc: 0.84644395113\n",
      "Epoch: 340, loss: 0.473785430193, acc: 0.87230604887\n",
      "Val: 0.0231681037694\n",
      "Epoch: 341, loss: 0.403912365437, acc: 0.880926728249\n",
      "Epoch: 342, loss: 0.429422199726, acc: 0.870150864124\n",
      "Epoch: 343, loss: 0.428895950317, acc: 0.870150864124\n",
      "Epoch: 344, loss: 0.412109166384, acc: 0.878771543503\n",
      "Epoch: 345, loss: 0.395616590977, acc: 0.883620679379\n",
      "Epoch: 346, loss: 0.449439615011, acc: 0.871767222881\n",
      "Epoch: 347, loss: 0.363319754601, acc: 0.88793104887\n",
      "Epoch: 348, loss: 0.38687902689, acc: 0.886853456497\n",
      "Epoch: 349, loss: 0.385140389204, acc: 0.886314630508\n",
      "Epoch: 350, loss: 0.41251128912, acc: 0.875\n",
      "Val: 0.0199353452772\n",
      "Epoch: 351, loss: 0.397380083799, acc: 0.882543087006\n",
      "Epoch: 352, loss: 0.413900494576, acc: 0.880926728249\n",
      "Epoch: 353, loss: 0.353408455849, acc: 0.889547407627\n",
      "Epoch: 354, loss: 0.394243687391, acc: 0.891702592373\n",
      "Epoch: 355, loss: 0.371310889721, acc: 0.892780184746\n",
      "Epoch: 356, loss: 0.338775247335, acc: 0.902478456497\n",
      "Epoch: 357, loss: 0.311819046736, acc: 0.904633641243\n",
      "Epoch: 358, loss: 0.409775286913, acc: 0.87230604887\n",
      "Epoch: 359, loss: 0.300831198692, acc: 0.906788766384\n",
      "Epoch: 360, loss: 0.339723199606, acc: 0.900323271751\n",
      "Val: 0.0323275849223\n",
      "Epoch: 361, loss: 0.389127671719, acc: 0.889008641243\n",
      "Epoch: 362, loss: 0.373085081577, acc: 0.894396543503\n",
      "Epoch: 363, loss: 0.339367359877, acc: 0.900323271751\n",
      "Epoch: 364, loss: 0.351014554501, acc: 0.898706912994\n",
      "Epoch: 365, loss: 0.360898852348, acc: 0.894935369492\n",
      "Epoch: 366, loss: 0.3319696486, acc: 0.902478456497\n",
      "Epoch: 367, loss: 0.382584244013, acc: 0.883081912994\n",
      "Epoch: 368, loss: 0.3504345119, acc: 0.897629320621\n",
      "Epoch: 369, loss: 0.300950080156, acc: 0.907866358757\n",
      "Epoch: 370, loss: 0.303634703159, acc: 0.90894395113\n",
      "Val: 0.0264008622617\n",
      "Epoch: 371, loss: 0.374575287104, acc: 0.889008641243\n",
      "Epoch: 372, loss: 0.360258340836, acc: 0.891163766384\n",
      "Epoch: 373, loss: 0.279002457857, acc: 0.921336233616\n",
      "Epoch: 374, loss: 0.319820731878, acc: 0.90625\n",
      "Epoch: 375, loss: 0.314982056618, acc: 0.913254320621\n",
      "Epoch: 376, loss: 0.288640320301, acc: 0.918642222881\n",
      "Epoch: 377, loss: 0.280612230301, acc: 0.914870679379\n",
      "Epoch: 378, loss: 0.258903414011, acc: 0.921875\n",
      "Epoch: 379, loss: 0.328292012215, acc: 0.899784505367\n",
      "Epoch: 380, loss: 0.303929507732, acc: 0.90894395113\n",
      "Val: 0.03125\n",
      "Epoch: 381, loss: 0.317564129829, acc: 0.906788766384\n",
      "Epoch: 382, loss: 0.3051661551, acc: 0.906788766384\n",
      "Epoch: 383, loss: 0.300446301699, acc: 0.910560369492\n",
      "Epoch: 384, loss: 0.252117007971, acc: 0.92726290226\n",
      "Epoch: 385, loss: 0.31499221921, acc: 0.90894395113\n",
      "Epoch: 386, loss: 0.274745672941, acc: 0.926185369492\n",
      "Epoch: 387, loss: 0.306066453457, acc: 0.912715494633\n",
      "Epoch: 388, loss: 0.234226480126, acc: 0.93480604887\n",
      "Epoch: 389, loss: 0.278841644526, acc: 0.913254320621\n",
      "Epoch: 390, loss: 0.263382732868, acc: 0.920797407627\n",
      "Val: 0.0371767245233\n",
      "Epoch: 391, loss: 0.283412277699, acc: 0.912715494633\n",
      "Epoch: 392, loss: 0.258573621511, acc: 0.91918104887\n",
      "Epoch: 393, loss: 0.263997077942, acc: 0.92456895113\n",
      "Epoch: 394, loss: 0.260019212961, acc: 0.921875\n",
      "Epoch: 395, loss: 0.256866455078, acc: 0.923491358757\n",
      "Epoch: 396, loss: 0.25053268671, acc: 0.925646543503\n",
      "Epoch: 397, loss: 0.2744820714, acc: 0.921336233616\n",
      "Epoch: 398, loss: 0.293679863214, acc: 0.912715494633\n",
      "Epoch: 399, loss: 0.241383329034, acc: 0.928340494633\n",
      "Epoch: 400, loss: 0.260027527809, acc: 0.913254320621\n",
      "Val: 0.0355603434145\n",
      "Epoch: 401, loss: 0.252741545439, acc: 0.918103456497\n",
      "Epoch: 402, loss: 0.280371695757, acc: 0.911099135876\n",
      "Epoch: 403, loss: 0.258304297924, acc: 0.92456895113\n",
      "Epoch: 404, loss: 0.259400486946, acc: 0.91918104887\n",
      "Epoch: 405, loss: 0.236302211881, acc: 0.936961233616\n",
      "Epoch: 406, loss: 0.262299835682, acc: 0.92456895113\n",
      "Epoch: 407, loss: 0.263482064009, acc: 0.919719815254\n",
      "Epoch: 408, loss: 0.282922506332, acc: 0.922952592373\n",
      "Epoch: 409, loss: 0.267733424902, acc: 0.920258641243\n",
      "Epoch: 410, loss: 0.236866354942, acc: 0.920797407627\n",
      "Val: 0.0317887924612\n",
      "Epoch: 411, loss: 0.245122775435, acc: 0.926724135876\n",
      "Epoch: 412, loss: 0.2059379071, acc: 0.936422407627\n",
      "Epoch: 413, loss: 0.276785284281, acc: 0.91163790226\n",
      "Epoch: 414, loss: 0.244807034731, acc: 0.924030184746\n",
      "Epoch: 415, loss: 0.218211770058, acc: 0.935883641243\n",
      "Epoch: 416, loss: 0.197428822517, acc: 0.941271543503\n",
      "Epoch: 417, loss: 0.268097966909, acc: 0.921875\n",
      "Epoch: 418, loss: 0.23461677134, acc: 0.925646543503\n",
      "Epoch: 419, loss: 0.217936381698, acc: 0.93211209774\n",
      "Epoch: 420, loss: 0.203544393182, acc: 0.940732777119\n",
      "Val: 0.0307112075388\n",
      "Epoch: 421, loss: 0.221506476402, acc: 0.93211209774\n",
      "Epoch: 422, loss: 0.18338920176, acc: 0.942349135876\n",
      "Epoch: 423, loss: 0.196480289102, acc: 0.943426728249\n",
      "Epoch: 424, loss: 0.227255657315, acc: 0.936961233616\n",
      "Epoch: 425, loss: 0.259893327951, acc: 0.920797407627\n",
      "Epoch: 426, loss: 0.209136873484, acc: 0.938038766384\n",
      "Epoch: 427, loss: 0.211166039109, acc: 0.942349135876\n",
      "Epoch: 428, loss: 0.218645423651, acc: 0.936961233616\n",
      "Epoch: 429, loss: 0.226881489158, acc: 0.926724135876\n",
      "Epoch: 430, loss: 0.213861912489, acc: 0.936422407627\n",
      "Val: 0.0296336207539\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-55ed7cee857e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minceptionv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_corrupt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m'''REP 0 with label corruption at 0.8'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.8lcp_rep0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.pyc\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0mend_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_batch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0mval_batch_no\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtot_val_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                         \u001b[0mouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_idxs_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                         \u001b[0;31m# Global Average Pooling the activations : saves space and removes pixel dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                         \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.pyc\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(self, inputs, layer)\u001b[0m\n\u001b[1;32m     57\u001b[0m         get_layer_output = K.function([self.model.layers[0].input],\n\u001b[1;32m     58\u001b[0m                               [self.model.get_layer(layer).output])\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "inceptionv3 = InceptionV3(batch_size=64)\n",
    "dataset = Dataset(train_data, val_data, test_data, label_corrupt_p = 0.8, random_seed=0) \n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "'''REP 0 with label corruption at 0.8'''\n",
    "inceptionv3.save('0.8lcp_rep0', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "History not saved\n"
     ]
    }
   ],
   "source": [
    "inceptionv3.save('0.8lcp_rep0', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 17.9912278652\n",
      "Epoch: 0, loss: 4.22448062897, acc: 0.0161637924612\n",
      "Val: 0.0215517245233\n",
      "Epoch: 1, loss: 4.13922929764, acc: 0.0199353452772\n",
      "Epoch: 2, loss: 4.11470508575, acc: 0.0215517245233\n",
      "Epoch: 3, loss: 4.06615638733, acc: 0.0274784490466\n",
      "Epoch: 4, loss: 4.32785987854, acc: 0.0215517245233\n",
      "Epoch: 5, loss: 4.1319565773, acc: 0.0264008622617\n",
      "Epoch: 6, loss: 4.05671453476, acc: 0.0350215509534\n",
      "Epoch: 7, loss: 4.05777788162, acc: 0.0274784490466\n",
      "Epoch: 8, loss: 6.40079784393, acc: 0.0215517245233\n",
      "Epoch: 9, loss: 4.11753320694, acc: 0.0237068962306\n",
      "Epoch: 10, loss: 4.03929042816, acc: 0.0220905169845\n",
      "Val: 0.0188577584922\n",
      "Epoch: 11, loss: 4.01333761215, acc: 0.0350215509534\n",
      "Epoch: 12, loss: 4.00271701813, acc: 0.0339439660311\n",
      "Epoch: 13, loss: 3.9786157608, acc: 0.0463362075388\n",
      "Epoch: 14, loss: 4.00674629211, acc: 0.0344827584922\n",
      "Epoch: 15, loss: 3.99836969376, acc: 0.036099139601\n",
      "Epoch: 16, loss: 3.98764610291, acc: 0.0436422415078\n",
      "Epoch: 17, loss: 3.94607186317, acc: 0.0377155169845\n",
      "Epoch: 18, loss: 3.94795894623, acc: 0.0474137924612\n",
      "Epoch: 19, loss: 3.92523169518, acc: 0.0474137924612\n",
      "Epoch: 20, loss: 3.92327594757, acc: 0.042025860399\n",
      "Val: 0.0172413792461\n",
      "Epoch: 21, loss: 3.90200996399, acc: 0.057650860399\n",
      "Epoch: 22, loss: 3.90835213661, acc: 0.0592672415078\n",
      "Epoch: 23, loss: 3.83432102203, acc: 0.0673491358757\n",
      "Epoch: 24, loss: 3.80669140816, acc: 0.0695043131709\n",
      "Epoch: 25, loss: 3.76803064346, acc: 0.0673491358757\n",
      "Epoch: 26, loss: 3.76432609558, acc: 0.0678879320621\n",
      "Epoch: 27, loss: 3.7707657814, acc: 0.0711206868291\n",
      "Epoch: 28, loss: 3.76533675194, acc: 0.0727370679379\n",
      "Epoch: 29, loss: 3.73885440826, acc: 0.0759698301554\n",
      "Epoch: 30, loss: 3.68876457214, acc: 0.0700431019068\n",
      "Val: 0.0377155169845\n",
      "Epoch: 31, loss: 3.71047568321, acc: 0.0754310339689\n",
      "Epoch: 32, loss: 3.80681753159, acc: 0.0651939660311\n",
      "Epoch: 33, loss: 3.76010394096, acc: 0.0635775849223\n",
      "Epoch: 34, loss: 3.76103496552, acc: 0.0678879320621\n",
      "Epoch: 35, loss: 3.69924712181, acc: 0.0818965509534\n",
      "Epoch: 36, loss: 3.69218111038, acc: 0.0835129320621\n",
      "Epoch: 37, loss: 3.70004534721, acc: 0.0727370679379\n",
      "Epoch: 38, loss: 3.67716383934, acc: 0.0802801698446\n",
      "Epoch: 39, loss: 3.62796998024, acc: 0.0630387961864\n",
      "Epoch: 40, loss: 3.46200633049, acc: 0.0867456868291\n",
      "Val: 0.0344827584922\n",
      "Epoch: 41, loss: 3.46363043785, acc: 0.0818965509534\n",
      "Epoch: 42, loss: 3.41003632545, acc: 0.0797413811088\n",
      "Epoch: 43, loss: 3.4281847477, acc: 0.0808189660311\n",
      "Epoch: 44, loss: 3.34978318214, acc: 0.0985991358757\n",
      "Epoch: 45, loss: 3.35145998001, acc: 0.103448279202\n",
      "Epoch: 46, loss: 3.3502805233, acc: 0.104525864124\n",
      "Epoch: 47, loss: 3.31986737251, acc: 0.114224135876\n",
      "Epoch: 48, loss: 3.27397966385, acc: 0.10506465286\n",
      "Epoch: 49, loss: 3.25040841103, acc: 0.130387932062\n",
      "Epoch: 50, loss: 3.30711817741, acc: 0.119073279202\n",
      "Val: 0.0387931019068\n",
      "Epoch: 51, loss: 3.2397248745, acc: 0.129849135876\n",
      "Epoch: 52, loss: 3.24259114265, acc: 0.126077592373\n",
      "Epoch: 53, loss: 3.22649621964, acc: 0.139547407627\n",
      "Epoch: 54, loss: 3.2019340992, acc: 0.128232762218\n",
      "Epoch: 55, loss: 3.16195082664, acc: 0.143857762218\n",
      "Epoch: 56, loss: 3.12810850143, acc: 0.158405169845\n",
      "Epoch: 57, loss: 3.15251350403, acc: 0.145474135876\n",
      "Epoch: 58, loss: 3.1301753521, acc: 0.148706898093\n",
      "Epoch: 59, loss: 3.07933735847, acc: 0.16487069428\n",
      "Epoch: 60, loss: 3.05996203423, acc: 0.175107762218\n",
      "Val: 0.0528017245233\n",
      "Epoch: 61, loss: 3.09423851967, acc: 0.158943966031\n",
      "Epoch: 62, loss: 3.06771159172, acc: 0.175107762218\n",
      "Epoch: 63, loss: 3.02990579605, acc: 0.177801728249\n",
      "Epoch: 64, loss: 2.96603083611, acc: 0.184267237782\n",
      "Epoch: 65, loss: 2.98669600487, acc: 0.189655169845\n",
      "Epoch: 66, loss: 2.93973183632, acc: 0.202047407627\n",
      "Epoch: 67, loss: 2.94294977188, acc: 0.195043101907\n",
      "Epoch: 68, loss: 2.91654920578, acc: 0.204741373658\n",
      "Epoch: 69, loss: 2.90704369545, acc: 0.210668101907\n",
      "Epoch: 70, loss: 2.90189671516, acc: 0.195581898093\n",
      "Val: 0.0705818980932\n",
      "Epoch: 71, loss: 2.84148645401, acc: 0.223060339689\n",
      "Epoch: 72, loss: 2.84804463387, acc: 0.203663796186\n",
      "Epoch: 73, loss: 2.84522938728, acc: 0.206357762218\n",
      "Epoch: 74, loss: 2.79636716843, acc: 0.231142237782\n",
      "Epoch: 75, loss: 2.76822566986, acc: 0.233836203814\n",
      "Epoch: 76, loss: 2.77068424225, acc: 0.220905169845\n",
      "Epoch: 77, loss: 2.769708395, acc: 0.22575430572\n",
      "Epoch: 78, loss: 2.70120549202, acc: 0.247844830155\n",
      "Epoch: 79, loss: 2.74189710617, acc: 0.240301728249\n",
      "Epoch: 80, loss: 2.68559765816, acc: 0.238146558404\n",
      "Val: 0.0921336188912\n",
      "Epoch: 81, loss: 2.706553936, acc: 0.231681033969\n",
      "Epoch: 82, loss: 2.6202776432, acc: 0.260775864124\n",
      "Epoch: 83, loss: 2.7160577774, acc: 0.252155184746\n",
      "Epoch: 84, loss: 2.63215208054, acc: 0.25269395113\n",
      "Epoch: 85, loss: 2.61371350288, acc: 0.260237067938\n",
      "Epoch: 86, loss: 2.61209630966, acc: 0.253771543503\n",
      "Epoch: 87, loss: 2.57024693489, acc: 0.279094815254\n",
      "Epoch: 88, loss: 2.57191801071, acc: 0.277478456497\n",
      "Epoch: 89, loss: 2.53264284134, acc: 0.277478456497\n",
      "Epoch: 90, loss: 2.52380657196, acc: 0.284482747316\n",
      "Val: 0.0862068980932\n",
      "Epoch: 91, loss: 2.49018716812, acc: 0.281788796186\n",
      "Epoch: 92, loss: 2.44090270996, acc: 0.291487067938\n",
      "Epoch: 93, loss: 2.44112086296, acc: 0.310344815254\n",
      "Epoch: 94, loss: 2.42789888382, acc: 0.304956883192\n",
      "Epoch: 95, loss: 2.4596683979, acc: 0.295797407627\n",
      "Epoch: 96, loss: 2.31870484352, acc: 0.321659475565\n",
      "Epoch: 97, loss: 2.31898760796, acc: 0.324892252684\n",
      "Epoch: 98, loss: 2.33096504211, acc: 0.327047407627\n",
      "Epoch: 99, loss: 2.25846195221, acc: 0.34375\n",
      "Epoch: 100, loss: 2.27018451691, acc: 0.339978456497\n",
      "Val: 0.0926724150777\n",
      "Epoch: 101, loss: 2.22610878944, acc: 0.361530184746\n",
      "Epoch: 102, loss: 2.18252134323, acc: 0.382004320621\n",
      "Epoch: 103, loss: 2.15268921852, acc: 0.379310339689\n",
      "Epoch: 104, loss: 2.14258527756, acc: 0.378771543503\n",
      "Epoch: 105, loss: 2.18875455856, acc: 0.376077592373\n",
      "Epoch: 106, loss: 2.12027311325, acc: 0.390625\n",
      "Epoch: 107, loss: 2.09682130814, acc: 0.395474135876\n",
      "Epoch: 108, loss: 2.07796406746, acc: 0.402478456497\n",
      "Epoch: 109, loss: 1.99032866955, acc: 0.417025864124\n",
      "Epoch: 110, loss: 2.00153422356, acc: 0.414331883192\n",
      "Val: 0.106142237782\n",
      "Epoch: 111, loss: 1.98475182056, acc: 0.411099135876\n",
      "Epoch: 112, loss: 2.00128936768, acc: 0.411099135876\n",
      "Epoch: 113, loss: 1.92254126072, acc: 0.433189660311\n",
      "Epoch: 114, loss: 1.91706478596, acc: 0.433728456497\n",
      "Epoch: 115, loss: 1.91167783737, acc: 0.433189660311\n",
      "Epoch: 116, loss: 1.94020926952, acc: 0.438038796186\n",
      "Epoch: 117, loss: 1.84413123131, acc: 0.469288796186\n",
      "Epoch: 118, loss: 1.7618124485, acc: 0.482219815254\n",
      "Epoch: 119, loss: 1.85079813004, acc: 0.459590524435\n",
      "Epoch: 120, loss: 1.77067112923, acc: 0.470905184746\n",
      "Val: 0.104525864124\n",
      "Epoch: 121, loss: 1.73017907143, acc: 0.488146543503\n",
      "Epoch: 122, loss: 1.70155286789, acc: 0.494612067938\n",
      "Epoch: 123, loss: 1.68573009968, acc: 0.500538766384\n",
      "Epoch: 124, loss: 1.66669106483, acc: 0.510775864124\n",
      "Epoch: 125, loss: 1.70164275169, acc: 0.494073271751\n",
      "Epoch: 126, loss: 1.64460217953, acc: 0.51293104887\n",
      "Epoch: 127, loss: 1.61144995689, acc: 0.519396543503\n",
      "Epoch: 128, loss: 1.59983694553, acc: 0.52101290226\n",
      "Epoch: 129, loss: 1.57406282425, acc: 0.537715494633\n",
      "Epoch: 130, loss: 1.48951411247, acc: 0.548491358757\n",
      "Val: 0.105603449047\n",
      "Epoch: 131, loss: 1.50118017197, acc: 0.551185369492\n",
      "Epoch: 132, loss: 1.50318610668, acc: 0.556573271751\n",
      "Epoch: 133, loss: 1.44137871265, acc: 0.579741358757\n",
      "Epoch: 134, loss: 1.42045736313, acc: 0.574353456497\n",
      "Epoch: 135, loss: 1.4115691185, acc: 0.566271543503\n",
      "Epoch: 136, loss: 1.38515019417, acc: 0.594288766384\n",
      "Epoch: 137, loss: 1.38190197945, acc: 0.596982777119\n",
      "Epoch: 138, loss: 1.33180916309, acc: 0.590517222881\n",
      "Epoch: 139, loss: 1.38014006615, acc: 0.59105604887\n",
      "Epoch: 140, loss: 1.3311766386, acc: 0.604525864124\n",
      "Val: 0.119073279202\n",
      "Epoch: 141, loss: 1.26828932762, acc: 0.611530184746\n",
      "Epoch: 142, loss: 1.25515365601, acc: 0.645474135876\n",
      "Epoch: 143, loss: 1.20815515518, acc: 0.629849135876\n",
      "Epoch: 144, loss: 1.2286118269, acc: 0.627155184746\n",
      "Epoch: 145, loss: 1.18392455578, acc: 0.651939630508\n",
      "Epoch: 146, loss: 1.13667333126, acc: 0.660560369492\n",
      "Epoch: 147, loss: 1.20356678963, acc: 0.630926728249\n",
      "Epoch: 148, loss: 1.08689510822, acc: 0.66648709774\n",
      "Epoch: 149, loss: 1.06560409069, acc: 0.67456895113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, loss: 1.09661877155, acc: 0.663793087006\n",
      "Val: 0.118534483016\n",
      "Epoch: 151, loss: 1.07151710987, acc: 0.670258641243\n",
      "Epoch: 152, loss: 0.995580732822, acc: 0.701508641243\n",
      "Epoch: 153, loss: 1.03542530537, acc: 0.688577592373\n",
      "Epoch: 154, loss: 1.02971994877, acc: 0.686961233616\n",
      "Epoch: 155, loss: 1.00346112251, acc: 0.690732777119\n",
      "Epoch: 156, loss: 1.01127409935, acc: 0.68480604887\n",
      "Epoch: 157, loss: 1.02399849892, acc: 0.678879320621\n",
      "Epoch: 158, loss: 0.909112811089, acc: 0.714978456497\n",
      "Epoch: 159, loss: 0.956461489201, acc: 0.711745679379\n",
      "Epoch: 160, loss: 0.892854273319, acc: 0.734375\n",
      "Val: 0.102370686829\n",
      "Epoch: 161, loss: 0.872834563255, acc: 0.726831912994\n",
      "Epoch: 162, loss: 0.897854626179, acc: 0.731142222881\n",
      "Epoch: 163, loss: 0.846710920334, acc: 0.732758641243\n",
      "Epoch: 164, loss: 0.89856338501, acc: 0.738685369492\n",
      "Epoch: 165, loss: 0.859209120274, acc: 0.730603456497\n",
      "Epoch: 166, loss: 0.857198238373, acc: 0.746767222881\n",
      "Epoch: 167, loss: 0.849512815475, acc: 0.748922407627\n",
      "Epoch: 168, loss: 0.83266299963, acc: 0.762392222881\n",
      "Epoch: 169, loss: 0.788876950741, acc: 0.761314630508\n",
      "Epoch: 170, loss: 0.800588488579, acc: 0.750538766384\n",
      "Val: 0.116379313171\n",
      "Epoch: 171, loss: 0.742027342319, acc: 0.772629320621\n",
      "Epoch: 172, loss: 0.788373172283, acc: 0.761853456497\n",
      "Epoch: 173, loss: 0.757713973522, acc: 0.77101290226\n",
      "Epoch: 174, loss: 0.739096820354, acc: 0.773168087006\n",
      "Epoch: 175, loss: 0.707277059555, acc: 0.780711233616\n",
      "Epoch: 176, loss: 0.687839865685, acc: 0.793642222881\n",
      "Epoch: 177, loss: 0.654985785484, acc: 0.799030184746\n",
      "Epoch: 178, loss: 0.705072522163, acc: 0.784482777119\n",
      "Epoch: 179, loss: 0.663029909134, acc: 0.798491358757\n",
      "Epoch: 180, loss: 0.710582792759, acc: 0.782866358757\n",
      "Val: 0.110991381109\n",
      "Epoch: 181, loss: 0.616915225983, acc: 0.811961233616\n",
      "Epoch: 182, loss: 0.688055872917, acc: 0.790409505367\n",
      "Epoch: 183, loss: 0.6703145504, acc: 0.804418087006\n",
      "Epoch: 184, loss: 0.589844226837, acc: 0.827047407627\n",
      "ERR. something not working\n",
      "(778, 900, 4)\n",
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'max_row' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-d032ab849c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minceptionv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_corrupt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m'''REP 3'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.corr_rep2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.py\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             for x_batch, y_batch in datagen.flow(x_train[shuffle_idxs_train], \n\u001b[1;32m    213\u001b[0m                                                  \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffle_idxs_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                                                  batch_size=batch_size):\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m             \u001b[0;31m#removing standardize cause I already did it in my code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;31m#x = self.image_data_generator.standardize(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/image.pyc\u001b[0m in \u001b[0;36mrandom_transform\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m#left_crop_size = (patch_size + 1) / 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mmin_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0mmax_row\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mpatch_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m             \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0;31m#print(\"min_row: {}, max_row: {}\".format(min_row, max_row))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'max_row' referenced before assignment"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "inceptionv3 = InceptionV3(batch_size=64)\n",
    "dataset = Dataset(train_data, val_data, test_data, label_corrupt_p = 0., random_seed=3) \n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "'''REP 3'''\n",
    "inceptionv3.save('0.corr_rep2', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 27.4207718372\n",
      "Epoch: 0, loss: 4.25268268585, acc: 0.0199353452772\n",
      "Val: 0.0210129301995\n",
      "Epoch: 1, loss: 4.08088541031, acc: 0.0140086207539\n",
      "Epoch: 2, loss: 4.08960580826, acc: 0.0183189660311\n",
      "Epoch: 3, loss: 4.28994655609, acc: 0.0188577584922\n",
      "Epoch: 4, loss: 4.10726165771, acc: 0.0220905169845\n",
      "Epoch: 5, loss: 4.08601951599, acc: 0.0172413792461\n",
      "Epoch: 6, loss: 4.11702013016, acc: 0.0220905169845\n",
      "Epoch: 7, loss: 4.38893842697, acc: 0.0215517245233\n",
      "Epoch: 8, loss: 4.08279895782, acc: 0.0204741377383\n",
      "Epoch: 9, loss: 4.08253002167, acc: 0.0172413792461\n",
      "Epoch: 10, loss: 4.05529880524, acc: 0.015625\n",
      "Val: 0.0210129301995\n",
      "Epoch: 11, loss: 4.09417819977, acc: 0.0199353452772\n",
      "Epoch: 12, loss: 3.89196228981, acc: 0.0210129301995\n",
      "Epoch: 13, loss: 3.85472178459, acc: 0.0188577584922\n",
      "Epoch: 14, loss: 3.84703850746, acc: 0.0258620698005\n",
      "Epoch: 15, loss: 3.8386824131, acc: 0.0258620698005\n",
      "Epoch: 16, loss: 3.8363904953, acc: 0.0237068962306\n",
      "Epoch: 17, loss: 3.83768486977, acc: 0.0285560339689\n",
      "Epoch: 18, loss: 3.82369971275, acc: 0.0307112075388\n",
      "Epoch: 19, loss: 3.81847453117, acc: 0.0317887924612\n",
      "Epoch: 20, loss: 3.82111334801, acc: 0.0317887924612\n",
      "Val: 0.0188577584922\n",
      "Epoch: 21, loss: 3.81153106689, acc: 0.0328663811088\n",
      "Epoch: 22, loss: 3.81566023827, acc: 0.0280172415078\n",
      "Epoch: 23, loss: 3.81083703041, acc: 0.0334051735699\n",
      "Epoch: 24, loss: 3.80993056297, acc: 0.0350215509534\n",
      "Epoch: 25, loss: 3.80229902267, acc: 0.0307112075388\n",
      "Epoch: 26, loss: 3.79086208344, acc: 0.0387931019068\n",
      "Epoch: 27, loss: 3.79676270485, acc: 0.03125\n",
      "Epoch: 28, loss: 3.79730129242, acc: 0.0398706905544\n",
      "Epoch: 29, loss: 3.79317235947, acc: 0.0366379320621\n",
      "Epoch: 30, loss: 3.79551005363, acc: 0.0323275849223\n",
      "Val: 0.0220905169845\n",
      "Epoch: 31, loss: 3.7822599411, acc: 0.042025860399\n",
      "Epoch: 32, loss: 3.78740406036, acc: 0.0366379320621\n",
      "Epoch: 33, loss: 3.78555369377, acc: 0.0339439660311\n",
      "Epoch: 34, loss: 3.77667188644, acc: 0.0457974150777\n",
      "Epoch: 35, loss: 3.77526545525, acc: 0.0366379320621\n",
      "Epoch: 36, loss: 3.77629065514, acc: 0.0452586188912\n",
      "Epoch: 37, loss: 3.77762079239, acc: 0.046875\n",
      "Epoch: 38, loss: 3.76345205307, acc: 0.0425646565855\n",
      "Epoch: 39, loss: 3.7565433979, acc: 0.0538793094456\n",
      "Epoch: 40, loss: 3.75184011459, acc: 0.051724139601\n",
      "Val: 0.0231681037694\n",
      "Epoch: 41, loss: 3.7500731945, acc: 0.0452586188912\n",
      "Epoch: 42, loss: 3.75430393219, acc: 0.0414870679379\n",
      "Epoch: 43, loss: 3.75302791595, acc: 0.0414870679379\n",
      "Epoch: 44, loss: 3.75327110291, acc: 0.0474137924612\n",
      "Epoch: 45, loss: 3.74811983109, acc: 0.051724139601\n",
      "Epoch: 46, loss: 3.72679400444, acc: 0.0528017245233\n",
      "Epoch: 47, loss: 3.73464298248, acc: 0.0506465509534\n",
      "Epoch: 48, loss: 3.73068284988, acc: 0.0506465509534\n",
      "Epoch: 49, loss: 3.7287106514, acc: 0.0560344830155\n",
      "Epoch: 50, loss: 3.71662592888, acc: 0.0463362075388\n",
      "Val: 0.0366379320621\n",
      "Epoch: 51, loss: 3.72263598442, acc: 0.0528017245233\n",
      "Epoch: 52, loss: 3.71921253204, acc: 0.0528017245233\n",
      "Epoch: 53, loss: 3.70572209358, acc: 0.0571120679379\n",
      "Epoch: 54, loss: 3.69606614113, acc: 0.0592672415078\n",
      "Epoch: 55, loss: 3.69869923592, acc: 0.0571120679379\n",
      "Epoch: 56, loss: 3.69779205322, acc: 0.057650860399\n",
      "Epoch: 57, loss: 3.6817252636, acc: 0.0587284490466\n",
      "Epoch: 58, loss: 3.67637348175, acc: 0.0630387961864\n",
      "Epoch: 59, loss: 3.66619706154, acc: 0.0603448264301\n",
      "Epoch: 60, loss: 3.67029857635, acc: 0.0619612075388\n",
      "Val: 0.0280172415078\n",
      "Epoch: 61, loss: 3.66880393028, acc: 0.0608836188912\n",
      "Epoch: 62, loss: 3.66862010956, acc: 0.0614224150777\n",
      "Epoch: 63, loss: 3.65551400185, acc: 0.0673491358757\n",
      "Epoch: 64, loss: 3.67159509659, acc: 0.0608836188912\n",
      "Epoch: 65, loss: 3.65775108337, acc: 0.0657327622175\n",
      "Epoch: 66, loss: 3.62678265572, acc: 0.0700431019068\n",
      "Epoch: 67, loss: 3.63233160973, acc: 0.0678879320621\n",
      "Epoch: 68, loss: 3.61766386032, acc: 0.0678879320621\n",
      "Epoch: 69, loss: 3.62632656097, acc: 0.0651939660311\n",
      "Epoch: 70, loss: 3.62326979637, acc: 0.0678879320621\n",
      "Val: 0.03125\n",
      "Epoch: 71, loss: 3.60898566246, acc: 0.0614224150777\n",
      "Epoch: 72, loss: 3.59423351288, acc: 0.0738146528602\n",
      "Epoch: 73, loss: 3.60756158829, acc: 0.0646551698446\n",
      "Epoch: 74, loss: 3.58324456215, acc: 0.084051720798\n",
      "Epoch: 75, loss: 3.59861397743, acc: 0.0813577622175\n",
      "Epoch: 76, loss: 3.58766007423, acc: 0.078125\n",
      "Epoch: 77, loss: 3.57636332512, acc: 0.0818965509534\n",
      "Epoch: 78, loss: 3.54447317123, acc: 0.0883620679379\n",
      "Epoch: 79, loss: 3.55019211769, acc: 0.0905172377825\n",
      "Epoch: 80, loss: 3.54850530624, acc: 0.0770474150777\n",
      "Val: 0.0328663811088\n",
      "Epoch: 81, loss: 3.55048418045, acc: 0.0905172377825\n",
      "Epoch: 82, loss: 3.52534985542, acc: 0.0905172377825\n",
      "Epoch: 83, loss: 3.54287171364, acc: 0.0818965509534\n",
      "Epoch: 84, loss: 3.57050395012, acc: 0.0775862038136\n",
      "Epoch: 85, loss: 3.52668166161, acc: 0.0883620679379\n",
      "Epoch: 86, loss: 3.51212239265, acc: 0.0894396528602\n",
      "Epoch: 87, loss: 3.50513148308, acc: 0.09375\n",
      "Epoch: 88, loss: 3.50164628029, acc: 0.0889008641243\n",
      "Epoch: 89, loss: 3.48870706558, acc: 0.0910560339689\n",
      "Epoch: 90, loss: 3.48208189011, acc: 0.103448279202\n",
      "Val: 0.0377155169845\n",
      "Epoch: 91, loss: 3.46522068977, acc: 0.109913796186\n",
      "Epoch: 92, loss: 3.48440480232, acc: 0.100215516984\n",
      "Epoch: 93, loss: 3.47311115265, acc: 0.101293101907\n",
      "Epoch: 94, loss: 3.442248106, acc: 0.0964439660311\n",
      "Epoch: 95, loss: 3.45422506332, acc: 0.101293101907\n",
      "Epoch: 96, loss: 3.43714213371, acc: 0.107219830155\n",
      "Epoch: 97, loss: 3.4223587513, acc: 0.114762932062\n",
      "Epoch: 98, loss: 3.43252205849, acc: 0.103987067938\n",
      "Epoch: 99, loss: 3.41681814194, acc: 0.101293101907\n",
      "Epoch: 100, loss: 3.40735816956, acc: 0.112607762218\n",
      "Val: 0.042025860399\n",
      "Epoch: 101, loss: 3.42930269241, acc: 0.10506465286\n",
      "Epoch: 102, loss: 3.38180279732, acc: 0.12068965286\n",
      "Epoch: 103, loss: 3.38770604134, acc: 0.115301720798\n",
      "Epoch: 104, loss: 3.36377120018, acc: 0.118534483016\n",
      "Epoch: 105, loss: 3.37991833687, acc: 0.119073279202\n",
      "Epoch: 106, loss: 3.35904979706, acc: 0.115840516984\n",
      "Epoch: 107, loss: 3.36240029335, acc: 0.119612067938\n",
      "Epoch: 108, loss: 3.35090136528, acc: 0.119073279202\n",
      "Epoch: 109, loss: 3.33987212181, acc: 0.123922415078\n",
      "Epoch: 110, loss: 3.32077288628, acc: 0.119612067938\n",
      "Val: 0.036099139601\n",
      "Epoch: 111, loss: 3.31307482719, acc: 0.129849135876\n",
      "Epoch: 112, loss: 3.27208614349, acc: 0.139547407627\n",
      "Epoch: 113, loss: 3.25114703178, acc: 0.138469830155\n",
      "Epoch: 114, loss: 3.26541113853, acc: 0.146551728249\n",
      "Epoch: 115, loss: 3.2255012989, acc: 0.148706898093\n",
      "Epoch: 116, loss: 3.22143769264, acc: 0.153017237782\n",
      "Epoch: 117, loss: 3.23751854897, acc: 0.153556033969\n",
      "Epoch: 118, loss: 3.23353552818, acc: 0.154633626342\n",
      "Epoch: 119, loss: 3.20939683914, acc: 0.144396558404\n",
      "Epoch: 120, loss: 3.17175030708, acc: 0.169719830155\n",
      "Val: 0.0247844830155\n",
      "Epoch: 121, loss: 3.19252729416, acc: 0.14924569428\n",
      "Epoch: 122, loss: 3.17447733879, acc: 0.176724135876\n",
      "Epoch: 123, loss: 3.21844387054, acc: 0.159482762218\n",
      "Epoch: 124, loss: 3.15593266487, acc: 0.167564660311\n",
      "Epoch: 125, loss: 3.16314387321, acc: 0.172413796186\n",
      "Epoch: 126, loss: 3.15452933311, acc: 0.170258626342\n",
      "Epoch: 127, loss: 3.09742999077, acc: 0.18049569428\n",
      "Epoch: 128, loss: 3.13100719452, acc: 0.171875\n",
      "Epoch: 129, loss: 3.10082912445, acc: 0.182112067938\n",
      "Epoch: 130, loss: 3.05778312683, acc: 0.186961203814\n",
      "Val: 0.0323275849223\n",
      "Epoch: 131, loss: 3.04736256599, acc: 0.190193966031\n",
      "Epoch: 132, loss: 3.0203499794, acc: 0.186961203814\n",
      "Epoch: 133, loss: 3.02896475792, acc: 0.193426728249\n",
      "Epoch: 134, loss: 3.03846955299, acc: 0.198814660311\n",
      "Epoch: 135, loss: 2.93962335587, acc: 0.217672407627\n",
      "Epoch: 136, loss: 3.01330971718, acc: 0.201508626342\n",
      "Epoch: 137, loss: 2.99910640717, acc: 0.202586203814\n",
      "Epoch: 138, loss: 2.99281740189, acc: 0.211206898093\n",
      "Epoch: 139, loss: 2.96507287025, acc: 0.202047407627\n",
      "Epoch: 140, loss: 2.95811200142, acc: 0.218211203814\n",
      "Val: 0.0409482754767\n",
      "Epoch: 141, loss: 2.93247580528, acc: 0.21875\n",
      "Epoch: 142, loss: 2.88368082047, acc: 0.233836203814\n",
      "Epoch: 143, loss: 2.87803578377, acc: 0.228448271751\n",
      "Epoch: 144, loss: 2.8663084507, acc: 0.247844830155\n",
      "Epoch: 145, loss: 2.8809440136, acc: 0.228448271751\n",
      "Epoch: 146, loss: 2.8903696537, acc: 0.245689660311\n",
      "Epoch: 147, loss: 2.87353301048, acc: 0.240301728249\n",
      "Epoch: 148, loss: 2.84023356438, acc: 0.237068966031\n",
      "Epoch: 149, loss: 2.83867073059, acc: 0.238685339689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150, loss: 2.85848760605, acc: 0.238146558404\n",
      "Val: 0.0393318980932\n",
      "Epoch: 151, loss: 2.78147721291, acc: 0.254849135876\n",
      "Epoch: 152, loss: 2.75659704208, acc: 0.26831895113\n",
      "Epoch: 153, loss: 2.78502392769, acc: 0.261853456497\n",
      "Epoch: 154, loss: 2.76608371735, acc: 0.257004320621\n",
      "Epoch: 155, loss: 2.7260863781, acc: 0.259159475565\n",
      "Epoch: 156, loss: 2.76397657394, acc: 0.260237067938\n",
      "Epoch: 157, loss: 2.71518540382, acc: 0.281788796186\n",
      "Epoch: 158, loss: 2.67981314659, acc: 0.28125\n",
      "Epoch: 159, loss: 2.65944933891, acc: 0.304418116808\n",
      "Epoch: 160, loss: 2.61624121666, acc: 0.285560339689\n",
      "Val: 0.036099139601\n",
      "Epoch: 161, loss: 2.61343979836, acc: 0.304418116808\n",
      "Epoch: 162, loss: 2.62194061279, acc: 0.297952592373\n",
      "Epoch: 163, loss: 2.56215238571, acc: 0.311961203814\n",
      "Epoch: 164, loss: 2.59094023705, acc: 0.306573271751\n",
      "Epoch: 165, loss: 2.56906914711, acc: 0.316271543503\n",
      "Epoch: 166, loss: 2.58695650101, acc: 0.313577592373\n",
      "Epoch: 167, loss: 2.48215961456, acc: 0.327047407627\n",
      "Epoch: 168, loss: 2.55144071579, acc: 0.324892252684\n",
      "Epoch: 169, loss: 2.44240736961, acc: 0.346982747316\n",
      "Epoch: 170, loss: 2.50471019745, acc: 0.329741388559\n",
      "Val: 0.0463362075388\n",
      "Epoch: 171, loss: 2.49044656754, acc: 0.329741388559\n",
      "Epoch: 172, loss: 2.44162583351, acc: 0.346982747316\n",
      "Epoch: 173, loss: 2.40532207489, acc: 0.350754320621\n",
      "Epoch: 174, loss: 2.44879102707, acc: 0.342133611441\n",
      "Epoch: 175, loss: 2.45894932747, acc: 0.342133611441\n",
      "Epoch: 176, loss: 2.38410043716, acc: 0.355064660311\n",
      "Epoch: 177, loss: 2.32495188713, acc: 0.37769395113\n",
      "Epoch: 178, loss: 2.37590694427, acc: 0.358836203814\n",
      "Epoch: 179, loss: 2.34413576126, acc: 0.366918116808\n",
      "Epoch: 180, loss: 2.26800394058, acc: 0.394935339689\n",
      "Val: 0.0366379320621\n",
      "Epoch: 181, loss: 2.37882184982, acc: 0.358836203814\n",
      "Epoch: 182, loss: 2.30149316788, acc: 0.382004320621\n",
      "Epoch: 183, loss: 2.26581382751, acc: 0.391702592373\n",
      "Epoch: 184, loss: 2.24403214455, acc: 0.389547407627\n",
      "Epoch: 185, loss: 2.21717047691, acc: 0.413793116808\n",
      "Epoch: 186, loss: 2.14397358894, acc: 0.421336203814\n",
      "Epoch: 187, loss: 2.19185972214, acc: 0.404633611441\n",
      "Epoch: 188, loss: 2.21323299408, acc: 0.410560339689\n",
      "Epoch: 189, loss: 2.13917255402, acc: 0.407327592373\n",
      "Epoch: 190, loss: 2.11521482468, acc: 0.433189660311\n",
      "Val: 0.0409482754767\n",
      "Epoch: 191, loss: 2.13446831703, acc: 0.427262932062\n",
      "Epoch: 192, loss: 2.08537554741, acc: 0.432112067938\n",
      "Epoch: 193, loss: 2.02869820595, acc: 0.452047407627\n",
      "Epoch: 194, loss: 2.09502840042, acc: 0.443426728249\n",
      "Epoch: 195, loss: 2.09456133842, acc: 0.432112067938\n",
      "Epoch: 196, loss: 1.99644291401, acc: 0.464978456497\n",
      "Epoch: 197, loss: 2.00656962395, acc: 0.460129320621\n",
      "Epoch: 198, loss: 1.98402690887, acc: 0.45581895113\n",
      "Epoch: 199, loss: 1.92625546455, acc: 0.470366388559\n",
      "Epoch: 200, loss: 1.98149502277, acc: 0.459051728249\n",
      "Val: 0.0355603434145\n",
      "Epoch: 201, loss: 2.00155234337, acc: 0.463362067938\n",
      "Epoch: 202, loss: 1.91387784481, acc: 0.47144395113\n",
      "Epoch: 203, loss: 1.9789506197, acc: 0.463362067938\n",
      "Epoch: 204, loss: 1.91348791122, acc: 0.46875\n",
      "Epoch: 205, loss: 1.89163923264, acc: 0.480603456497\n",
      "Epoch: 206, loss: 1.85585737228, acc: 0.498922407627\n",
      "Epoch: 207, loss: 1.82636415958, acc: 0.500538766384\n",
      "Epoch: 208, loss: 1.74463427067, acc: 0.519396543503\n",
      "Epoch: 209, loss: 1.78748607635, acc: 0.511853456497\n",
      "Epoch: 210, loss: 1.75725924969, acc: 0.524245679379\n",
      "Val: 0.0452586188912\n",
      "Epoch: 211, loss: 1.8400952816, acc: 0.501077592373\n",
      "Epoch: 212, loss: 1.75661540031, acc: 0.517241358757\n",
      "Epoch: 213, loss: 1.79626178741, acc: 0.517241358757\n",
      "Epoch: 214, loss: 1.76188158989, acc: 0.517780184746\n",
      "Epoch: 215, loss: 1.69564926624, acc: 0.542564630508\n",
      "Epoch: 216, loss: 1.62587428093, acc: 0.564116358757\n",
      "Epoch: 217, loss: 1.64681196213, acc: 0.561422407627\n",
      "Epoch: 218, loss: 1.71181082726, acc: 0.52586209774\n",
      "Epoch: 219, loss: 1.66252827644, acc: 0.550646543503\n",
      "Epoch: 220, loss: 1.58503901958, acc: 0.567349135876\n",
      "Val: 0.0404094830155\n",
      "Epoch: 221, loss: 1.58220589161, acc: 0.564655184746\n",
      "Epoch: 222, loss: 1.5954580307, acc: 0.563038766384\n",
      "Epoch: 223, loss: 1.52902197838, acc: 0.580280184746\n",
      "Epoch: 224, loss: 1.58670341969, acc: 0.55980604887\n",
      "Epoch: 225, loss: 1.51930069923, acc: 0.578663766384\n",
      "Epoch: 226, loss: 1.54269611835, acc: 0.575969815254\n",
      "Epoch: 227, loss: 1.48116767406, acc: 0.593211233616\n",
      "Epoch: 228, loss: 1.50651097298, acc: 0.576508641243\n",
      "Epoch: 229, loss: 1.51301658154, acc: 0.585668087006\n",
      "Epoch: 230, loss: 1.52430474758, acc: 0.58351290226\n",
      "Val: 0.046875\n",
      "Epoch: 231, loss: 1.41066789627, acc: 0.61206895113\n",
      "Epoch: 232, loss: 1.43390250206, acc: 0.59644395113\n",
      "Epoch: 233, loss: 1.41873717308, acc: 0.601831912994\n",
      "Epoch: 234, loss: 1.33166348934, acc: 0.62769395113\n",
      "Epoch: 235, loss: 1.45206797123, acc: 0.582974135876\n",
      "Epoch: 236, loss: 1.40778696537, acc: 0.61206895113\n",
      "Epoch: 237, loss: 1.36106789112, acc: 0.633081912994\n",
      "Epoch: 238, loss: 1.3557895422, acc: 0.620150864124\n",
      "Epoch: 239, loss: 1.35201597214, acc: 0.623383641243\n",
      "Epoch: 240, loss: 1.34853756428, acc: 0.62230604887\n",
      "Val: 0.0387931019068\n",
      "Epoch: 241, loss: 1.40767896175, acc: 0.61476290226\n",
      "Epoch: 242, loss: 1.28894138336, acc: 0.628232777119\n",
      "Epoch: 243, loss: 1.28330218792, acc: 0.63793104887\n",
      "Epoch: 244, loss: 1.27957522869, acc: 0.634698271751\n",
      "Epoch: 245, loss: 1.19604313374, acc: 0.654633641243\n",
      "Epoch: 246, loss: 1.26231729984, acc: 0.649245679379\n",
      "Epoch: 247, loss: 1.20763790607, acc: 0.647629320621\n",
      "Epoch: 248, loss: 1.13175725937, acc: 0.685344815254\n",
      "Epoch: 249, loss: 1.17320919037, acc: 0.66648709774\n",
      "Epoch: 250, loss: 1.13429689407, acc: 0.671336233616\n",
      "Val: 0.0431034490466\n",
      "Epoch: 251, loss: 1.24327266216, acc: 0.657327592373\n",
      "Epoch: 252, loss: 1.16783046722, acc: 0.675646543503\n",
      "Epoch: 253, loss: 1.17903876305, acc: 0.660560369492\n",
      "Epoch: 254, loss: 1.11505413055, acc: 0.678340494633\n",
      "Epoch: 255, loss: 1.08823788166, acc: 0.696120679379\n",
      "Epoch: 256, loss: 1.16286635399, acc: 0.670797407627\n",
      "Epoch: 257, loss: 1.07760047913, acc: 0.698814630508\n",
      "Epoch: 258, loss: 1.06625282764, acc: 0.686422407627\n",
      "Epoch: 259, loss: 1.0064637661, acc: 0.696659505367\n",
      "Epoch: 260, loss: 1.08217680454, acc: 0.692349135876\n",
      "Val: 0.0398706905544\n",
      "Epoch: 261, loss: 1.09555757046, acc: 0.688038766384\n",
      "Epoch: 262, loss: 0.970892846584, acc: 0.716594815254\n",
      "Epoch: 263, loss: 1.05823898315, acc: 0.686422407627\n",
      "Epoch: 264, loss: 0.999227285385, acc: 0.717133641243\n",
      "Epoch: 265, loss: 0.986047506332, acc: 0.713900864124\n",
      "Epoch: 266, loss: 1.00406837463, acc: 0.701508641243\n",
      "Epoch: 267, loss: 0.958729445934, acc: 0.723060369492\n",
      "Epoch: 268, loss: 1.04719829559, acc: 0.702586233616\n",
      "Epoch: 269, loss: 0.9926623106, acc: 0.717133641243\n",
      "Epoch: 270, loss: 0.971967458725, acc: 0.715517222881\n",
      "Val: 0.0323275849223\n",
      "Epoch: 271, loss: 0.943737447262, acc: 0.735991358757\n",
      "Epoch: 272, loss: 0.967151880264, acc: 0.729525864124\n",
      "Epoch: 273, loss: 0.889718770981, acc: 0.73976290226\n",
      "Epoch: 274, loss: 0.878185987473, acc: 0.74730604887\n",
      "Epoch: 275, loss: 0.86494410038, acc: 0.754310369492\n",
      "Epoch: 276, loss: 0.91061258316, acc: 0.741918087006\n",
      "Epoch: 277, loss: 0.89576202631, acc: 0.748383641243\n",
      "Epoch: 278, loss: 0.860204935074, acc: 0.754310369492\n",
      "Epoch: 279, loss: 0.823872685432, acc: 0.759698271751\n",
      "Epoch: 280, loss: 0.946460783482, acc: 0.740301728249\n",
      "Val: 0.0398706905544\n",
      "Epoch: 281, loss: 0.864662110806, acc: 0.752155184746\n",
      "Epoch: 282, loss: 0.770266830921, acc: 0.771551728249\n",
      "Epoch: 283, loss: 0.797202527523, acc: 0.757543087006\n",
      "Epoch: 284, loss: 0.768755376339, acc: 0.77101290226\n",
      "Epoch: 285, loss: 0.81958335638, acc: 0.764547407627\n",
      "Epoch: 286, loss: 0.752475678921, acc: 0.782866358757\n",
      "Epoch: 287, loss: 0.813581645489, acc: 0.766163766384\n",
      "Epoch: 288, loss: 0.755098462105, acc: 0.774784505367\n",
      "Epoch: 289, loss: 0.748914778233, acc: 0.787715494633\n",
      "Epoch: 290, loss: 0.740191280842, acc: 0.78394395113\n",
      "Val: 0.0409482754767\n",
      "Epoch: 291, loss: 0.743280589581, acc: 0.772629320621\n",
      "Epoch: 292, loss: 0.728455603123, acc: 0.782327592373\n",
      "Epoch: 293, loss: 0.727669894695, acc: 0.782327592373\n",
      "Epoch: 294, loss: 0.736358463764, acc: 0.780172407627\n",
      "Epoch: 295, loss: 0.829052150249, acc: 0.766702592373\n",
      "Epoch: 296, loss: 0.715581059456, acc: 0.789870679379\n",
      "Epoch: 297, loss: 0.693424642086, acc: 0.792025864124\n",
      "Epoch: 298, loss: 0.72868937254, acc: 0.784482777119\n",
      "Epoch: 299, loss: 0.680323243141, acc: 0.787715494633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300, loss: 0.669840395451, acc: 0.802801728249\n",
      "Val: 0.0393318980932\n",
      "Epoch: 301, loss: 0.696126818657, acc: 0.793103456497\n",
      "Epoch: 302, loss: 0.726688623428, acc: 0.788254320621\n",
      "Epoch: 303, loss: 0.644797682762, acc: 0.806573271751\n",
      "Epoch: 304, loss: 0.70049148798, acc: 0.803340494633\n",
      "Epoch: 305, loss: 0.665074050426, acc: 0.807650864124\n",
      "Epoch: 306, loss: 0.620556354523, acc: 0.801724135876\n",
      "Epoch: 307, loss: 0.644273638725, acc: 0.810344815254\n",
      "Epoch: 308, loss: 0.681252539158, acc: 0.801185369492\n",
      "Epoch: 309, loss: 0.595883905888, acc: 0.834051728249\n",
      "Epoch: 310, loss: 0.608031749725, acc: 0.821120679379\n",
      "Val: 0.0398706905544\n",
      "Epoch: 311, loss: 0.660578608513, acc: 0.806573271751\n",
      "Epoch: 312, loss: 0.607635557652, acc: 0.823275864124\n",
      "Epoch: 313, loss: 0.594187378883, acc: 0.816810369492\n",
      "Epoch: 314, loss: 0.598370909691, acc: 0.827047407627\n",
      "Epoch: 315, loss: 0.575464129448, acc: 0.829741358757\n",
      "Epoch: 316, loss: 0.59504109621, acc: 0.822198271751\n",
      "Epoch: 317, loss: 0.596738874912, acc: 0.826508641243\n",
      "Epoch: 318, loss: 0.656593441963, acc: 0.805495679379\n",
      "Epoch: 319, loss: 0.585210084915, acc: 0.826508641243\n",
      "Epoch: 320, loss: 0.569097161293, acc: 0.837823271751\n",
      "Val: 0.0431034490466\n",
      "Epoch: 321, loss: 0.562928438187, acc: 0.831357777119\n",
      "Epoch: 322, loss: 0.563919186592, acc: 0.837284505367\n",
      "Epoch: 323, loss: 0.580288410187, acc: 0.823814630508\n",
      "Epoch: 324, loss: 0.503871142864, acc: 0.839439630508\n",
      "Epoch: 325, loss: 0.584533452988, acc: 0.81519395113\n",
      "Epoch: 326, loss: 0.54654276371, acc: 0.83836209774\n",
      "Epoch: 327, loss: 0.571501374245, acc: 0.83351290226\n",
      "Epoch: 328, loss: 0.602439761162, acc: 0.825969815254\n",
      "Epoch: 329, loss: 0.556594312191, acc: 0.836745679379\n",
      "Epoch: 330, loss: 0.535508930683, acc: 0.848599135876\n",
      "Val: 0.0431034490466\n",
      "Epoch: 331, loss: 0.487812370062, acc: 0.848060369492\n",
      "Epoch: 332, loss: 0.469875156879, acc: 0.850215494633\n",
      "Epoch: 333, loss: 0.466494560242, acc: 0.853448271751\n",
      "Epoch: 334, loss: 0.474575638771, acc: 0.857758641243\n",
      "Epoch: 335, loss: 0.523990154266, acc: 0.841594815254\n",
      "Epoch: 336, loss: 0.503201603889, acc: 0.853448271751\n",
      "Epoch: 337, loss: 0.534630298615, acc: 0.839439630508\n",
      "Epoch: 338, loss: 0.472746282816, acc: 0.856142222881\n",
      "Epoch: 339, loss: 0.482036828995, acc: 0.855603456497\n",
      "Epoch: 340, loss: 0.512732386589, acc: 0.83836209774\n",
      "Val: 0.0366379320621\n",
      "Epoch: 341, loss: 0.54327994585, acc: 0.849676728249\n",
      "Epoch: 342, loss: 0.477607905865, acc: 0.858836233616\n",
      "Epoch: 343, loss: 0.50537031889, acc: 0.845366358757\n",
      "Epoch: 344, loss: 0.48217445612, acc: 0.857219815254\n",
      "Epoch: 345, loss: 0.475808084011, acc: 0.84913790226\n",
      "Epoch: 346, loss: 0.477063387632, acc: 0.847521543503\n",
      "Epoch: 347, loss: 0.450192540884, acc: 0.86206895113\n",
      "Epoch: 348, loss: 0.524587392807, acc: 0.848599135876\n",
      "Epoch: 349, loss: 0.448493003845, acc: 0.866379320621\n",
      "Epoch: 350, loss: 0.503193259239, acc: 0.84644395113\n",
      "Val: 0.0409482754767\n",
      "Epoch: 351, loss: 0.411600649357, acc: 0.877155184746\n",
      "Epoch: 352, loss: 0.413908421993, acc: 0.873383641243\n",
      "Epoch: 353, loss: 0.402580559254, acc: 0.883620679379\n",
      "Epoch: 354, loss: 0.451818346977, acc: 0.858836233616\n",
      "Epoch: 355, loss: 0.458723157644, acc: 0.860991358757\n",
      "Epoch: 356, loss: 0.440005600452, acc: 0.865301728249\n",
      "Epoch: 357, loss: 0.438256084919, acc: 0.864224135876\n",
      "Epoch: 358, loss: 0.380408942699, acc: 0.880926728249\n",
      "Epoch: 359, loss: 0.40052691102, acc: 0.873383641243\n",
      "Epoch: 360, loss: 0.412954598665, acc: 0.873383641243\n",
      "Val: 0.0371767245233\n",
      "Epoch: 361, loss: 0.440109431744, acc: 0.877155184746\n",
      "Epoch: 362, loss: 0.435393929482, acc: 0.86206895113\n",
      "Epoch: 363, loss: 0.375082284212, acc: 0.880926728249\n",
      "Epoch: 364, loss: 0.36891618371, acc: 0.884159505367\n",
      "Epoch: 365, loss: 0.420200556517, acc: 0.88038790226\n",
      "Epoch: 366, loss: 0.408198446035, acc: 0.873383641243\n",
      "Epoch: 367, loss: 0.387061417103, acc: 0.874461233616\n",
      "Epoch: 368, loss: 0.378589093685, acc: 0.885775864124\n",
      "Epoch: 369, loss: 0.4107196033, acc: 0.883620679379\n",
      "Epoch: 370, loss: 0.373690903187, acc: 0.883081912994\n",
      "Val: 0.0404094830155\n",
      "Epoch: 371, loss: 0.331011265516, acc: 0.90086209774\n",
      "Epoch: 372, loss: 0.414951503277, acc: 0.87769395113\n",
      "Epoch: 373, loss: 0.407765895128, acc: 0.882004320621\n",
      "Epoch: 374, loss: 0.354113906622, acc: 0.88793104887\n",
      "Epoch: 375, loss: 0.327040314674, acc: 0.891163766384\n",
      "Epoch: 376, loss: 0.383795440197, acc: 0.883620679379\n",
      "Epoch: 377, loss: 0.363960385323, acc: 0.884698271751\n",
      "Epoch: 378, loss: 0.362714856863, acc: 0.884159505367\n",
      "Epoch: 379, loss: 0.386691093445, acc: 0.881465494633\n",
      "Epoch: 380, loss: 0.362890303135, acc: 0.889008641243\n",
      "Val: 0.0425646565855\n",
      "Epoch: 381, loss: 0.345872789621, acc: 0.891163766384\n",
      "Epoch: 382, loss: 0.354952275753, acc: 0.88523709774\n",
      "Epoch: 383, loss: 0.362523317337, acc: 0.88523709774\n",
      "Epoch: 384, loss: 0.371243089437, acc: 0.892241358757\n",
      "Epoch: 385, loss: 0.368198603392, acc: 0.884698271751\n",
      "Epoch: 386, loss: 0.378421753645, acc: 0.884698271751\n",
      "Epoch: 387, loss: 0.375673681498, acc: 0.885775864124\n",
      "Epoch: 388, loss: 0.324445217848, acc: 0.897629320621\n",
      "Epoch: 389, loss: 0.360266506672, acc: 0.89331895113\n",
      "Epoch: 390, loss: 0.316322296858, acc: 0.905172407627\n",
      "Val: 0.0474137924612\n",
      "Epoch: 391, loss: 0.385239422321, acc: 0.890625\n",
      "Epoch: 392, loss: 0.365754306316, acc: 0.891163766384\n",
      "Epoch: 393, loss: 0.302170664072, acc: 0.90355604887\n",
      "Epoch: 394, loss: 0.320580661297, acc: 0.907866358757\n",
      "Epoch: 395, loss: 0.299895256758, acc: 0.91163790226\n",
      "Epoch: 396, loss: 0.280414402485, acc: 0.917564630508\n",
      "Epoch: 397, loss: 0.331196039915, acc: 0.902478456497\n",
      "Epoch: 398, loss: 0.351118296385, acc: 0.894935369492\n",
      "Epoch: 399, loss: 0.29188221693, acc: 0.908405184746\n",
      "Epoch: 400, loss: 0.316618710756, acc: 0.904633641243\n",
      "Val: 0.0387931019068\n",
      "Epoch: 401, loss: 0.347585767508, acc: 0.894396543503\n",
      "Epoch: 402, loss: 0.320349425077, acc: 0.908405184746\n",
      "Epoch: 403, loss: 0.307058393955, acc: 0.909482777119\n",
      "Epoch: 404, loss: 0.300453096628, acc: 0.90625\n",
      "Epoch: 405, loss: 0.306293278933, acc: 0.909482777119\n",
      "Epoch: 406, loss: 0.309044659138, acc: 0.907327592373\n",
      "Epoch: 407, loss: 0.3089453578, acc: 0.903017222881\n",
      "Epoch: 408, loss: 0.336101263762, acc: 0.897629320621\n",
      "Epoch: 409, loss: 0.30634534359, acc: 0.897629320621\n",
      "Epoch: 410, loss: 0.284559607506, acc: 0.914331912994\n",
      "Val: 0.0452586188912\n",
      "Epoch: 411, loss: 0.331616640091, acc: 0.899784505367\n",
      "Epoch: 412, loss: 0.284998297691, acc: 0.910560369492\n",
      "Epoch: 413, loss: 0.309018820524, acc: 0.908405184746\n",
      "Epoch: 414, loss: 0.254994541407, acc: 0.922952592373\n",
      "Epoch: 415, loss: 0.272348582745, acc: 0.914870679379\n",
      "Epoch: 416, loss: 0.300016820431, acc: 0.901400864124\n",
      "Epoch: 417, loss: 0.286778271198, acc: 0.910560369492\n",
      "Epoch: 418, loss: 0.277656197548, acc: 0.913793087006\n",
      "Epoch: 419, loss: 0.289051413536, acc: 0.917025864124\n",
      "Epoch: 420, loss: 0.266265511513, acc: 0.91648709774\n",
      "Val: 0.0506465509534\n",
      "Epoch: 421, loss: 0.241722524166, acc: 0.933189630508\n",
      "Epoch: 422, loss: 0.301088005304, acc: 0.912715494633\n",
      "Epoch: 423, loss: 0.284979850054, acc: 0.914870679379\n",
      "Epoch: 424, loss: 0.311222374439, acc: 0.904633641243\n",
      "Epoch: 425, loss: 0.24387934804, acc: 0.913793087006\n",
      "Epoch: 426, loss: 0.236286461353, acc: 0.92456895113\n",
      "Epoch: 427, loss: 0.271841734648, acc: 0.91648709774\n",
      "Epoch: 428, loss: 0.342882603407, acc: 0.892780184746\n",
      "Epoch: 429, loss: 0.304904580116, acc: 0.91648709774\n",
      "Epoch: 430, loss: 0.271969556808, acc: 0.917564630508\n",
      "Val: 0.0463362075388\n",
      "Epoch: 431, loss: 0.316999047995, acc: 0.904094815254\n",
      "Epoch: 432, loss: 0.301255255938, acc: 0.907327592373\n",
      "Epoch: 433, loss: 0.305128157139, acc: 0.907866358757\n",
      "Epoch: 434, loss: 0.271482557058, acc: 0.913793087006\n",
      "Epoch: 435, loss: 0.245360210538, acc: 0.920258641243\n",
      "Epoch: 436, loss: 0.207948356867, acc: 0.934267222881\n",
      "Epoch: 437, loss: 0.232188031077, acc: 0.929956912994\n",
      "Epoch: 438, loss: 0.276578813791, acc: 0.912715494633\n",
      "Epoch: 439, loss: 0.253931343555, acc: 0.926185369492\n",
      "Epoch: 440, loss: 0.272623002529, acc: 0.914331912994\n",
      "Val: 0.0506465509534\n",
      "Epoch: 441, loss: 0.222064062953, acc: 0.930495679379\n",
      "Epoch: 442, loss: 0.23776820302, acc: 0.922413766384\n",
      "Epoch: 443, loss: 0.256739616394, acc: 0.923491358757\n",
      "Epoch: 444, loss: 0.262706518173, acc: 0.922952592373\n",
      "Epoch: 445, loss: 0.237793385983, acc: 0.929956912994\n",
      "Epoch: 446, loss: 0.207861930132, acc: 0.934267222881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 447, loss: 0.235496684909, acc: 0.92726290226\n",
      "Epoch: 448, loss: 0.266398191452, acc: 0.913793087006\n",
      "Epoch: 449, loss: 0.210345119238, acc: 0.93480604887\n",
      "Epoch: 450, loss: 0.240713313222, acc: 0.922413766384\n",
      "Val: 0.0511853434145\n",
      "Epoch: 451, loss: 0.221486732364, acc: 0.93211209774\n",
      "Epoch: 452, loss: 0.23868958652, acc: 0.935344815254\n",
      "Epoch: 453, loss: 0.25883731246, acc: 0.925107777119\n",
      "Epoch: 454, loss: 0.274329334497, acc: 0.91918104887\n",
      "Epoch: 455, loss: 0.212261974812, acc: 0.93480604887\n",
      "Epoch: 456, loss: 0.254715830088, acc: 0.918642222881\n",
      "Epoch: 457, loss: 0.236626416445, acc: 0.924030184746\n",
      "Epoch: 458, loss: 0.241201147437, acc: 0.92726290226\n",
      "Epoch: 459, loss: 0.234390720725, acc: 0.925107777119\n",
      "Epoch: 460, loss: 0.202315837145, acc: 0.9375\n",
      "Val: 0.0436422415078\n",
      "Epoch: 461, loss: 0.239998966455, acc: 0.929956912994\n",
      "Epoch: 462, loss: 0.218117281795, acc: 0.933728456497\n",
      "Epoch: 463, loss: 0.31380340457, acc: 0.90355604887\n",
      "Epoch: 464, loss: 0.219379365444, acc: 0.933728456497\n",
      "Epoch: 465, loss: 0.255638003349, acc: 0.921875\n",
      "Epoch: 466, loss: 0.244425043464, acc: 0.929956912994\n",
      "Epoch: 467, loss: 0.219264477491, acc: 0.933189630508\n",
      "Epoch: 468, loss: 0.221270576119, acc: 0.927801728249\n",
      "Epoch: 469, loss: 0.253936052322, acc: 0.920797407627\n",
      "Epoch: 470, loss: 0.260877162218, acc: 0.917564630508\n",
      "Val: 0.0484913811088\n",
      "Epoch: 471, loss: 0.21944013238, acc: 0.933189630508\n",
      "Epoch: 472, loss: 0.214159786701, acc: 0.934267222881\n",
      "Epoch: 473, loss: 0.233977675438, acc: 0.928879320621\n",
      "Epoch: 474, loss: 0.232005953789, acc: 0.933728456497\n",
      "Epoch: 475, loss: 0.250883579254, acc: 0.921875\n",
      "Epoch: 476, loss: 0.213769376278, acc: 0.93480604887\n",
      "Epoch: 477, loss: 0.241038545966, acc: 0.928879320621\n",
      "Epoch: 478, loss: 0.218875795603, acc: 0.931034505367\n",
      "Epoch: 479, loss: 0.221892774105, acc: 0.933189630508\n",
      "Epoch: 480, loss: 0.219041392207, acc: 0.929956912994\n",
      "Val: 0.0501077584922\n",
      "Epoch: 481, loss: 0.209060817957, acc: 0.93480604887\n",
      "Epoch: 482, loss: 0.192651748657, acc: 0.94019395113\n",
      "Epoch: 483, loss: 0.197975039482, acc: 0.938038766384\n",
      "Epoch: 484, loss: 0.271862417459, acc: 0.925646543503\n",
      "Epoch: 485, loss: 0.198895931244, acc: 0.942349135876\n",
      "Epoch: 486, loss: 0.183910623193, acc: 0.94288790226\n",
      "Epoch: 487, loss: 0.220860615373, acc: 0.928879320621\n",
      "Epoch: 488, loss: 0.229711160064, acc: 0.922952592373\n",
      "Epoch: 489, loss: 0.265910804272, acc: 0.920258641243\n",
      "Epoch: 490, loss: 0.205029547215, acc: 0.93480604887\n",
      "Val: 0.0581896565855\n",
      "Epoch: 491, loss: 0.217084467411, acc: 0.9375\n",
      "Epoch: 492, loss: 0.21599522233, acc: 0.935883641243\n",
      "Epoch: 493, loss: 0.208168447018, acc: 0.932650864124\n",
      "Epoch: 494, loss: 0.186341658235, acc: 0.940732777119\n",
      "Epoch: 495, loss: 0.196743220091, acc: 0.942349135876\n",
      "Epoch: 496, loss: 0.196569100022, acc: 0.93211209774\n",
      "Epoch: 497, loss: 0.228183284402, acc: 0.929418087006\n",
      "Epoch: 498, loss: 0.213034749031, acc: 0.93211209774\n",
      "Epoch: 499, loss: 0.180509373546, acc: 0.94773709774\n",
      "Epoch: 500, loss: 0.198437049985, acc: 0.939655184746\n",
      "Val: 0.0501077584922\n",
      "Epoch: 501, loss: 0.178038284183, acc: 0.938577592373\n",
      "Epoch: 502, loss: 0.188004985452, acc: 0.948814630508\n",
      "Epoch: 503, loss: 0.176322475076, acc: 0.949353456497\n",
      "Epoch: 504, loss: 0.188691526651, acc: 0.9375\n",
      "Epoch: 505, loss: 0.158712252975, acc: 0.946120679379\n",
      "Epoch: 506, loss: 0.211012199521, acc: 0.940732777119\n",
      "Epoch: 507, loss: 0.175457730889, acc: 0.941810369492\n",
      "Epoch: 508, loss: 0.15595510602, acc: 0.945043087006\n",
      "Epoch: 509, loss: 0.166707962751, acc: 0.949892222881\n",
      "Epoch: 510, loss: 0.238210350275, acc: 0.933728456497\n",
      "Val: 0.0371767245233\n",
      "Epoch: 511, loss: 0.167141914368, acc: 0.942349135876\n",
      "Epoch: 512, loss: 0.20156814158, acc: 0.931573271751\n",
      "Epoch: 513, loss: 0.188302949071, acc: 0.938038766384\n",
      "Epoch: 514, loss: 0.205386668444, acc: 0.935883641243\n",
      "Epoch: 515, loss: 0.227217316628, acc: 0.939655184746\n",
      "Epoch: 516, loss: 0.181158229709, acc: 0.942349135876\n",
      "Epoch: 517, loss: 0.187455296516, acc: 0.945043087006\n",
      "Epoch: 518, loss: 0.207483410835, acc: 0.935883641243\n",
      "Epoch: 519, loss: 0.165653437376, acc: 0.949353456497\n",
      "Epoch: 520, loss: 0.186286717653, acc: 0.941810369492\n",
      "Val: 0.0511853434145\n",
      "Epoch: 521, loss: 0.14322322607, acc: 0.95851290226\n",
      "Epoch: 522, loss: 0.144345059991, acc: 0.951508641243\n",
      "Epoch: 523, loss: 0.160577252507, acc: 0.95581895113\n",
      "Epoch: 524, loss: 0.158233582973, acc: 0.947198271751\n",
      "Epoch: 525, loss: 0.217938527465, acc: 0.942349135876\n",
      "Epoch: 526, loss: 0.190863922238, acc: 0.948275864124\n",
      "Epoch: 527, loss: 0.151071965694, acc: 0.949892222881\n",
      "Epoch: 528, loss: 0.15993976593, acc: 0.950969815254\n",
      "Epoch: 529, loss: 0.193203285336, acc: 0.94773709774\n",
      "Epoch: 530, loss: 0.202651813626, acc: 0.939655184746\n",
      "Val: 0.0484913811088\n",
      "Epoch: 531, loss: 0.153008118272, acc: 0.950969815254\n",
      "Epoch: 532, loss: 0.197147980332, acc: 0.94019395113\n",
      "Epoch: 533, loss: 0.148615419865, acc: 0.953663766384\n",
      "Epoch: 534, loss: 0.144301518798, acc: 0.949892222881\n",
      "Epoch: 535, loss: 0.171813622117, acc: 0.945581912994\n",
      "Epoch: 536, loss: 0.163379609585, acc: 0.943965494633\n",
      "Epoch: 537, loss: 0.173059806228, acc: 0.948275864124\n",
      "Epoch: 538, loss: 0.163786232471, acc: 0.945581912994\n",
      "Epoch: 539, loss: 0.169627174735, acc: 0.948275864124\n",
      "Epoch: 540, loss: 0.174015522003, acc: 0.946659505367\n",
      "Val: 0.0554956905544\n",
      "Epoch: 541, loss: 0.161768272519, acc: 0.946659505367\n",
      "Epoch: 542, loss: 0.139324277639, acc: 0.954202592373\n",
      "Epoch: 543, loss: 0.17318867147, acc: 0.948275864124\n",
      "Epoch: 544, loss: 0.196236804128, acc: 0.945043087006\n",
      "Epoch: 545, loss: 0.169277787209, acc: 0.951508641243\n",
      "Epoch: 546, loss: 0.158170208335, acc: 0.946659505367\n",
      "Epoch: 547, loss: 0.145331740379, acc: 0.95581895113\n",
      "Epoch: 548, loss: 0.154855936766, acc: 0.95043104887\n",
      "Epoch: 549, loss: 0.151331588626, acc: 0.95581895113\n",
      "Epoch: 550, loss: 0.161858588457, acc: 0.947198271751\n",
      "Val: 0.0511853434145\n",
      "Epoch: 551, loss: 0.165988728404, acc: 0.948275864124\n",
      "Epoch: 552, loss: 0.189729690552, acc: 0.942349135876\n",
      "Epoch: 553, loss: 0.178290963173, acc: 0.946120679379\n",
      "Epoch: 554, loss: 0.17236289382, acc: 0.94288790226\n",
      "Epoch: 555, loss: 0.162057608366, acc: 0.949353456497\n",
      "Epoch: 556, loss: 0.19581772387, acc: 0.940732777119\n",
      "Epoch: 557, loss: 0.16265116632, acc: 0.944504320621\n",
      "Epoch: 558, loss: 0.191185906529, acc: 0.943965494633\n",
      "Epoch: 559, loss: 0.171249240637, acc: 0.947198271751\n",
      "Epoch: 560, loss: 0.190731495619, acc: 0.936961233616\n",
      "Val: 0.0463362075388\n",
      "Epoch: 561, loss: 0.153166100383, acc: 0.947198271751\n",
      "Epoch: 562, loss: 0.187799140811, acc: 0.95043104887\n",
      "Epoch: 563, loss: 0.152932479978, acc: 0.952586233616\n",
      "Epoch: 564, loss: 0.169173270464, acc: 0.946120679379\n",
      "Epoch: 565, loss: 0.136829003692, acc: 0.956357777119\n",
      "Epoch: 566, loss: 0.15255574882, acc: 0.95851290226\n",
      "Epoch: 567, loss: 0.170479610562, acc: 0.951508641243\n",
      "Epoch: 568, loss: 0.128559932113, acc: 0.956896543503\n",
      "Epoch: 569, loss: 0.128374665976, acc: 0.959590494633\n",
      "Epoch: 570, loss: 0.165826961398, acc: 0.947198271751\n",
      "Val: 0.0414870679379\n",
      "Epoch: 571, loss: 0.170693576336, acc: 0.949353456497\n",
      "Epoch: 572, loss: 0.130536586046, acc: 0.960668087006\n",
      "Epoch: 573, loss: 0.156345531344, acc: 0.953663766384\n",
      "Epoch: 574, loss: 0.17830504477, acc: 0.941810369492\n",
      "Epoch: 575, loss: 0.164453133941, acc: 0.948275864124\n",
      "Epoch: 576, loss: 0.125028952956, acc: 0.960129320621\n",
      "Epoch: 577, loss: 0.155490621924, acc: 0.94773709774\n",
      "Epoch: 578, loss: 0.17157047987, acc: 0.94773709774\n",
      "Epoch: 579, loss: 0.147364035249, acc: 0.956896543503\n",
      "Epoch: 580, loss: 0.130005046725, acc: 0.960129320621\n",
      "Val: 0.0393318980932\n",
      "Epoch: 581, loss: 0.188127979636, acc: 0.941810369492\n",
      "Epoch: 582, loss: 0.161521285772, acc: 0.948275864124\n",
      "Epoch: 583, loss: 0.131241306663, acc: 0.960129320621\n",
      "Epoch: 584, loss: 0.159904047847, acc: 0.948275864124\n",
      "Epoch: 585, loss: 0.130073204637, acc: 0.961745679379\n",
      "Epoch: 586, loss: 0.154876723886, acc: 0.950969815254\n",
      "Epoch: 587, loss: 0.142355471849, acc: 0.956357777119\n",
      "Epoch: 588, loss: 0.140388458967, acc: 0.96336209774\n",
      "Epoch: 589, loss: 0.146870270371, acc: 0.954202592373\n",
      "Epoch: 590, loss: 0.156759798527, acc: 0.952047407627\n",
      "Val: 0.0506465509534\n",
      "Epoch: 591, loss: 0.148747220635, acc: 0.952047407627\n",
      "Epoch: 592, loss: 0.121027961373, acc: 0.96336209774\n",
      "Epoch: 593, loss: 0.126004844904, acc: 0.957974135876\n",
      "Epoch: 594, loss: 0.16821180284, acc: 0.94773709774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 595, loss: 0.145787462592, acc: 0.952586233616\n",
      "Epoch: 596, loss: 0.150804743171, acc: 0.947198271751\n",
      "Epoch: 597, loss: 0.105947829783, acc: 0.962823271751\n",
      "Epoch: 598, loss: 0.124876432121, acc: 0.959051728249\n",
      "Epoch: 599, loss: 0.161490127444, acc: 0.952047407627\n",
      "Epoch: 600, loss: 0.165263697505, acc: 0.945043087006\n",
      "Val: 0.0414870679379\n",
      "Epoch: 601, loss: 0.152475938201, acc: 0.953125\n",
      "Epoch: 602, loss: 0.144011124969, acc: 0.953125\n",
      "Epoch: 603, loss: 0.112013831735, acc: 0.96336209774\n",
      "Epoch: 604, loss: 0.149423182011, acc: 0.957974135876\n",
      "Epoch: 605, loss: 0.160055607557, acc: 0.952586233616\n",
      "Epoch: 606, loss: 0.135203421116, acc: 0.960129320621\n",
      "Epoch: 607, loss: 0.115299448371, acc: 0.961745679379\n",
      "Epoch: 608, loss: 0.124729000032, acc: 0.957974135876\n",
      "Epoch: 609, loss: 0.137692481279, acc: 0.957974135876\n",
      "Epoch: 610, loss: 0.1497656703, acc: 0.95851290226\n",
      "Val: 0.0490301735699\n",
      "Epoch: 611, loss: 0.160557597876, acc: 0.954741358757\n",
      "Epoch: 612, loss: 0.129519402981, acc: 0.95851290226\n",
      "Epoch: 613, loss: 0.1334425807, acc: 0.955280184746\n",
      "Epoch: 614, loss: 0.123187467456, acc: 0.960668087006\n",
      "Epoch: 615, loss: 0.132452130318, acc: 0.960129320621\n",
      "Epoch: 616, loss: 0.116126865149, acc: 0.960668087006\n",
      "Epoch: 617, loss: 0.144000321627, acc: 0.95581895113\n",
      "Epoch: 618, loss: 0.126003459096, acc: 0.957974135876\n",
      "Epoch: 619, loss: 0.157373771071, acc: 0.955280184746\n",
      "Epoch: 620, loss: 0.138906866312, acc: 0.960129320621\n",
      "Val: 0.0554956905544\n",
      "Epoch: 621, loss: 0.12234698981, acc: 0.964439630508\n",
      "Epoch: 622, loss: 0.140007346869, acc: 0.957435369492\n",
      "Epoch: 623, loss: 0.124078243971, acc: 0.961206912994\n",
      "Epoch: 624, loss: 0.152894824743, acc: 0.954202592373\n",
      "Epoch: 625, loss: 0.106221608818, acc: 0.963900864124\n",
      "Epoch: 626, loss: 0.121365003288, acc: 0.959590494633\n",
      "Epoch: 627, loss: 0.126396611333, acc: 0.95851290226\n",
      "Epoch: 628, loss: 0.124479547143, acc: 0.961745679379\n",
      "Epoch: 629, loss: 0.154460683465, acc: 0.947198271751\n",
      "Epoch: 630, loss: 0.134094417095, acc: 0.961745679379\n",
      "Val: 0.0436422415078\n",
      "Epoch: 631, loss: 0.131985127926, acc: 0.959590494633\n",
      "Epoch: 632, loss: 0.142994210124, acc: 0.955280184746\n",
      "Epoch: 633, loss: 0.120506182313, acc: 0.964978456497\n",
      "Epoch: 634, loss: 0.148360341787, acc: 0.959051728249\n",
      "Epoch: 635, loss: 0.126549124718, acc: 0.964439630508\n",
      "Epoch: 636, loss: 0.117522418499, acc: 0.960668087006\n",
      "Epoch: 637, loss: 0.14566385746, acc: 0.956896543503\n",
      "Epoch: 638, loss: 0.151587709785, acc: 0.951508641243\n",
      "Epoch: 639, loss: 0.0956661105156, acc: 0.967133641243\n",
      "Epoch: 640, loss: 0.0870675221086, acc: 0.970905184746\n",
      "Val: 0.051724139601\n",
      "Epoch: 641, loss: 0.118834607303, acc: 0.962284505367\n",
      "Epoch: 642, loss: 0.121984981, acc: 0.960129320621\n",
      "Epoch: 643, loss: 0.119863659143, acc: 0.960668087006\n",
      "Epoch: 644, loss: 0.139260411263, acc: 0.95581895113\n",
      "Epoch: 645, loss: 0.102924346924, acc: 0.965517222881\n",
      "Epoch: 646, loss: 0.102186575532, acc: 0.962823271751\n",
      "Epoch: 647, loss: 0.139052569866, acc: 0.956357777119\n",
      "Epoch: 648, loss: 0.127514377236, acc: 0.960668087006\n",
      "Epoch: 649, loss: 0.100497178733, acc: 0.967133641243\n",
      "Epoch: 650, loss: 0.135083824396, acc: 0.95851290226\n",
      "Val: 0.0425646565855\n",
      "Epoch: 651, loss: 0.162909135222, acc: 0.949353456497\n",
      "Epoch: 652, loss: 0.11541556567, acc: 0.96336209774\n",
      "Epoch: 653, loss: 0.126325145364, acc: 0.963900864124\n",
      "Epoch: 654, loss: 0.12578471005, acc: 0.95851290226\n",
      "Epoch: 655, loss: 0.15050329268, acc: 0.957435369492\n",
      "Epoch: 656, loss: 0.127514362335, acc: 0.957435369492\n",
      "Epoch: 657, loss: 0.107771813869, acc: 0.963900864124\n",
      "Epoch: 658, loss: 0.0999125763774, acc: 0.968211233616\n",
      "Epoch: 659, loss: 0.139152511954, acc: 0.954202592373\n",
      "Epoch: 660, loss: 0.103505298495, acc: 0.964978456497\n",
      "Val: 0.0479525849223\n",
      "Epoch: 661, loss: 0.121323168278, acc: 0.969827592373\n",
      "Epoch: 662, loss: 0.11147852242, acc: 0.964439630508\n",
      "Epoch: 663, loss: 0.131727606058, acc: 0.95851290226\n",
      "Epoch: 664, loss: 0.110991567373, acc: 0.96336209774\n",
      "Epoch: 665, loss: 0.125024557114, acc: 0.959051728249\n",
      "Epoch: 666, loss: 0.169559985399, acc: 0.954202592373\n",
      "Epoch: 667, loss: 0.149672493339, acc: 0.954741358757\n",
      "Epoch: 668, loss: 0.116495974362, acc: 0.96336209774\n",
      "Epoch: 669, loss: 0.138293460011, acc: 0.953125\n",
      "Epoch: 670, loss: 0.109769836068, acc: 0.965517222881\n",
      "Val: 0.0538793094456\n",
      "Epoch: 671, loss: 0.11674143374, acc: 0.961206912994\n",
      "Epoch: 672, loss: 0.0960030630231, acc: 0.971982777119\n",
      "Epoch: 673, loss: 0.116859532893, acc: 0.960668087006\n",
      "Epoch: 674, loss: 0.136908918619, acc: 0.952586233616\n",
      "Epoch: 675, loss: 0.099245890975, acc: 0.965517222881\n",
      "Epoch: 676, loss: 0.101042024791, acc: 0.967672407627\n",
      "Epoch: 677, loss: 0.127225980163, acc: 0.960668087006\n",
      "Epoch: 678, loss: 0.114710971713, acc: 0.964978456497\n",
      "Epoch: 679, loss: 0.121590398252, acc: 0.960668087006\n",
      "Epoch: 680, loss: 0.114337749779, acc: 0.96605604887\n",
      "Val: 0.0490301735699\n",
      "Epoch: 681, loss: 0.130154415965, acc: 0.960668087006\n",
      "Epoch: 682, loss: 0.108943939209, acc: 0.964978456497\n",
      "Epoch: 683, loss: 0.123157784343, acc: 0.962823271751\n",
      "Epoch: 684, loss: 0.121022157371, acc: 0.962284505367\n",
      "Epoch: 685, loss: 0.115856282413, acc: 0.961745679379\n",
      "Epoch: 686, loss: 0.133833646774, acc: 0.956357777119\n",
      "Epoch: 687, loss: 0.114276789129, acc: 0.966594815254\n",
      "Epoch: 688, loss: 0.109625466168, acc: 0.967133641243\n",
      "Epoch: 689, loss: 0.119884781539, acc: 0.96605604887\n",
      "Epoch: 690, loss: 0.108515180647, acc: 0.966594815254\n",
      "Val: 0.0484913811088\n",
      "Epoch: 691, loss: 0.121380098164, acc: 0.961206912994\n",
      "Epoch: 692, loss: 0.111731462181, acc: 0.963900864124\n",
      "Epoch: 693, loss: 0.0901346355677, acc: 0.970366358757\n",
      "Epoch: 694, loss: 0.125242292881, acc: 0.963900864124\n",
      "Epoch: 695, loss: 0.103065893054, acc: 0.96605604887\n",
      "Epoch: 696, loss: 0.11297237128, acc: 0.963900864124\n",
      "Epoch: 697, loss: 0.140080153942, acc: 0.963900864124\n",
      "Epoch: 698, loss: 0.111583165824, acc: 0.96605604887\n",
      "Epoch: 699, loss: 0.12509252131, acc: 0.96336209774\n",
      "Epoch: 700, loss: 0.137408331037, acc: 0.959590494633\n",
      "Val: 0.051724139601\n",
      "Epoch: 701, loss: 0.11821513623, acc: 0.961206912994\n",
      "Epoch: 702, loss: 0.135398238897, acc: 0.965517222881\n",
      "Epoch: 703, loss: 0.0973974168301, acc: 0.97144395113\n",
      "Epoch: 704, loss: 0.103668354452, acc: 0.959051728249\n",
      "Epoch: 705, loss: 0.100400157273, acc: 0.96605604887\n",
      "Epoch: 706, loss: 0.107775397599, acc: 0.966594815254\n",
      "Epoch: 707, loss: 0.135359168053, acc: 0.956357777119\n",
      "Epoch: 708, loss: 0.102389909327, acc: 0.966594815254\n",
      "Epoch: 709, loss: 0.104416437447, acc: 0.969288766384\n",
      "Epoch: 710, loss: 0.101749181747, acc: 0.968211233616\n",
      "Val: 0.0398706905544\n",
      "Epoch: 711, loss: 0.154314517975, acc: 0.944504320621\n",
      "Epoch: 712, loss: 0.0985265299678, acc: 0.967133641243\n",
      "Epoch: 713, loss: 0.0931304246187, acc: 0.973599135876\n",
      "Epoch: 714, loss: 0.133239969611, acc: 0.960668087006\n",
      "Epoch: 715, loss: 0.10196185112, acc: 0.96605604887\n",
      "Epoch: 716, loss: 0.0939166545868, acc: 0.967672407627\n",
      "Epoch: 717, loss: 0.143886461854, acc: 0.95851290226\n",
      "Epoch: 718, loss: 0.111766561866, acc: 0.96336209774\n",
      "Epoch: 719, loss: 0.101667612791, acc: 0.967133641243\n",
      "Epoch: 720, loss: 0.125595584512, acc: 0.957974135876\n",
      "Val: 0.0522629320621\n",
      "Epoch: 721, loss: 0.100635319948, acc: 0.968211233616\n",
      "Epoch: 722, loss: 0.121532969177, acc: 0.957974135876\n",
      "Epoch: 723, loss: 0.112553656101, acc: 0.957974135876\n",
      "Epoch: 724, loss: 0.088590927422, acc: 0.97144395113\n",
      "Epoch: 725, loss: 0.124236442149, acc: 0.960668087006\n",
      "Epoch: 726, loss: 0.129608258605, acc: 0.964439630508\n",
      "Epoch: 727, loss: 0.12104023248, acc: 0.963900864124\n",
      "Epoch: 728, loss: 0.122105047107, acc: 0.961745679379\n",
      "Epoch: 729, loss: 0.103356823325, acc: 0.964439630508\n",
      "Epoch: 730, loss: 0.0781790465117, acc: 0.979525864124\n",
      "Val: 0.0506465509534\n",
      "Epoch: 731, loss: 0.0973696261644, acc: 0.967133641243\n",
      "Epoch: 732, loss: 0.114775918424, acc: 0.96336209774\n",
      "Epoch: 733, loss: 0.110943496227, acc: 0.961206912994\n",
      "Epoch: 734, loss: 0.100550636649, acc: 0.969288766384\n",
      "Epoch: 735, loss: 0.0979185327888, acc: 0.970366358757\n",
      "Epoch: 736, loss: 0.111029535532, acc: 0.96336209774\n",
      "Epoch: 737, loss: 0.101027838886, acc: 0.969288766384\n",
      "Epoch: 738, loss: 0.0972054973245, acc: 0.970366358757\n",
      "Epoch: 739, loss: 0.0969242304564, acc: 0.968211233616\n",
      "Epoch: 740, loss: 0.098434433341, acc: 0.96875\n",
      "Val: 0.0587284490466\n",
      "Epoch: 741, loss: 0.0900220870972, acc: 0.97144395113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 742, loss: 0.114998623729, acc: 0.964439630508\n",
      "Epoch: 743, loss: 0.0890658125281, acc: 0.970366358757\n",
      "Epoch: 744, loss: 0.0971723347902, acc: 0.967672407627\n",
      "Epoch: 745, loss: 0.10052330792, acc: 0.969288766384\n",
      "Epoch: 746, loss: 0.101630985737, acc: 0.965517222881\n",
      "Epoch: 747, loss: 0.0964543819427, acc: 0.967672407627\n",
      "Epoch: 748, loss: 0.139772713184, acc: 0.956357777119\n",
      "Epoch: 749, loss: 0.0873459130526, acc: 0.971982777119\n",
      "Epoch: 750, loss: 0.096187248826, acc: 0.971982777119\n",
      "Val: 0.0457974150777\n",
      "Epoch: 751, loss: 0.0957023352385, acc: 0.967672407627\n",
      "Epoch: 752, loss: 0.0950485095382, acc: 0.972521543503\n",
      "Epoch: 753, loss: 0.107856795192, acc: 0.967133641243\n",
      "Epoch: 754, loss: 0.104762464762, acc: 0.964439630508\n",
      "Epoch: 755, loss: 0.0954008176923, acc: 0.969827592373\n",
      "Epoch: 756, loss: 0.0797303020954, acc: 0.976293087006\n",
      "Epoch: 757, loss: 0.102856814861, acc: 0.96875\n",
      "Epoch: 758, loss: 0.100798338652, acc: 0.968211233616\n",
      "Epoch: 759, loss: 0.117305755615, acc: 0.963900864124\n",
      "Epoch: 760, loss: 0.133163675666, acc: 0.960668087006\n",
      "Val: 0.0544181019068\n",
      "Epoch: 761, loss: 0.117937959731, acc: 0.96605604887\n",
      "Epoch: 762, loss: 0.119696982205, acc: 0.966594815254\n",
      "Epoch: 763, loss: 0.0999704375863, acc: 0.967672407627\n",
      "Epoch: 764, loss: 0.0928512215614, acc: 0.970366358757\n",
      "Epoch: 765, loss: 0.093747086823, acc: 0.969827592373\n",
      "Epoch: 766, loss: 0.0810952410102, acc: 0.973060369492\n",
      "Epoch: 767, loss: 0.098458699882, acc: 0.965517222881\n",
      "Epoch: 768, loss: 0.124080367386, acc: 0.963900864124\n",
      "Epoch: 769, loss: 0.0928310453892, acc: 0.969827592373\n",
      "Epoch: 770, loss: 0.0757851004601, acc: 0.974676728249\n",
      "Val: 0.0490301735699\n",
      "Epoch: 771, loss: 0.106405943632, acc: 0.964439630508\n",
      "Epoch: 772, loss: 0.0915927663445, acc: 0.969827592373\n",
      "Epoch: 773, loss: 0.0914385318756, acc: 0.967672407627\n",
      "Epoch: 774, loss: 0.0990335345268, acc: 0.968211233616\n",
      "Epoch: 775, loss: 0.0980263501406, acc: 0.969288766384\n",
      "Epoch: 776, loss: 0.105491228402, acc: 0.973060369492\n",
      "Epoch: 777, loss: 0.136298969388, acc: 0.967133641243\n",
      "Epoch: 778, loss: 0.0840955525637, acc: 0.976831912994\n",
      "Epoch: 779, loss: 0.0909963771701, acc: 0.971982777119\n",
      "Epoch: 780, loss: 0.124851308763, acc: 0.966594815254\n",
      "Val: 0.0474137924612\n",
      "Epoch: 781, loss: 0.0726380944252, acc: 0.976831912994\n",
      "Epoch: 782, loss: 0.086775444448, acc: 0.96875\n",
      "Epoch: 783, loss: 0.103950113058, acc: 0.964978456497\n",
      "Epoch: 784, loss: 0.0846217647195, acc: 0.973060369492\n",
      "Epoch: 785, loss: 0.0867996960878, acc: 0.974676728249\n",
      "Epoch: 786, loss: 0.113194637001, acc: 0.96875\n",
      "Epoch: 787, loss: 0.0792978256941, acc: 0.973060369492\n",
      "Epoch: 788, loss: 0.129584550858, acc: 0.961745679379\n",
      "Epoch: 789, loss: 0.0876501873136, acc: 0.971982777119\n",
      "Epoch: 790, loss: 0.0914665088058, acc: 0.971982777119\n",
      "Val: 0.0522629320621\n",
      "Epoch: 791, loss: 0.0868327021599, acc: 0.966594815254\n",
      "Epoch: 792, loss: 0.0991703122854, acc: 0.97144395113\n",
      "Epoch: 793, loss: 0.0889342725277, acc: 0.975754320621\n",
      "Epoch: 794, loss: 0.121043846011, acc: 0.963900864124\n",
      "Epoch: 795, loss: 0.0965935885906, acc: 0.97144395113\n",
      "Epoch: 796, loss: 0.0809338837862, acc: 0.968211233616\n",
      "Epoch: 797, loss: 0.107404284179, acc: 0.965517222881\n",
      "Epoch: 798, loss: 0.0777945816517, acc: 0.972521543503\n",
      "Epoch: 799, loss: 0.0966954007745, acc: 0.972521543503\n",
      "Epoch: 800, loss: 0.091220356524, acc: 0.970366358757\n",
      "Val: 0.0549568980932\n",
      "Epoch: 801, loss: 0.0892082005739, acc: 0.975754320621\n",
      "Epoch: 802, loss: 0.0859425514936, acc: 0.971982777119\n",
      "Epoch: 803, loss: 0.105256289244, acc: 0.969288766384\n",
      "Epoch: 804, loss: 0.0793608799577, acc: 0.970905184746\n",
      "Epoch: 805, loss: 0.108134135604, acc: 0.972521543503\n",
      "Epoch: 806, loss: 0.10773691535, acc: 0.968211233616\n",
      "Epoch: 807, loss: 0.0730130746961, acc: 0.970905184746\n",
      "Epoch: 808, loss: 0.0877898186445, acc: 0.974676728249\n",
      "Epoch: 809, loss: 0.0834062993526, acc: 0.973060369492\n",
      "Epoch: 810, loss: 0.0779643580317, acc: 0.974676728249\n",
      "Val: 0.0463362075388\n",
      "Epoch: 811, loss: 0.105705976486, acc: 0.967133641243\n",
      "Epoch: 812, loss: 0.110833801329, acc: 0.963900864124\n",
      "Epoch: 813, loss: 0.0747281387448, acc: 0.970366358757\n",
      "Epoch: 814, loss: 0.108336739242, acc: 0.964978456497\n",
      "Epoch: 815, loss: 0.100861594081, acc: 0.97144395113\n",
      "Epoch: 816, loss: 0.0858652070165, acc: 0.97144395113\n",
      "Epoch: 817, loss: 0.100492291152, acc: 0.97144395113\n",
      "Epoch: 818, loss: 0.082672432065, acc: 0.973599135876\n",
      "Epoch: 819, loss: 0.0871909484267, acc: 0.967672407627\n",
      "Epoch: 820, loss: 0.0984399244189, acc: 0.96875\n",
      "Val: 0.0441810339689\n",
      "Epoch: 821, loss: 0.0917602479458, acc: 0.975754320621\n",
      "Epoch: 822, loss: 0.0867690369487, acc: 0.972521543503\n",
      "Epoch: 823, loss: 0.0854237377644, acc: 0.970905184746\n",
      "Epoch: 824, loss: 0.0799248963594, acc: 0.976293087006\n",
      "Epoch: 825, loss: 0.0854597911239, acc: 0.969827592373\n",
      "Epoch: 826, loss: 0.0734826102853, acc: 0.979525864124\n",
      "Epoch: 827, loss: 0.0685781762004, acc: 0.979525864124\n",
      "Epoch: 828, loss: 0.0986770540476, acc: 0.971982777119\n",
      "Epoch: 829, loss: 0.0836526826024, acc: 0.974676728249\n",
      "Epoch: 830, loss: 0.0659767314792, acc: 0.98168104887\n",
      "Val: 0.0533405169845\n",
      "Epoch: 831, loss: 0.0810667127371, acc: 0.970905184746\n",
      "Epoch: 832, loss: 0.0889133661985, acc: 0.96875\n",
      "Epoch: 833, loss: 0.0793371126056, acc: 0.975754320621\n",
      "Epoch: 834, loss: 0.101856857538, acc: 0.967672407627\n",
      "Epoch: 835, loss: 0.118766091764, acc: 0.965517222881\n",
      "Epoch: 836, loss: 0.0778107792139, acc: 0.976293087006\n",
      "Epoch: 837, loss: 0.0719647556543, acc: 0.972521543503\n",
      "Epoch: 838, loss: 0.0868257880211, acc: 0.97144395113\n",
      "Epoch: 839, loss: 0.0851053744555, acc: 0.974676728249\n",
      "Epoch: 840, loss: 0.116626098752, acc: 0.962284505367\n",
      "Val: 0.0452586188912\n",
      "Epoch: 841, loss: 0.0996786653996, acc: 0.967133641243\n",
      "Epoch: 842, loss: 0.0820281207561, acc: 0.970366358757\n",
      "Epoch: 843, loss: 0.0844818800688, acc: 0.975754320621\n",
      "Epoch: 844, loss: 0.080170802772, acc: 0.975754320621\n",
      "Epoch: 845, loss: 0.080281034112, acc: 0.97144395113\n",
      "Epoch: 846, loss: 0.106307715178, acc: 0.964439630508\n",
      "Epoch: 847, loss: 0.0937133803964, acc: 0.96875\n",
      "Epoch: 848, loss: 0.104616224766, acc: 0.96336209774\n",
      "Epoch: 849, loss: 0.12236430496, acc: 0.96336209774\n",
      "Epoch: 850, loss: 0.102871835232, acc: 0.966594815254\n",
      "Val: 0.0404094830155\n",
      "Epoch: 851, loss: 0.09001763165, acc: 0.96875\n",
      "Epoch: 852, loss: 0.0846767872572, acc: 0.975215494633\n",
      "Epoch: 853, loss: 0.0689929425716, acc: 0.975215494633\n",
      "Epoch: 854, loss: 0.0833568871021, acc: 0.973060369492\n",
      "Epoch: 855, loss: 0.0707083866, acc: 0.970905184746\n",
      "Epoch: 856, loss: 0.0821501612663, acc: 0.97144395113\n",
      "Epoch: 857, loss: 0.105639711022, acc: 0.964439630508\n",
      "Epoch: 858, loss: 0.103240229189, acc: 0.96605604887\n",
      "Epoch: 859, loss: 0.0715126171708, acc: 0.975754320621\n",
      "Epoch: 860, loss: 0.0539617799222, acc: 0.982758641243\n",
      "Val: 0.0425646565855\n",
      "Epoch: 861, loss: 0.0946697667241, acc: 0.97413790226\n",
      "Epoch: 862, loss: 0.0785942897201, acc: 0.977370679379\n",
      "Epoch: 863, loss: 0.0743082538247, acc: 0.975754320621\n",
      "Epoch: 864, loss: 0.0984539166093, acc: 0.969288766384\n",
      "Epoch: 865, loss: 0.0725770816207, acc: 0.975754320621\n",
      "Epoch: 866, loss: 0.0868699401617, acc: 0.968211233616\n",
      "Epoch: 867, loss: 0.0704603418708, acc: 0.974676728249\n",
      "Epoch: 868, loss: 0.117600433528, acc: 0.967672407627\n",
      "Epoch: 869, loss: 0.11321426183, acc: 0.96875\n",
      "Epoch: 870, loss: 0.0627249851823, acc: 0.975754320621\n",
      "Val: 0.0398706905544\n",
      "Epoch: 871, loss: 0.0906356796622, acc: 0.973599135876\n",
      "Epoch: 872, loss: 0.0983719304204, acc: 0.967133641243\n",
      "Epoch: 873, loss: 0.0641410499811, acc: 0.97898709774\n",
      "Epoch: 874, loss: 0.0876383259892, acc: 0.970366358757\n",
      "Epoch: 875, loss: 0.0590951368213, acc: 0.977370679379\n",
      "Epoch: 876, loss: 0.0979045554996, acc: 0.968211233616\n",
      "Epoch: 877, loss: 0.0681782886386, acc: 0.977909505367\n",
      "Epoch: 878, loss: 0.0840811803937, acc: 0.971982777119\n",
      "Epoch: 879, loss: 0.06822168082, acc: 0.975754320621\n",
      "Epoch: 880, loss: 0.071426063776, acc: 0.97898709774\n",
      "Val: 0.0549568980932\n",
      "Epoch: 881, loss: 0.122974134982, acc: 0.967133641243\n",
      "Epoch: 882, loss: 0.0926615297794, acc: 0.975754320621\n",
      "Epoch: 883, loss: 0.0722097381949, acc: 0.979525864124\n",
      "Epoch: 884, loss: 0.0848120674491, acc: 0.970905184746\n",
      "Epoch: 885, loss: 0.0750744491816, acc: 0.976293087006\n",
      "Epoch: 886, loss: 0.0678952634335, acc: 0.971982777119\n",
      "Epoch: 887, loss: 0.0745304152369, acc: 0.972521543503\n",
      "Epoch: 888, loss: 0.0796302035451, acc: 0.973060369492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 889, loss: 0.0866763070226, acc: 0.97144395113\n",
      "Epoch: 890, loss: 0.0915589332581, acc: 0.96875\n",
      "Val: 0.051724139601\n",
      "Epoch: 891, loss: 0.091717146337, acc: 0.969827592373\n",
      "Epoch: 892, loss: 0.0764416605234, acc: 0.974676728249\n",
      "Epoch: 893, loss: 0.0734575837851, acc: 0.97413790226\n",
      "Epoch: 894, loss: 0.0825768113136, acc: 0.975215494633\n",
      "Epoch: 895, loss: 0.0822642669082, acc: 0.971982777119\n",
      "Epoch: 896, loss: 0.0730016306043, acc: 0.977370679379\n",
      "Epoch: 897, loss: 0.0943477153778, acc: 0.972521543503\n",
      "Epoch: 898, loss: 0.0897889733315, acc: 0.969827592373\n",
      "Epoch: 899, loss: 0.110017605126, acc: 0.970905184746\n",
      "Epoch: 900, loss: 0.0863924920559, acc: 0.966594815254\n",
      "Val: 0.0398706905544\n",
      "Epoch: 901, loss: 0.0879517123103, acc: 0.973599135876\n",
      "Epoch: 902, loss: 0.0886958315969, acc: 0.970905184746\n",
      "Epoch: 903, loss: 0.0774229615927, acc: 0.97413790226\n",
      "Epoch: 904, loss: 0.0703469663858, acc: 0.975754320621\n",
      "Epoch: 905, loss: 0.106263957918, acc: 0.97144395113\n",
      "Epoch: 906, loss: 0.0676475316286, acc: 0.977370679379\n",
      "Epoch: 907, loss: 0.08260846138, acc: 0.971982777119\n",
      "Epoch: 908, loss: 0.0816739350557, acc: 0.976293087006\n",
      "Epoch: 909, loss: 0.0653697103262, acc: 0.978448271751\n",
      "Epoch: 910, loss: 0.0739747881889, acc: 0.977370679379\n",
      "Val: 0.0484913811088\n",
      "Epoch: 911, loss: 0.0791122913361, acc: 0.976293087006\n",
      "Epoch: 912, loss: 0.0683155357838, acc: 0.98168104887\n",
      "Epoch: 913, loss: 0.0862314626575, acc: 0.971982777119\n",
      "Epoch: 914, loss: 0.0779697149992, acc: 0.977370679379\n",
      "Epoch: 915, loss: 0.0713489502668, acc: 0.980603456497\n",
      "Epoch: 916, loss: 0.0936451405287, acc: 0.973599135876\n",
      "Epoch: 917, loss: 0.0623707287014, acc: 0.977909505367\n",
      "Epoch: 918, loss: 0.0794133618474, acc: 0.975754320621\n",
      "Epoch: 919, loss: 0.0724453702569, acc: 0.974676728249\n",
      "Epoch: 920, loss: 0.0617563389242, acc: 0.980603456497\n",
      "Val: 0.0457974150777\n",
      "Epoch: 921, loss: 0.0786284133792, acc: 0.975754320621\n",
      "Epoch: 922, loss: 0.0805950462818, acc: 0.97413790226\n",
      "Epoch: 923, loss: 0.0914799720049, acc: 0.968211233616\n",
      "Epoch: 924, loss: 0.0702187344432, acc: 0.97413790226\n",
      "Epoch: 925, loss: 0.0939610004425, acc: 0.971982777119\n",
      "Epoch: 926, loss: 0.0737605914474, acc: 0.981142222881\n",
      "Epoch: 927, loss: 0.0550954006612, acc: 0.978448271751\n",
      "Epoch: 928, loss: 0.0697998404503, acc: 0.97413790226\n",
      "Epoch: 929, loss: 0.0976431295276, acc: 0.96875\n",
      "Epoch: 930, loss: 0.0691639110446, acc: 0.973060369492\n",
      "Val: 0.0441810339689\n",
      "Epoch: 931, loss: 0.1038909778, acc: 0.969288766384\n",
      "Epoch: 932, loss: 0.0802583098412, acc: 0.97413790226\n",
      "Epoch: 933, loss: 0.0867973268032, acc: 0.972521543503\n",
      "Epoch: 934, loss: 0.0729543939233, acc: 0.975754320621\n",
      "Epoch: 935, loss: 0.0744128450751, acc: 0.97413790226\n",
      "Epoch: 936, loss: 0.0748142153025, acc: 0.97413790226\n",
      "Epoch: 937, loss: 0.069803416729, acc: 0.975754320621\n",
      "Epoch: 938, loss: 0.0766510739923, acc: 0.976293087006\n",
      "Epoch: 939, loss: 0.0799231529236, acc: 0.97413790226\n",
      "Epoch: 940, loss: 0.0907906740904, acc: 0.972521543503\n",
      "Val: 0.046875\n",
      "Epoch: 941, loss: 0.0709572136402, acc: 0.976831912994\n",
      "Epoch: 942, loss: 0.100102715194, acc: 0.967133641243\n",
      "Epoch: 943, loss: 0.0758825093508, acc: 0.969827592373\n",
      "Epoch: 944, loss: 0.104060441256, acc: 0.970366358757\n",
      "Epoch: 945, loss: 0.0715767294168, acc: 0.976831912994\n",
      "Epoch: 946, loss: 0.0537919998169, acc: 0.981142222881\n",
      "Epoch: 947, loss: 0.0851661637425, acc: 0.972521543503\n",
      "Epoch: 948, loss: 0.0929696112871, acc: 0.970905184746\n",
      "Epoch: 949, loss: 0.100365310907, acc: 0.96875\n",
      "Epoch: 950, loss: 0.0681770965457, acc: 0.975215494633\n",
      "Val: 0.0404094830155\n",
      "Epoch: 951, loss: 0.0601044818759, acc: 0.980603456497\n",
      "Epoch: 952, loss: 0.0650746226311, acc: 0.977909505367\n",
      "Epoch: 953, loss: 0.0606904029846, acc: 0.980064630508\n",
      "Epoch: 954, loss: 0.0650265440345, acc: 0.977909505367\n",
      "Epoch: 955, loss: 0.0741627216339, acc: 0.975754320621\n",
      "Epoch: 956, loss: 0.0920170918107, acc: 0.97413790226\n",
      "Epoch: 957, loss: 0.0765369534492, acc: 0.975215494633\n",
      "Epoch: 958, loss: 0.0619389042258, acc: 0.976831912994\n",
      "Epoch: 959, loss: 0.0577522739768, acc: 0.980603456497\n",
      "Epoch: 960, loss: 0.073249168694, acc: 0.977909505367\n",
      "Val: 0.0463362075388\n",
      "Epoch: 961, loss: 0.0772797241807, acc: 0.973599135876\n",
      "Epoch: 962, loss: 0.0795138776302, acc: 0.977909505367\n",
      "Epoch: 963, loss: 0.0696942284703, acc: 0.974676728249\n",
      "Epoch: 964, loss: 0.0980380028486, acc: 0.969288766384\n",
      "Epoch: 965, loss: 0.0767591148615, acc: 0.97413790226\n",
      "Epoch: 966, loss: 0.0790805742145, acc: 0.972521543503\n",
      "Epoch: 967, loss: 0.0776002183557, acc: 0.973599135876\n",
      "Epoch: 968, loss: 0.0695760548115, acc: 0.975215494633\n",
      "Epoch: 969, loss: 0.0585035644472, acc: 0.983297407627\n",
      "Epoch: 970, loss: 0.0583328381181, acc: 0.979525864124\n",
      "Val: 0.0592672415078\n",
      "Epoch: 971, loss: 0.059532482177, acc: 0.982758641243\n",
      "Epoch: 972, loss: 0.0554191581905, acc: 0.97898709774\n",
      "Epoch: 973, loss: 0.0619065202773, acc: 0.980064630508\n",
      "Epoch: 974, loss: 0.0777297839522, acc: 0.97413790226\n",
      "Epoch: 975, loss: 0.0926946923137, acc: 0.974676728249\n",
      "Epoch: 976, loss: 0.0703357383609, acc: 0.975754320621\n",
      "Epoch: 977, loss: 0.0697434619069, acc: 0.976831912994\n",
      "Epoch: 978, loss: 0.0629361793399, acc: 0.980064630508\n",
      "Epoch: 979, loss: 0.0720790475607, acc: 0.975754320621\n",
      "Epoch: 980, loss: 0.0526126287878, acc: 0.981142222881\n",
      "Val: 0.0447198264301\n",
      "Epoch: 981, loss: 0.0754442662001, acc: 0.976831912994\n",
      "Epoch: 982, loss: 0.0823315903544, acc: 0.976293087006\n",
      "Epoch: 983, loss: 0.100069075823, acc: 0.971982777119\n",
      "Epoch: 984, loss: 0.0849558711052, acc: 0.974676728249\n",
      "Epoch: 985, loss: 0.0805896297097, acc: 0.97413790226\n",
      "Epoch: 986, loss: 0.0601873397827, acc: 0.980603456497\n",
      "Epoch: 987, loss: 0.0524347499013, acc: 0.981142222881\n",
      "Epoch: 988, loss: 0.0823132693768, acc: 0.97898709774\n",
      "Epoch: 989, loss: 0.0667787715793, acc: 0.974676728249\n",
      "Epoch: 990, loss: 0.0710633918643, acc: 0.977909505367\n",
      "Val: 0.0452586188912\n",
      "Epoch: 991, loss: 0.0960650444031, acc: 0.972521543503\n",
      "Epoch: 992, loss: 0.0654998272657, acc: 0.977370679379\n",
      "Epoch: 993, loss: 0.0718292891979, acc: 0.975754320621\n",
      "Epoch: 994, loss: 0.0564551278949, acc: 0.97898709774\n",
      "Epoch: 995, loss: 0.0454650335014, acc: 0.984913766384\n",
      "Epoch: 996, loss: 0.0926578566432, acc: 0.975754320621\n",
      "Epoch: 997, loss: 0.0878777727485, acc: 0.974676728249\n",
      "Epoch: 998, loss: 0.0856834203005, acc: 0.975754320621\n",
      "Epoch: 999, loss: 0.0921382978559, acc: 0.970905184746\n",
      "Epoch: 1000, loss: 0.093232318759, acc: 0.971982777119\n",
      "Val: 0.0522629320621\n",
      "Saved model to disk\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b7bb830c3795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_monitor_with_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m'''REP 3'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.5corr_rep3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, name, folder)\u001b[0m\n\u001b[1;32m    559\u001b[0m                     \u001b[0membedding_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# this is a layer counter. I use it only to store the activations in the correct place in embedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                     \u001b[0mval_batch_no\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "inceptionv3.train_and_monitor_with_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "'''REP 3\n",
    "Train generator ready, time elapsed: 27.4207718372\n",
    "...\n",
    "Epoch: 1000, loss: 0.093232318759, acc: 0.971982777119\n",
    "Val: 0.0522629320621'''\n",
    "inceptionv3.save('0.5corr_rep3', '/mnt/nas2/results/IntermediateResults/Mara/probes/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 18.0930669308\n",
      "Epoch: 0, loss: 4.18517637253, acc: 0.0199353452772\n",
      "Val: 0.0210129301995\n",
      "Epoch: 1, loss: 4.23651313782, acc: 0.0177801717073\n",
      "Epoch: 2, loss: 4.11085796356, acc: 0.0177801717073\n",
      "Epoch: 3, loss: 4.08305692673, acc: 0.015625\n",
      "Epoch: 4, loss: 4.06026601791, acc: 0.0193965509534\n",
      "Epoch: 5, loss: 4.17016983032, acc: 0.0118534481153\n",
      "Epoch: 6, loss: 4.10532045364, acc: 0.0220905169845\n",
      "Epoch: 7, loss: 4.21075391769, acc: 0.0177801717073\n",
      "Epoch: 8, loss: 4.54003095627, acc: 0.0129310349002\n",
      "Epoch: 9, loss: 4.22225809097, acc: 0.0172413792461\n",
      "Epoch: 10, loss: 4.58729410172, acc: 0.015625\n",
      "Val: 0.0204741377383\n",
      "Epoch: 11, loss: 4.08041143417, acc: 0.0215517245233\n",
      "Epoch: 12, loss: 4.21485567093, acc: 0.0253232754767\n",
      "Epoch: 13, loss: 4.09442281723, acc: 0.0172413792461\n",
      "Epoch: 14, loss: 4.11020326614, acc: 0.0172413792461\n",
      "Epoch: 15, loss: 4.13824892044, acc: 0.0129310349002\n",
      "Epoch: 16, loss: 4.1647439003, acc: 0.0199353452772\n",
      "Epoch: 17, loss: 4.12505960464, acc: 0.0210129301995\n",
      "Epoch: 18, loss: 4.09853839874, acc: 0.0150862066075\n",
      "Epoch: 19, loss: 4.06580448151, acc: 0.0204741377383\n",
      "Epoch: 20, loss: 4.06906223297, acc: 0.0247844830155\n",
      "Val: 0.0204741377383\n",
      "Epoch: 21, loss: 4.06823682785, acc: 0.0269396547228\n",
      "Epoch: 22, loss: 4.04364967346, acc: 0.0317887924612\n",
      "Epoch: 23, loss: 4.23138666153, acc: 0.0323275849223\n",
      "Epoch: 24, loss: 4.11195802689, acc: 0.0231681037694\n",
      "Epoch: 25, loss: 4.02261447906, acc: 0.0247844830155\n",
      "Epoch: 26, loss: 4.05309295654, acc: 0.0199353452772\n",
      "Epoch: 27, loss: 4.06051778793, acc: 0.0204741377383\n",
      "Epoch: 28, loss: 4.18590927124, acc: 0.0188577584922\n",
      "Epoch: 29, loss: 3.87286734581, acc: 0.0280172415078\n",
      "Epoch: 30, loss: 3.84168386459, acc: 0.0226293094456\n",
      "Val: 0.0220905169845\n",
      "Epoch: 31, loss: 3.83075499535, acc: 0.0264008622617\n",
      "Epoch: 32, loss: 3.83447480202, acc: 0.0242456905544\n",
      "Epoch: 33, loss: 3.83021855354, acc: 0.0285560339689\n",
      "Epoch: 34, loss: 3.82684254646, acc: 0.0204741377383\n",
      "Epoch: 35, loss: 3.83896803856, acc: 0.0285560339689\n",
      "Epoch: 36, loss: 3.83947324753, acc: 0.0231681037694\n",
      "Epoch: 37, loss: 3.82524394989, acc: 0.0269396547228\n",
      "Epoch: 38, loss: 3.82090353966, acc: 0.0226293094456\n",
      "Epoch: 39, loss: 3.82216072083, acc: 0.0339439660311\n",
      "Epoch: 40, loss: 3.81118273735, acc: 0.0317887924612\n",
      "Val: 0.0226293094456\n",
      "Epoch: 41, loss: 3.8159570694, acc: 0.0334051735699\n",
      "Epoch: 42, loss: 3.80886554718, acc: 0.0371767245233\n",
      "Epoch: 43, loss: 3.80126047134, acc: 0.0328663811088\n",
      "Epoch: 44, loss: 3.79858756065, acc: 0.0355603434145\n",
      "Epoch: 45, loss: 3.79659724236, acc: 0.0377155169845\n",
      "Epoch: 46, loss: 3.79657006264, acc: 0.0457974150777\n",
      "Epoch: 47, loss: 3.78669190407, acc: 0.0414870679379\n",
      "Epoch: 48, loss: 3.78200674057, acc: 0.0431034490466\n",
      "Epoch: 49, loss: 3.78568053246, acc: 0.0409482754767\n",
      "Epoch: 50, loss: 3.76856470108, acc: 0.0382543094456\n",
      "Val: 0.0220905169845\n",
      "Epoch: 51, loss: 3.76914978027, acc: 0.0393318980932\n",
      "Epoch: 52, loss: 3.76103281975, acc: 0.0431034490466\n",
      "Epoch: 53, loss: 3.76201415062, acc: 0.0495689660311\n",
      "Epoch: 54, loss: 3.75076770782, acc: 0.046875\n",
      "Epoch: 55, loss: 3.74446368217, acc: 0.0560344830155\n",
      "Epoch: 56, loss: 3.74620056152, acc: 0.0495689660311\n",
      "Epoch: 57, loss: 3.75416398048, acc: 0.046875\n",
      "Epoch: 58, loss: 3.74123644829, acc: 0.0474137924612\n",
      "Epoch: 59, loss: 3.74198389053, acc: 0.0484913811088\n",
      "Epoch: 60, loss: 3.73561048508, acc: 0.0404094830155\n",
      "Val: 0.0210129301995\n",
      "Epoch: 61, loss: 3.73625206947, acc: 0.0490301735699\n",
      "Epoch: 62, loss: 3.72093558311, acc: 0.0538793094456\n",
      "Epoch: 63, loss: 3.71896028519, acc: 0.0603448264301\n",
      "Epoch: 64, loss: 3.72071504593, acc: 0.0592672415078\n",
      "Epoch: 65, loss: 3.70753765106, acc: 0.0614224150777\n",
      "Epoch: 66, loss: 3.70333242416, acc: 0.0565732754767\n",
      "Epoch: 67, loss: 3.71876335144, acc: 0.0511853434145\n",
      "Epoch: 68, loss: 3.70887470245, acc: 0.0554956905544\n",
      "Epoch: 69, loss: 3.69561052322, acc: 0.0630387961864\n",
      "Epoch: 70, loss: 3.70705723763, acc: 0.0560344830155\n",
      "Val: 0.0264008622617\n",
      "Epoch: 71, loss: 3.69526600838, acc: 0.0549568980932\n",
      "Epoch: 72, loss: 3.68548631668, acc: 0.0635775849223\n",
      "Epoch: 73, loss: 3.69103527069, acc: 0.0630387961864\n",
      "Epoch: 74, loss: 3.68595194817, acc: 0.0603448264301\n",
      "Epoch: 75, loss: 3.67682337761, acc: 0.068426720798\n",
      "Epoch: 76, loss: 3.6937789917, acc: 0.0603448264301\n",
      "Epoch: 77, loss: 3.68671011925, acc: 0.0651939660311\n",
      "Epoch: 78, loss: 3.65876793861, acc: 0.0657327622175\n",
      "Epoch: 79, loss: 3.67342209816, acc: 0.0635775849223\n",
      "Epoch: 80, loss: 3.66993808746, acc: 0.0668103471398\n",
      "Val: 0.0220905169845\n",
      "Epoch: 81, loss: 3.68072605133, acc: 0.0625\n",
      "Epoch: 82, loss: 3.67303156853, acc: 0.0678879320621\n",
      "Epoch: 83, loss: 3.64071989059, acc: 0.0700431019068\n",
      "Epoch: 84, loss: 3.65707612038, acc: 0.0625\n",
      "Epoch: 85, loss: 3.66727042198, acc: 0.0662715509534\n",
      "Epoch: 86, loss: 3.62654209137, acc: 0.072198279202\n",
      "Epoch: 87, loss: 3.6623301506, acc: 0.0700431019068\n",
      "Epoch: 88, loss: 3.64066791534, acc: 0.0689655169845\n",
      "Epoch: 89, loss: 3.63772654533, acc: 0.0705818980932\n",
      "Epoch: 90, loss: 3.63349485397, acc: 0.0738146528602\n",
      "Val: 0.0290948282927\n",
      "Epoch: 91, loss: 3.63327884674, acc: 0.0743534490466\n",
      "Epoch: 92, loss: 3.64097547531, acc: 0.078125\n",
      "Epoch: 93, loss: 3.63353204727, acc: 0.0759698301554\n",
      "Epoch: 94, loss: 3.62343239784, acc: 0.0738146528602\n",
      "Epoch: 95, loss: 3.62340021133, acc: 0.0748922377825\n",
      "Epoch: 96, loss: 3.6101307869, acc: 0.0711206868291\n",
      "Epoch: 97, loss: 3.60681891441, acc: 0.0845905169845\n",
      "Epoch: 98, loss: 3.61737585068, acc: 0.0727370679379\n",
      "Epoch: 99, loss: 3.61821293831, acc: 0.0743534490466\n",
      "Epoch: 100, loss: 3.59961938858, acc: 0.0775862038136\n",
      "Val: 0.0220905169845\n",
      "Epoch: 101, loss: 3.62895274162, acc: 0.0835129320621\n",
      "Epoch: 102, loss: 3.60610890388, acc: 0.0851293131709\n",
      "Epoch: 103, loss: 3.58528661728, acc: 0.0883620679379\n",
      "Epoch: 104, loss: 3.60846447945, acc: 0.0894396528602\n",
      "Epoch: 105, loss: 3.58849740028, acc: 0.0829741358757\n",
      "Epoch: 106, loss: 3.57842421532, acc: 0.0872844830155\n",
      "Epoch: 107, loss: 3.59375429153, acc: 0.0889008641243\n",
      "Epoch: 108, loss: 3.58524632454, acc: 0.0921336188912\n",
      "Epoch: 109, loss: 3.5845952034, acc: 0.0851293131709\n",
      "Epoch: 110, loss: 3.57855582237, acc: 0.0802801698446\n",
      "Val: 0.0237068962306\n",
      "Epoch: 111, loss: 3.57320380211, acc: 0.0786637961864\n",
      "Epoch: 112, loss: 3.5705473423, acc: 0.087823279202\n",
      "Epoch: 113, loss: 3.57065868378, acc: 0.087823279202\n",
      "Epoch: 114, loss: 3.55966424942, acc: 0.0948275849223\n",
      "Epoch: 115, loss: 3.5673828125, acc: 0.0813577622175\n",
      "Epoch: 116, loss: 3.55895185471, acc: 0.0851293131709\n",
      "Epoch: 117, loss: 3.54177379608, acc: 0.0921336188912\n",
      "Epoch: 118, loss: 3.54339265823, acc: 0.0915948301554\n",
      "Epoch: 119, loss: 3.54153394699, acc: 0.09375\n",
      "Epoch: 120, loss: 3.54686808586, acc: 0.0905172377825\n",
      "Val: 0.0188577584922\n",
      "Epoch: 121, loss: 3.54477834702, acc: 0.0856681019068\n",
      "Epoch: 122, loss: 3.52816224098, acc: 0.0910560339689\n",
      "Epoch: 123, loss: 3.5165913105, acc: 0.0969827622175\n",
      "Epoch: 124, loss: 3.53854942322, acc: 0.0915948301554\n",
      "Epoch: 125, loss: 3.53142309189, acc: 0.0921336188912\n",
      "Epoch: 126, loss: 3.51948070526, acc: 0.0910560339689\n",
      "Epoch: 127, loss: 3.50977396965, acc: 0.0980603471398\n",
      "Epoch: 128, loss: 3.51804637909, acc: 0.0969827622175\n",
      "Epoch: 129, loss: 3.5081205368, acc: 0.107758618891\n",
      "Epoch: 130, loss: 3.48887729645, acc: 0.107219830155\n",
      "Val: 0.0264008622617\n",
      "Epoch: 131, loss: 3.51426959038, acc: 0.0975215509534\n",
      "Epoch: 132, loss: 3.51294803619, acc: 0.0921336188912\n",
      "Epoch: 133, loss: 3.50508975983, acc: 0.0991379320621\n",
      "Epoch: 134, loss: 3.50009918213, acc: 0.10506465286\n",
      "Epoch: 135, loss: 3.46678590775, acc: 0.102370686829\n",
      "Epoch: 136, loss: 3.49969744682, acc: 0.0894396528602\n",
      "Epoch: 137, loss: 3.45723867416, acc: 0.106142237782\n",
      "Epoch: 138, loss: 3.45392751694, acc: 0.110991381109\n",
      "Epoch: 139, loss: 3.45886325836, acc: 0.0991379320621\n",
      "Epoch: 140, loss: 3.4573469162, acc: 0.0969827622175\n",
      "Val: 0.03125\n",
      "Epoch: 141, loss: 3.47132563591, acc: 0.0980603471398\n",
      "Epoch: 142, loss: 3.44278621674, acc: 0.117995686829\n",
      "Epoch: 143, loss: 3.46788692474, acc: 0.103448279202\n",
      "Epoch: 144, loss: 3.45769524574, acc: 0.0991379320621\n",
      "Epoch: 145, loss: 3.4500451088, acc: 0.112607762218\n",
      "Epoch: 146, loss: 3.40659999847, acc: 0.121767237782\n",
      "Epoch: 147, loss: 3.40306091309, acc: 0.110452584922\n",
      "Epoch: 148, loss: 3.40118551254, acc: 0.108836203814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149, loss: 3.38697767258, acc: 0.132543101907\n",
      "Epoch: 150, loss: 3.42076683044, acc: 0.111530169845\n",
      "Val: 0.0220905169845\n",
      "Epoch: 151, loss: 3.41542291641, acc: 0.119073279202\n",
      "Epoch: 152, loss: 3.37834596634, acc: 0.12068965286\n",
      "Epoch: 153, loss: 3.38822174072, acc: 0.115301720798\n",
      "Epoch: 154, loss: 3.38441824913, acc: 0.127693966031\n",
      "Epoch: 155, loss: 3.38513636589, acc: 0.119612067938\n",
      "Epoch: 156, loss: 3.36854219437, acc: 0.123383618891\n",
      "Epoch: 157, loss: 3.37562298775, acc: 0.125\n",
      "Epoch: 158, loss: 3.33995485306, acc: 0.13362069428\n",
      "Epoch: 159, loss: 3.34061193466, acc: 0.131465524435\n",
      "Epoch: 160, loss: 3.3518037796, acc: 0.131465524435\n",
      "Val: 0.0355603434145\n",
      "Epoch: 161, loss: 3.35426998138, acc: 0.127693966031\n",
      "Epoch: 162, loss: 3.28558492661, acc: 0.144396558404\n",
      "Epoch: 163, loss: 3.33538579941, acc: 0.133081898093\n",
      "Epoch: 164, loss: 3.34043312073, acc: 0.148706898093\n",
      "Epoch: 165, loss: 3.35251760483, acc: 0.127155169845\n",
      "Epoch: 166, loss: 3.30166697502, acc: 0.142780169845\n",
      "Epoch: 167, loss: 3.325319767, acc: 0.143318966031\n",
      "Epoch: 168, loss: 3.30023598671, acc: 0.143857762218\n",
      "Epoch: 169, loss: 3.27194690704, acc: 0.157866373658\n",
      "Epoch: 170, loss: 3.28939223289, acc: 0.139547407627\n",
      "Val: 0.0269396547228\n",
      "Epoch: 171, loss: 3.24916744232, acc: 0.153556033969\n",
      "Epoch: 172, loss: 3.2738263607, acc: 0.143318966031\n",
      "Epoch: 173, loss: 3.25525212288, acc: 0.152478441596\n",
      "Epoch: 174, loss: 3.22531223297, acc: 0.155711203814\n",
      "Epoch: 175, loss: 3.26495075226, acc: 0.142780169845\n",
      "Epoch: 176, loss: 3.22086262703, acc: 0.15625\n",
      "Epoch: 177, loss: 3.23010706902, acc: 0.162715524435\n",
      "Epoch: 178, loss: 3.20926165581, acc: 0.152478441596\n",
      "Epoch: 179, loss: 3.20413827896, acc: 0.165409475565\n",
      "Epoch: 180, loss: 3.19022583961, acc: 0.166487067938\n",
      "Val: 0.0280172415078\n",
      "Epoch: 181, loss: 3.19108247757, acc: 0.167564660311\n",
      "Epoch: 182, loss: 3.19547772408, acc: 0.166487067938\n",
      "Epoch: 183, loss: 3.15684986115, acc: 0.17887930572\n",
      "Epoch: 184, loss: 3.14714455605, acc: 0.176185339689\n",
      "Epoch: 185, loss: 3.18524336815, acc: 0.170797407627\n",
      "Epoch: 186, loss: 3.14961791039, acc: 0.185883626342\n",
      "Epoch: 187, loss: 3.15290689468, acc: 0.174568966031\n",
      "Epoch: 188, loss: 3.1070587635, acc: 0.175107762218\n",
      "Epoch: 189, loss: 3.13812851906, acc: 0.183728441596\n",
      "Epoch: 190, loss: 3.09519743919, acc: 0.183728441596\n",
      "Val: 0.0280172415078\n",
      "Epoch: 191, loss: 3.10546565056, acc: 0.181034475565\n",
      "Epoch: 192, loss: 3.1120121479, acc: 0.182650864124\n",
      "Epoch: 193, loss: 3.12938189507, acc: 0.179418101907\n",
      "Epoch: 194, loss: 3.11445665359, acc: 0.189655169845\n",
      "Epoch: 195, loss: 3.04244995117, acc: 0.205818966031\n",
      "Epoch: 196, loss: 3.02724051476, acc: 0.204741373658\n",
      "Epoch: 197, loss: 3.04602980614, acc: 0.204202592373\n",
      "Epoch: 198, loss: 2.98391437531, acc: 0.200431033969\n",
      "Epoch: 199, loss: 3.0370490551, acc: 0.215517237782\n",
      "Epoch: 200, loss: 3.01738572121, acc: 0.213362067938\n",
      "Val: 0.0258620698005\n",
      "Epoch: 201, loss: 3.02462410927, acc: 0.201508626342\n",
      "Epoch: 202, loss: 2.97786664963, acc: 0.223060339689\n",
      "Epoch: 203, loss: 2.94962668419, acc: 0.217672407627\n",
      "Epoch: 204, loss: 2.95147132874, acc: 0.222521558404\n",
      "Epoch: 205, loss: 2.98198986053, acc: 0.219288796186\n",
      "Epoch: 206, loss: 2.90732622147, acc: 0.236530169845\n",
      "Epoch: 207, loss: 2.90483093262, acc: 0.230603441596\n",
      "Epoch: 208, loss: 2.91952729225, acc: 0.234913796186\n",
      "Epoch: 209, loss: 2.92932295799, acc: 0.227909475565\n",
      "Epoch: 210, loss: 2.90487480164, acc: 0.233836203814\n",
      "Val: 0.0183189660311\n",
      "Epoch: 211, loss: 2.89137887955, acc: 0.237607762218\n",
      "Epoch: 212, loss: 2.83626031876, acc: 0.240301728249\n",
      "Epoch: 213, loss: 2.86974382401, acc: 0.245689660311\n",
      "Epoch: 214, loss: 2.89635682106, acc: 0.239762932062\n",
      "Epoch: 215, loss: 2.84650206566, acc: 0.250538796186\n",
      "Epoch: 216, loss: 2.83448529243, acc: 0.250538796186\n",
      "Epoch: 217, loss: 2.82898759842, acc: 0.255926728249\n",
      "Epoch: 218, loss: 2.7876932621, acc: 0.263469815254\n",
      "Epoch: 219, loss: 2.82016539574, acc: 0.26831895113\n",
      "Epoch: 220, loss: 2.78360462189, acc: 0.269396543503\n",
      "Val: 0.036099139601\n",
      "Epoch: 221, loss: 2.78924846649, acc: 0.259698271751\n",
      "Epoch: 222, loss: 2.8237965107, acc: 0.250538796186\n",
      "Epoch: 223, loss: 2.77793693542, acc: 0.26831895113\n",
      "Epoch: 224, loss: 2.74513316154, acc: 0.281788796186\n",
      "Epoch: 225, loss: 2.76138925552, acc: 0.276400864124\n",
      "Epoch: 226, loss: 2.7514295578, acc: 0.275323271751\n",
      "Epoch: 227, loss: 2.75624918938, acc: 0.275323271751\n",
      "Epoch: 228, loss: 2.66752338409, acc: 0.292564660311\n",
      "Epoch: 229, loss: 2.69621992111, acc: 0.282866388559\n",
      "Epoch: 230, loss: 2.64991998672, acc: 0.289331883192\n",
      "Val: 0.030172413215\n",
      "Epoch: 231, loss: 2.5858259201, acc: 0.308189660311\n",
      "Epoch: 232, loss: 2.64270234108, acc: 0.311961203814\n",
      "Epoch: 233, loss: 2.65360021591, acc: 0.297413796186\n",
      "Epoch: 234, loss: 2.61615729332, acc: 0.308728456497\n",
      "Epoch: 235, loss: 2.6312224865, acc: 0.304956883192\n",
      "Epoch: 236, loss: 2.61537742615, acc: 0.304418116808\n",
      "Epoch: 237, loss: 2.54511022568, acc: 0.337823271751\n",
      "Epoch: 238, loss: 2.60425400734, acc: 0.309267252684\n",
      "Epoch: 239, loss: 2.54414987564, acc: 0.324892252684\n",
      "Epoch: 240, loss: 2.51963067055, acc: 0.327047407627\n",
      "Val: 0.0323275849223\n",
      "Epoch: 241, loss: 2.56893086433, acc: 0.317349135876\n",
      "Epoch: 242, loss: 2.45789766312, acc: 0.357758611441\n",
      "Epoch: 243, loss: 2.45373463631, acc: 0.345905184746\n",
      "Epoch: 244, loss: 2.41313147545, acc: 0.354525864124\n",
      "Epoch: 245, loss: 2.44207859039, acc: 0.342672407627\n",
      "Epoch: 246, loss: 2.52578878403, acc: 0.330280184746\n",
      "Epoch: 247, loss: 2.42482447624, acc: 0.351831883192\n",
      "Epoch: 248, loss: 2.4774851799, acc: 0.342133611441\n",
      "Epoch: 249, loss: 2.43472862244, acc: 0.353448271751\n",
      "Epoch: 250, loss: 2.38136363029, acc: 0.370689660311\n",
      "Val: 0.0290948282927\n",
      "Epoch: 251, loss: 2.449021101, acc: 0.360452592373\n",
      "Epoch: 252, loss: 2.38842439651, acc: 0.364224135876\n",
      "Epoch: 253, loss: 2.35780286789, acc: 0.369612067938\n",
      "Epoch: 254, loss: 2.34751677513, acc: 0.379849135876\n",
      "Epoch: 255, loss: 2.39817905426, acc: 0.363685339689\n",
      "Epoch: 256, loss: 2.32186579704, acc: 0.380387932062\n",
      "Epoch: 257, loss: 2.29530858994, acc: 0.389008611441\n",
      "Epoch: 258, loss: 2.30554890633, acc: 0.390086203814\n",
      "Epoch: 259, loss: 2.23856878281, acc: 0.405711203814\n",
      "Epoch: 260, loss: 2.27244663239, acc: 0.398706883192\n",
      "Val: 0.0355603434145\n",
      "Epoch: 261, loss: 2.30195069313, acc: 0.395474135876\n",
      "Epoch: 262, loss: 2.18533492088, acc: 0.403017252684\n",
      "Epoch: 263, loss: 2.16774964333, acc: 0.417025864124\n",
      "Epoch: 264, loss: 2.25174260139, acc: 0.40355604887\n",
      "Epoch: 265, loss: 2.26464653015, acc: 0.396551728249\n",
      "Epoch: 266, loss: 2.18760037422, acc: 0.415948271751\n",
      "Epoch: 267, loss: 2.11170363426, acc: 0.44019395113\n",
      "Epoch: 268, loss: 2.16358733177, acc: 0.424030184746\n",
      "Epoch: 269, loss: 2.11035752296, acc: 0.420797407627\n",
      "Epoch: 270, loss: 2.12212824821, acc: 0.429418116808\n",
      "Val: 0.0323275849223\n",
      "Epoch: 271, loss: 2.13830184937, acc: 0.420797407627\n",
      "Epoch: 272, loss: 2.05338597298, acc: 0.455280184746\n",
      "Epoch: 273, loss: 2.12476682663, acc: 0.44019395113\n",
      "Epoch: 274, loss: 2.04165697098, acc: 0.459051728249\n",
      "Epoch: 275, loss: 2.07312488556, acc: 0.447737067938\n",
      "Epoch: 276, loss: 2.05875754356, acc: 0.455280184746\n",
      "Epoch: 277, loss: 2.04251217842, acc: 0.449892252684\n",
      "Epoch: 278, loss: 1.9888125658, acc: 0.473060339689\n",
      "Epoch: 279, loss: 2.00919604301, acc: 0.45581895113\n",
      "Epoch: 280, loss: 1.94813549519, acc: 0.473060339689\n",
      "Val: 0.0350215509534\n",
      "Epoch: 281, loss: 1.99336361885, acc: 0.481142252684\n",
      "Epoch: 282, loss: 1.92763531208, acc: 0.474676728249\n",
      "Epoch: 283, loss: 1.93227517605, acc: 0.476831883192\n",
      "Epoch: 284, loss: 1.9013106823, acc: 0.497844815254\n",
      "Epoch: 285, loss: 1.92486572266, acc: 0.476293116808\n",
      "Epoch: 286, loss: 1.95558905602, acc: 0.482219815254\n",
      "Epoch: 287, loss: 1.87083542347, acc: 0.508620679379\n",
      "Epoch: 288, loss: 1.77625858784, acc: 0.522629320621\n",
      "Epoch: 289, loss: 1.82884192467, acc: 0.518857777119\n",
      "Epoch: 290, loss: 1.92590117455, acc: 0.476831883192\n",
      "Val: 0.0409482754767\n",
      "Epoch: 291, loss: 1.84927761555, acc: 0.495150864124\n",
      "Epoch: 292, loss: 1.77169573307, acc: 0.522629320621\n",
      "Epoch: 293, loss: 1.80175805092, acc: 0.512392222881\n",
      "Epoch: 294, loss: 1.7800194025, acc: 0.51293104887\n",
      "Epoch: 295, loss: 1.77380442619, acc: 0.519396543503\n",
      "Epoch: 296, loss: 1.84712207317, acc: 0.513469815254\n",
      "Epoch: 297, loss: 1.76555049419, acc: 0.533405184746\n",
      "Epoch: 298, loss: 1.73788917065, acc: 0.524784505367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299, loss: 1.80323135853, acc: 0.514008641243\n",
      "Epoch: 300, loss: 1.71612858772, acc: 0.530172407627\n",
      "Val: 0.0355603434145\n",
      "Epoch: 301, loss: 1.65863287449, acc: 0.553879320621\n",
      "Epoch: 302, loss: 1.7204657793, acc: 0.54148709774\n",
      "Epoch: 303, loss: 1.68213737011, acc: 0.546336233616\n",
      "Epoch: 304, loss: 1.61446416378, acc: 0.550646543503\n",
      "Epoch: 305, loss: 1.55273354053, acc: 0.573275864124\n",
      "Epoch: 306, loss: 1.67983424664, acc: 0.53125\n",
      "Epoch: 307, loss: 1.58346831799, acc: 0.567349135876\n",
      "Epoch: 308, loss: 1.59740972519, acc: 0.5625\n",
      "Epoch: 309, loss: 1.56077182293, acc: 0.574892222881\n",
      "Epoch: 310, loss: 1.52085471153, acc: 0.574892222881\n",
      "Val: 0.0274784490466\n",
      "Epoch: 311, loss: 1.61750841141, acc: 0.56519395113\n",
      "Epoch: 312, loss: 1.55794811249, acc: 0.57543104887\n",
      "Epoch: 313, loss: 1.55742537975, acc: 0.571120679379\n",
      "Epoch: 314, loss: 1.53304326534, acc: 0.590517222881\n",
      "Epoch: 315, loss: 1.49646496773, acc: 0.586745679379\n",
      "Epoch: 316, loss: 1.5521914959, acc: 0.582974135876\n",
      "Epoch: 317, loss: 1.4848035574, acc: 0.58081895113\n",
      "Epoch: 318, loss: 1.38399910927, acc: 0.609913766384\n",
      "Epoch: 319, loss: 1.48001968861, acc: 0.602370679379\n",
      "Epoch: 320, loss: 1.42485916615, acc: 0.608297407627\n",
      "Val: 0.0436422415078\n",
      "Epoch: 321, loss: 1.46120345592, acc: 0.605603456497\n",
      "Epoch: 322, loss: 1.47977876663, acc: 0.601831912994\n",
      "Epoch: 323, loss: 1.38228809834, acc: 0.619073271751\n",
      "Epoch: 324, loss: 1.39664816856, acc: 0.61476290226\n",
      "Epoch: 325, loss: 1.39639306068, acc: 0.622844815254\n",
      "Epoch: 326, loss: 1.48200058937, acc: 0.59375\n",
      "Epoch: 327, loss: 1.39682507515, acc: 0.619073271751\n",
      "Epoch: 328, loss: 1.37622785568, acc: 0.62230604887\n",
      "Epoch: 329, loss: 1.34325480461, acc: 0.627155184746\n",
      "Epoch: 330, loss: 1.33264660835, acc: 0.631465494633\n",
      "Val: 0.046875\n",
      "Epoch: 331, loss: 1.28563463688, acc: 0.642241358757\n",
      "Epoch: 332, loss: 1.31337404251, acc: 0.639547407627\n",
      "Epoch: 333, loss: 1.3334325552, acc: 0.629310369492\n",
      "Epoch: 334, loss: 1.2904599905, acc: 0.632004320621\n",
      "Epoch: 335, loss: 1.29042220116, acc: 0.635775864124\n",
      "Epoch: 336, loss: 1.29406774044, acc: 0.63038790226\n",
      "Epoch: 337, loss: 1.21710050106, acc: 0.66648709774\n",
      "Epoch: 338, loss: 1.22814512253, acc: 0.649245679379\n",
      "Epoch: 339, loss: 1.30749094486, acc: 0.645474135876\n",
      "Epoch: 340, loss: 1.28113651276, acc: 0.649245679379\n",
      "Val: 0.0452586188912\n",
      "Epoch: 341, loss: 1.30118608475, acc: 0.633620679379\n",
      "Epoch: 342, loss: 1.17647397518, acc: 0.678340494633\n",
      "Epoch: 343, loss: 1.21882402897, acc: 0.657866358757\n",
      "Epoch: 344, loss: 1.21620297432, acc: 0.649245679379\n",
      "Epoch: 345, loss: 1.13838994503, acc: 0.680495679379\n",
      "Epoch: 346, loss: 1.1292283535, acc: 0.689655184746\n",
      "Epoch: 347, loss: 1.13837003708, acc: 0.67726290226\n",
      "Epoch: 348, loss: 1.15579855442, acc: 0.679956912994\n",
      "Epoch: 349, loss: 1.09452605247, acc: 0.690732777119\n",
      "Epoch: 350, loss: 1.14811086655, acc: 0.676185369492\n",
      "Val: 0.0285560339689\n",
      "Epoch: 351, loss: 1.11934351921, acc: 0.690732777119\n",
      "Epoch: 352, loss: 1.11639285088, acc: 0.685344815254\n",
      "Epoch: 353, loss: 1.04965376854, acc: 0.697198271751\n",
      "Epoch: 354, loss: 1.0838547945, acc: 0.695581912994\n",
      "Epoch: 355, loss: 1.05861580372, acc: 0.694504320621\n",
      "Epoch: 356, loss: 1.109390378, acc: 0.6875\n",
      "Epoch: 357, loss: 1.07302868366, acc: 0.707435369492\n",
      "Epoch: 358, loss: 1.05914032459, acc: 0.696659505367\n",
      "Epoch: 359, loss: 0.964975118637, acc: 0.730064630508\n",
      "Epoch: 360, loss: 1.05963599682, acc: 0.711206912994\n",
      "Val: 0.036099139601\n",
      "Epoch: 361, loss: 0.993692755699, acc: 0.712823271751\n",
      "Epoch: 362, loss: 1.01340329647, acc: 0.703125\n",
      "Epoch: 363, loss: 1.0752042532, acc: 0.697198271751\n",
      "Epoch: 364, loss: 1.09138906002, acc: 0.702586233616\n",
      "Epoch: 365, loss: 0.89758181572, acc: 0.742456912994\n",
      "Epoch: 366, loss: 1.02175068855, acc: 0.698814630508\n",
      "Epoch: 367, loss: 0.8844140172, acc: 0.742995679379\n",
      "Epoch: 368, loss: 0.92710763216, acc: 0.739224135876\n",
      "Epoch: 369, loss: 0.941484391689, acc: 0.730064630508\n",
      "Epoch: 370, loss: 0.998398244381, acc: 0.723599135876\n",
      "Val: 0.0344827584922\n",
      "Epoch: 371, loss: 0.93853956461, acc: 0.732219815254\n",
      "Epoch: 372, loss: 0.941164135933, acc: 0.742995679379\n",
      "Epoch: 373, loss: 0.815695345402, acc: 0.761314630508\n",
      "Epoch: 374, loss: 0.934665739536, acc: 0.733297407627\n",
      "Epoch: 375, loss: 0.79914611578, acc: 0.770474135876\n",
      "Epoch: 376, loss: 0.878140151501, acc: 0.746228456497\n",
      "Epoch: 377, loss: 0.842963576317, acc: 0.762392222881\n",
      "Epoch: 378, loss: 0.887611925602, acc: 0.746767222881\n",
      "Epoch: 379, loss: 0.885166525841, acc: 0.742995679379\n",
      "Epoch: 380, loss: 0.830168128014, acc: 0.765086233616\n",
      "Val: 0.0511853434145\n",
      "Epoch: 381, loss: 0.848079741001, acc: 0.749461233616\n",
      "Epoch: 382, loss: 0.767935633659, acc: 0.784482777119\n",
      "Epoch: 383, loss: 0.814828395844, acc: 0.767241358757\n",
      "Epoch: 384, loss: 0.824561536312, acc: 0.761314630508\n",
      "Epoch: 385, loss: 0.854055821896, acc: 0.762392222881\n",
      "Epoch: 386, loss: 0.849362909794, acc: 0.755926728249\n",
      "Epoch: 387, loss: 0.838099122047, acc: 0.755926728249\n",
      "Epoch: 388, loss: 0.803275167942, acc: 0.768857777119\n",
      "Epoch: 389, loss: 0.720866382122, acc: 0.790948271751\n",
      "Epoch: 390, loss: 0.795080721378, acc: 0.765086233616\n",
      "Val: 0.0441810339689\n",
      "Epoch: 391, loss: 0.847921550274, acc: 0.754310369492\n",
      "Epoch: 392, loss: 0.862257838249, acc: 0.759159505367\n",
      "Epoch: 393, loss: 0.761159062386, acc: 0.786099135876\n",
      "Epoch: 394, loss: 0.70157122612, acc: 0.797952592373\n",
      "Epoch: 395, loss: 0.755515158176, acc: 0.77855604887\n",
      "Epoch: 396, loss: 0.803533554077, acc: 0.779094815254\n",
      "Epoch: 397, loss: 0.838869273663, acc: 0.765625\n",
      "Epoch: 398, loss: 0.71016895771, acc: 0.792025864124\n",
      "Epoch: 399, loss: 0.683520257473, acc: 0.800107777119\n",
      "Epoch: 400, loss: 0.707492113113, acc: 0.79418104887\n",
      "Val: 0.0425646565855\n",
      "Epoch: 401, loss: 0.782217860222, acc: 0.78125\n",
      "Epoch: 402, loss: 0.750355184078, acc: 0.774784505367\n",
      "Epoch: 403, loss: 0.692651152611, acc: 0.800107777119\n",
      "Epoch: 404, loss: 0.794064998627, acc: 0.765086233616\n",
      "Epoch: 405, loss: 0.668813347816, acc: 0.802801728249\n",
      "Epoch: 406, loss: 0.692459106445, acc: 0.79148709774\n",
      "Epoch: 407, loss: 0.679291009903, acc: 0.796875\n",
      "Epoch: 408, loss: 0.711404919624, acc: 0.798491358757\n",
      "Epoch: 409, loss: 0.664964199066, acc: 0.806573271751\n",
      "Epoch: 410, loss: 0.710060715675, acc: 0.790948271751\n",
      "Val: 0.0366379320621\n",
      "Epoch: 411, loss: 0.725100040436, acc: 0.799030184746\n",
      "Epoch: 412, loss: 0.728645980358, acc: 0.800646543503\n",
      "Epoch: 413, loss: 0.585996627808, acc: 0.828663766384\n",
      "Epoch: 414, loss: 0.7301825881, acc: 0.790409505367\n",
      "Epoch: 415, loss: 0.612962841988, acc: 0.818965494633\n",
      "Epoch: 416, loss: 0.601660907269, acc: 0.83081895113\n",
      "Epoch: 417, loss: 0.659168362617, acc: 0.805495679379\n",
      "Epoch: 418, loss: 0.597942471504, acc: 0.81788790226\n",
      "Epoch: 419, loss: 0.60227394104, acc: 0.824353456497\n",
      "Epoch: 420, loss: 0.661682128906, acc: 0.816271543503\n",
      "Val: 0.036099139601\n",
      "Epoch: 421, loss: 0.656243562698, acc: 0.805495679379\n",
      "Epoch: 422, loss: 0.59664273262, acc: 0.829202592373\n",
      "Epoch: 423, loss: 0.586754560471, acc: 0.821120679379\n",
      "Epoch: 424, loss: 0.577969670296, acc: 0.828663766384\n",
      "Epoch: 425, loss: 0.571021318436, acc: 0.838900864124\n",
      "Epoch: 426, loss: 0.553423464298, acc: 0.832974135876\n",
      "Epoch: 427, loss: 0.570888280869, acc: 0.831896543503\n",
      "Epoch: 428, loss: 0.574479579926, acc: 0.827586233616\n",
      "Epoch: 429, loss: 0.631858229637, acc: 0.815732777119\n",
      "Epoch: 430, loss: 0.554336369038, acc: 0.842133641243\n",
      "Val: 0.0339439660311\n",
      "Epoch: 431, loss: 0.558580338955, acc: 0.835668087006\n",
      "Epoch: 432, loss: 0.479711592197, acc: 0.84913790226\n",
      "Epoch: 433, loss: 0.544605910778, acc: 0.842133641243\n",
      "Epoch: 434, loss: 0.575672507286, acc: 0.826508641243\n",
      "Epoch: 435, loss: 0.566311299801, acc: 0.84375\n",
      "Epoch: 436, loss: 0.581826090813, acc: 0.835129320621\n",
      "Epoch: 437, loss: 0.57040899992, acc: 0.841594815254\n",
      "Epoch: 438, loss: 0.51797991991, acc: 0.847521543503\n",
      "Epoch: 439, loss: 0.491022408009, acc: 0.857219815254\n",
      "Epoch: 440, loss: 0.495490372181, acc: 0.844288766384\n",
      "Val: 0.0431034490466\n",
      "Epoch: 441, loss: 0.545995116234, acc: 0.842133641243\n",
      "Epoch: 442, loss: 0.548822760582, acc: 0.838900864124\n",
      "Epoch: 443, loss: 0.585771381855, acc: 0.821120679379\n",
      "Epoch: 444, loss: 0.529148161411, acc: 0.845905184746\n",
      "Epoch: 445, loss: 0.54912006855, acc: 0.835668087006\n",
      "Epoch: 446, loss: 0.487718135118, acc: 0.85668104887\n",
      "Epoch: 447, loss: 0.515372514725, acc: 0.842672407627\n",
      "Epoch: 448, loss: 0.485849171877, acc: 0.860991358757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449, loss: 0.492068469524, acc: 0.85398709774\n",
      "Epoch: 450, loss: 0.435580968857, acc: 0.875\n",
      "Val: 0.0355603434145\n",
      "Epoch: 451, loss: 0.466262847185, acc: 0.845905184746\n",
      "Epoch: 452, loss: 0.467698156834, acc: 0.867456912994\n",
      "Epoch: 453, loss: 0.494932830334, acc: 0.853448271751\n",
      "Epoch: 454, loss: 0.413324773312, acc: 0.87769395113\n",
      "Epoch: 455, loss: 0.509338617325, acc: 0.860452592373\n",
      "Epoch: 456, loss: 0.499359667301, acc: 0.838900864124\n",
      "Epoch: 457, loss: 0.538945496082, acc: 0.832974135876\n",
      "Epoch: 458, loss: 0.522358834743, acc: 0.850215494633\n",
      "Epoch: 459, loss: 0.413326859474, acc: 0.877155184746\n",
      "Epoch: 460, loss: 0.47072827816, acc: 0.854525864124\n",
      "Val: 0.0409482754767\n",
      "Epoch: 461, loss: 0.497113913298, acc: 0.854525864124\n",
      "Epoch: 462, loss: 0.49317073822, acc: 0.851831912994\n",
      "Epoch: 463, loss: 0.433478444815, acc: 0.870689630508\n",
      "Epoch: 464, loss: 0.454085081816, acc: 0.869073271751\n",
      "Epoch: 465, loss: 0.420996963978, acc: 0.866379320621\n",
      "Epoch: 466, loss: 0.473095923662, acc: 0.863685369492\n",
      "Epoch: 467, loss: 0.386352539062, acc: 0.887392222881\n",
      "Epoch: 468, loss: 0.38750103116, acc: 0.88038790226\n",
      "Epoch: 469, loss: 0.452646583319, acc: 0.862607777119\n",
      "Epoch: 470, loss: 0.46318179369, acc: 0.87230604887\n",
      "Val: 0.0490301735699\n",
      "Epoch: 471, loss: 0.456752151251, acc: 0.873922407627\n",
      "Epoch: 472, loss: 0.436058491468, acc: 0.872844815254\n",
      "Epoch: 473, loss: 0.434212714434, acc: 0.866918087006\n",
      "Epoch: 474, loss: 0.436645448208, acc: 0.87769395113\n",
      "Epoch: 475, loss: 0.475765019655, acc: 0.858297407627\n",
      "Epoch: 476, loss: 0.445996820927, acc: 0.865840494633\n",
      "Epoch: 477, loss: 0.402357041836, acc: 0.881465494633\n",
      "Epoch: 478, loss: 0.421146392822, acc: 0.870689630508\n",
      "Epoch: 479, loss: 0.435342401266, acc: 0.875\n",
      "Epoch: 480, loss: 0.399570584297, acc: 0.882543087006\n",
      "Val: 0.0544181019068\n",
      "Epoch: 481, loss: 0.450264722109, acc: 0.865301728249\n",
      "Epoch: 482, loss: 0.34582054615, acc: 0.888469815254\n",
      "Epoch: 483, loss: 0.495768785477, acc: 0.855603456497\n",
      "Epoch: 484, loss: 0.427249401808, acc: 0.873383641243\n",
      "Epoch: 485, loss: 0.357139647007, acc: 0.892241358757\n",
      "Epoch: 486, loss: 0.418688088655, acc: 0.87769395113\n",
      "Epoch: 487, loss: 0.401490479708, acc: 0.88523709774\n",
      "Epoch: 488, loss: 0.389091461897, acc: 0.887392222881\n",
      "Epoch: 489, loss: 0.38729211688, acc: 0.883620679379\n",
      "Epoch: 490, loss: 0.376218527555, acc: 0.89331895113\n",
      "Val: 0.0484913811088\n",
      "Epoch: 491, loss: 0.34941098094, acc: 0.894935369492\n",
      "Epoch: 492, loss: 0.362536132336, acc: 0.897090494633\n",
      "Epoch: 493, loss: 0.353059738874, acc: 0.888469815254\n",
      "Epoch: 494, loss: 0.362217485905, acc: 0.884698271751\n",
      "Epoch: 495, loss: 0.321515798569, acc: 0.90894395113\n",
      "Epoch: 496, loss: 0.370788127184, acc: 0.890086233616\n",
      "Epoch: 497, loss: 0.361408680677, acc: 0.897629320621\n",
      "Epoch: 498, loss: 0.3216753304, acc: 0.904633641243\n",
      "Epoch: 499, loss: 0.440885275602, acc: 0.863685369492\n",
      "Epoch: 500, loss: 0.393717855215, acc: 0.881465494633\n",
      "Val: 0.0414870679379\n",
      "Epoch: 501, loss: 0.335415959358, acc: 0.897629320621\n",
      "Epoch: 502, loss: 0.342483907938, acc: 0.892780184746\n",
      "Epoch: 503, loss: 0.354493170977, acc: 0.898168087006\n",
      "Epoch: 504, loss: 0.360984027386, acc: 0.898706912994\n",
      "Epoch: 505, loss: 0.385267913342, acc: 0.88793104887\n",
      "Epoch: 506, loss: 0.303405165672, acc: 0.90894395113\n",
      "Epoch: 507, loss: 0.304592132568, acc: 0.907866358757\n",
      "Epoch: 508, loss: 0.342483192682, acc: 0.890086233616\n",
      "Epoch: 509, loss: 0.370188504457, acc: 0.88523709774\n",
      "Epoch: 510, loss: 0.370103180408, acc: 0.888469815254\n",
      "Val: 0.0431034490466\n",
      "Epoch: 511, loss: 0.413295209408, acc: 0.874461233616\n",
      "Epoch: 512, loss: 0.351924240589, acc: 0.897090494633\n",
      "Epoch: 513, loss: 0.304553806782, acc: 0.907327592373\n",
      "Epoch: 514, loss: 0.297779113054, acc: 0.906788766384\n",
      "Epoch: 515, loss: 0.325135290623, acc: 0.905711233616\n",
      "Epoch: 516, loss: 0.325059264898, acc: 0.900323271751\n",
      "Epoch: 517, loss: 0.373427808285, acc: 0.88523709774\n",
      "Epoch: 518, loss: 0.271761566401, acc: 0.909482777119\n",
      "Epoch: 519, loss: 0.31460660696, acc: 0.905711233616\n",
      "Epoch: 520, loss: 0.317195564508, acc: 0.907866358757\n",
      "Val: 0.0371767245233\n",
      "Epoch: 521, loss: 0.300155520439, acc: 0.910021543503\n",
      "Epoch: 522, loss: 0.359669238329, acc: 0.88793104887\n",
      "Epoch: 523, loss: 0.327141433954, acc: 0.905711233616\n",
      "Epoch: 524, loss: 0.303110539913, acc: 0.907327592373\n",
      "Epoch: 525, loss: 0.326754629612, acc: 0.897629320621\n",
      "Epoch: 526, loss: 0.362699955702, acc: 0.886853456497\n",
      "Epoch: 527, loss: 0.352056235075, acc: 0.90086209774\n",
      "Epoch: 528, loss: 0.327550292015, acc: 0.896551728249\n",
      "Epoch: 529, loss: 0.309289395809, acc: 0.918103456497\n",
      "Epoch: 530, loss: 0.280180424452, acc: 0.908405184746\n",
      "Val: 0.0522629320621\n",
      "Epoch: 531, loss: 0.288730353117, acc: 0.908405184746\n",
      "Epoch: 532, loss: 0.374408692122, acc: 0.883620679379\n",
      "Epoch: 533, loss: 0.294542640448, acc: 0.911099135876\n",
      "Epoch: 534, loss: 0.306629121304, acc: 0.906788766384\n",
      "Epoch: 535, loss: 0.335578262806, acc: 0.90355604887\n",
      "Epoch: 536, loss: 0.302434504032, acc: 0.909482777119\n",
      "Epoch: 537, loss: 0.292162507772, acc: 0.91163790226\n",
      "Epoch: 538, loss: 0.295522660017, acc: 0.910021543503\n",
      "Epoch: 539, loss: 0.331720948219, acc: 0.901400864124\n",
      "Epoch: 540, loss: 0.293811559677, acc: 0.913254320621\n",
      "Val: 0.0296336207539\n",
      "Epoch: 541, loss: 0.318464517593, acc: 0.905172407627\n",
      "Epoch: 542, loss: 0.275632560253, acc: 0.913254320621\n",
      "Epoch: 543, loss: 0.27137067914, acc: 0.91648709774\n",
      "Epoch: 544, loss: 0.256914108992, acc: 0.918103456497\n",
      "Epoch: 545, loss: 0.259248942137, acc: 0.91918104887\n",
      "Epoch: 546, loss: 0.337008357048, acc: 0.905172407627\n",
      "Epoch: 547, loss: 0.29753947258, acc: 0.914870679379\n",
      "Epoch: 548, loss: 0.29264870286, acc: 0.915948271751\n",
      "Epoch: 549, loss: 0.286548167467, acc: 0.915948271751\n",
      "Epoch: 550, loss: 0.278088986874, acc: 0.918103456497\n",
      "Val: 0.0441810339689\n",
      "Epoch: 551, loss: 0.232741832733, acc: 0.92456895113\n",
      "Epoch: 552, loss: 0.305832982063, acc: 0.90894395113\n",
      "Epoch: 553, loss: 0.304282188416, acc: 0.909482777119\n",
      "Epoch: 554, loss: 0.250031232834, acc: 0.921875\n",
      "Epoch: 555, loss: 0.261021703482, acc: 0.915948271751\n",
      "Epoch: 556, loss: 0.280076891184, acc: 0.91163790226\n",
      "Epoch: 557, loss: 0.221085041761, acc: 0.934267222881\n",
      "Epoch: 558, loss: 0.354265749454, acc: 0.889547407627\n",
      "Epoch: 559, loss: 0.237115576863, acc: 0.926185369492\n",
      "Epoch: 560, loss: 0.280187577009, acc: 0.910560369492\n",
      "Val: 0.0398706905544\n",
      "Epoch: 561, loss: 0.254314184189, acc: 0.917564630508\n",
      "Epoch: 562, loss: 0.279926687479, acc: 0.91163790226\n",
      "Epoch: 563, loss: 0.266605466604, acc: 0.918642222881\n",
      "Epoch: 564, loss: 0.263170987368, acc: 0.919719815254\n",
      "Epoch: 565, loss: 0.313511937857, acc: 0.908405184746\n",
      "Epoch: 566, loss: 0.233831301332, acc: 0.93211209774\n",
      "Epoch: 567, loss: 0.262523829937, acc: 0.920797407627\n",
      "Epoch: 568, loss: 0.234446540475, acc: 0.930495679379\n",
      "Epoch: 569, loss: 0.301121205091, acc: 0.904633641243\n",
      "Epoch: 570, loss: 0.310764074326, acc: 0.912176728249\n",
      "Val: 0.0355603434145\n",
      "Epoch: 571, loss: 0.284377634525, acc: 0.920258641243\n",
      "Epoch: 572, loss: 0.243220061064, acc: 0.92456895113\n",
      "Epoch: 573, loss: 0.217261508107, acc: 0.929418087006\n",
      "Epoch: 574, loss: 0.252699971199, acc: 0.931034505367\n",
      "Epoch: 575, loss: 0.277370065451, acc: 0.91918104887\n",
      "Epoch: 576, loss: 0.24624337256, acc: 0.925107777119\n",
      "Epoch: 577, loss: 0.257442653179, acc: 0.918642222881\n",
      "Epoch: 578, loss: 0.263456910849, acc: 0.921875\n",
      "Epoch: 579, loss: 0.213965311646, acc: 0.934267222881\n",
      "Epoch: 580, loss: 0.259686172009, acc: 0.921336233616\n",
      "Val: 0.0387931019068\n",
      "Epoch: 581, loss: 0.253985106945, acc: 0.924030184746\n",
      "Epoch: 582, loss: 0.202468916774, acc: 0.936422407627\n",
      "Epoch: 583, loss: 0.317842692137, acc: 0.905172407627\n",
      "Epoch: 584, loss: 0.244231492281, acc: 0.925646543503\n",
      "Epoch: 585, loss: 0.241933658719, acc: 0.917564630508\n",
      "Epoch: 586, loss: 0.21032486856, acc: 0.932650864124\n",
      "Epoch: 587, loss: 0.230615526438, acc: 0.925107777119\n",
      "Epoch: 588, loss: 0.220457807183, acc: 0.933189630508\n",
      "Epoch: 589, loss: 0.227049157023, acc: 0.92456895113\n",
      "Epoch: 590, loss: 0.246109604836, acc: 0.924030184746\n",
      "Val: 0.0371767245233\n",
      "Epoch: 591, loss: 0.236191481352, acc: 0.926185369492\n",
      "Epoch: 592, loss: 0.224572241306, acc: 0.930495679379\n",
      "Epoch: 593, loss: 0.240854009986, acc: 0.925107777119\n",
      "Epoch: 594, loss: 0.22528000176, acc: 0.932650864124\n",
      "Epoch: 595, loss: 0.258149564266, acc: 0.924030184746\n",
      "Epoch: 596, loss: 0.26586830616, acc: 0.922952592373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 597, loss: 0.293362975121, acc: 0.911099135876\n",
      "Epoch: 598, loss: 0.246279701591, acc: 0.93211209774\n",
      "Epoch: 599, loss: 0.238756939769, acc: 0.929956912994\n",
      "Epoch: 600, loss: 0.218265101314, acc: 0.928879320621\n",
      "Val: 0.0447198264301\n",
      "Epoch: 601, loss: 0.214218646288, acc: 0.936961233616\n",
      "Epoch: 602, loss: 0.208456575871, acc: 0.933728456497\n",
      "Epoch: 603, loss: 0.266337275505, acc: 0.928340494633\n",
      "Epoch: 604, loss: 0.25370246172, acc: 0.922413766384\n",
      "Epoch: 605, loss: 0.204627096653, acc: 0.938038766384\n",
      "Epoch: 606, loss: 0.222492307425, acc: 0.928879320621\n",
      "Epoch: 607, loss: 0.263931721449, acc: 0.91918104887\n",
      "Epoch: 608, loss: 0.229898005724, acc: 0.934267222881\n",
      "Epoch: 609, loss: 0.218636497855, acc: 0.936961233616\n",
      "Epoch: 610, loss: 0.179345965385, acc: 0.94288790226\n",
      "Val: 0.0366379320621\n",
      "Epoch: 611, loss: 0.236345291138, acc: 0.933728456497\n",
      "Epoch: 612, loss: 0.259404152632, acc: 0.92456895113\n",
      "Epoch: 613, loss: 0.215653255582, acc: 0.933728456497\n",
      "Epoch: 614, loss: 0.197952359915, acc: 0.93480604887\n",
      "Epoch: 615, loss: 0.213410541415, acc: 0.939655184746\n",
      "Epoch: 616, loss: 0.238495409489, acc: 0.93211209774\n",
      "Epoch: 617, loss: 0.223126471043, acc: 0.929418087006\n",
      "Epoch: 618, loss: 0.213186964393, acc: 0.929418087006\n",
      "Epoch: 619, loss: 0.211147561669, acc: 0.938038766384\n",
      "Epoch: 620, loss: 0.194791615009, acc: 0.941810369492\n",
      "Val: 0.046875\n",
      "Epoch: 621, loss: 0.190528705716, acc: 0.940732777119\n",
      "Epoch: 622, loss: 0.212617054582, acc: 0.933189630508\n",
      "Epoch: 623, loss: 0.232605084777, acc: 0.930495679379\n",
      "Epoch: 624, loss: 0.209622859955, acc: 0.938038766384\n",
      "Epoch: 625, loss: 0.26322299242, acc: 0.926724135876\n",
      "Epoch: 626, loss: 0.224180072546, acc: 0.928879320621\n",
      "Epoch: 627, loss: 0.266970425844, acc: 0.926724135876\n",
      "Epoch: 628, loss: 0.219742938876, acc: 0.939116358757\n",
      "Epoch: 629, loss: 0.158565059304, acc: 0.948814630508\n",
      "Epoch: 630, loss: 0.209492519498, acc: 0.928340494633\n",
      "Val: 0.0366379320621\n",
      "Epoch: 631, loss: 0.192490831017, acc: 0.93480604887\n",
      "Epoch: 632, loss: 0.178380653262, acc: 0.938038766384\n",
      "Epoch: 633, loss: 0.224830195308, acc: 0.931573271751\n",
      "Epoch: 634, loss: 0.195016056299, acc: 0.941271543503\n",
      "Epoch: 635, loss: 0.213627338409, acc: 0.936422407627\n",
      "Epoch: 636, loss: 0.2522110641, acc: 0.927801728249\n",
      "Epoch: 637, loss: 0.258390009403, acc: 0.921336233616\n",
      "Epoch: 638, loss: 0.230818077922, acc: 0.934267222881\n",
      "Epoch: 639, loss: 0.221357315779, acc: 0.931034505367\n",
      "Epoch: 640, loss: 0.193441107869, acc: 0.939116358757\n",
      "Val: 0.0350215509534\n",
      "Epoch: 641, loss: 0.169651091099, acc: 0.944504320621\n",
      "Epoch: 642, loss: 0.223019480705, acc: 0.936422407627\n",
      "Epoch: 643, loss: 0.283521473408, acc: 0.924030184746\n",
      "Epoch: 644, loss: 0.184693753719, acc: 0.945581912994\n",
      "Epoch: 645, loss: 0.204216435552, acc: 0.942349135876\n",
      "Epoch: 646, loss: 0.198559299111, acc: 0.939116358757\n",
      "Epoch: 647, loss: 0.206986472011, acc: 0.9375\n",
      "Epoch: 648, loss: 0.203914135695, acc: 0.941271543503\n",
      "Epoch: 649, loss: 0.213788062334, acc: 0.936422407627\n",
      "Epoch: 650, loss: 0.185187861323, acc: 0.946659505367\n",
      "Val: 0.0484913811088\n",
      "Epoch: 651, loss: 0.195930868387, acc: 0.9375\n",
      "Epoch: 652, loss: 0.169950053096, acc: 0.945581912994\n",
      "Epoch: 653, loss: 0.191541388631, acc: 0.946120679379\n",
      "Epoch: 654, loss: 0.193102687597, acc: 0.938038766384\n",
      "Epoch: 655, loss: 0.163365259767, acc: 0.953663766384\n",
      "Epoch: 656, loss: 0.204857364297, acc: 0.939655184746\n",
      "Epoch: 657, loss: 0.179260790348, acc: 0.942349135876\n",
      "Epoch: 658, loss: 0.207954078913, acc: 0.941810369492\n",
      "Epoch: 659, loss: 0.187394246459, acc: 0.948275864124\n",
      "Epoch: 660, loss: 0.190296664834, acc: 0.94288790226\n",
      "Val: 0.0398706905544\n",
      "Epoch: 661, loss: 0.203781396151, acc: 0.939116358757\n",
      "Epoch: 662, loss: 0.174624159932, acc: 0.943426728249\n",
      "Epoch: 663, loss: 0.177662357688, acc: 0.939116358757\n",
      "Epoch: 664, loss: 0.251357614994, acc: 0.930495679379\n",
      "Epoch: 665, loss: 0.169129908085, acc: 0.94019395113\n",
      "Epoch: 666, loss: 0.172436878085, acc: 0.946120679379\n",
      "Epoch: 667, loss: 0.14935426414, acc: 0.949892222881\n",
      "Epoch: 668, loss: 0.211436897516, acc: 0.93480604887\n",
      "Epoch: 669, loss: 0.178136482835, acc: 0.95043104887\n",
      "Epoch: 670, loss: 0.181951954961, acc: 0.947198271751\n",
      "Val: 0.0490301735699\n",
      "Epoch: 671, loss: 0.15737220645, acc: 0.950969815254\n",
      "Epoch: 672, loss: 0.200385823846, acc: 0.938038766384\n",
      "Epoch: 673, loss: 0.158800274134, acc: 0.946659505367\n",
      "Epoch: 674, loss: 0.174076870084, acc: 0.948275864124\n",
      "Epoch: 675, loss: 0.157792076468, acc: 0.944504320621\n",
      "Epoch: 676, loss: 0.184738636017, acc: 0.94019395113\n",
      "Epoch: 677, loss: 0.185522690415, acc: 0.94288790226\n",
      "Epoch: 678, loss: 0.207639247179, acc: 0.938577592373\n",
      "Epoch: 679, loss: 0.151752024889, acc: 0.948814630508\n",
      "Epoch: 680, loss: 0.178491473198, acc: 0.948275864124\n",
      "Val: 0.0479525849223\n",
      "Epoch: 681, loss: 0.163872927427, acc: 0.952586233616\n",
      "Epoch: 682, loss: 0.168152049184, acc: 0.952586233616\n",
      "Epoch: 683, loss: 0.162146791816, acc: 0.948275864124\n",
      "Epoch: 684, loss: 0.182156056166, acc: 0.949892222881\n",
      "Epoch: 685, loss: 0.147575885057, acc: 0.955280184746\n",
      "Epoch: 686, loss: 0.173619776964, acc: 0.948814630508\n",
      "Epoch: 687, loss: 0.221292823553, acc: 0.936422407627\n",
      "Epoch: 688, loss: 0.158168509603, acc: 0.953125\n",
      "Epoch: 689, loss: 0.181093811989, acc: 0.95043104887\n",
      "Epoch: 690, loss: 0.253502696753, acc: 0.930495679379\n",
      "Val: 0.0425646565855\n",
      "Epoch: 691, loss: 0.148425355554, acc: 0.954741358757\n",
      "Epoch: 692, loss: 0.158382833004, acc: 0.948275864124\n",
      "Epoch: 693, loss: 0.141141220927, acc: 0.950969815254\n",
      "Epoch: 694, loss: 0.142901420593, acc: 0.95581895113\n",
      "Epoch: 695, loss: 0.226514920592, acc: 0.933189630508\n",
      "Epoch: 696, loss: 0.153377145529, acc: 0.954741358757\n",
      "Epoch: 697, loss: 0.170904517174, acc: 0.948814630508\n",
      "Epoch: 698, loss: 0.149877935648, acc: 0.953663766384\n",
      "Epoch: 699, loss: 0.19282849133, acc: 0.944504320621\n",
      "Epoch: 700, loss: 0.164368510246, acc: 0.947198271751\n",
      "Val: 0.0404094830155\n",
      "Epoch: 701, loss: 0.162165477872, acc: 0.948814630508\n",
      "Epoch: 702, loss: 0.158738270402, acc: 0.947198271751\n",
      "Epoch: 703, loss: 0.156248450279, acc: 0.949353456497\n",
      "Epoch: 704, loss: 0.190921276808, acc: 0.947198271751\n",
      "Epoch: 705, loss: 0.180103391409, acc: 0.948814630508\n",
      "Epoch: 706, loss: 0.161620289087, acc: 0.944504320621\n",
      "Epoch: 707, loss: 0.181636318564, acc: 0.939116358757\n",
      "Epoch: 708, loss: 0.178155839443, acc: 0.946659505367\n",
      "Epoch: 709, loss: 0.180112332106, acc: 0.940732777119\n",
      "Epoch: 710, loss: 0.195346876979, acc: 0.94773709774\n",
      "Val: 0.0355603434145\n",
      "Epoch: 711, loss: 0.161625266075, acc: 0.950969815254\n",
      "Epoch: 712, loss: 0.188952922821, acc: 0.939116358757\n",
      "Epoch: 713, loss: 0.139342904091, acc: 0.95581895113\n",
      "Epoch: 714, loss: 0.140381917357, acc: 0.953125\n",
      "Epoch: 715, loss: 0.164753854275, acc: 0.95043104887\n",
      "Epoch: 716, loss: 0.160393878818, acc: 0.956896543503\n",
      "Epoch: 717, loss: 0.160217821598, acc: 0.95043104887\n",
      "Epoch: 718, loss: 0.165888592601, acc: 0.952047407627\n",
      "Epoch: 719, loss: 0.138005465269, acc: 0.957974135876\n",
      "Epoch: 720, loss: 0.159241393209, acc: 0.953663766384\n",
      "Val: 0.030172413215\n",
      "Epoch: 721, loss: 0.158482328057, acc: 0.959590494633\n",
      "Epoch: 722, loss: 0.184932172298, acc: 0.94288790226\n",
      "Epoch: 723, loss: 0.174399554729, acc: 0.945581912994\n",
      "Epoch: 724, loss: 0.155733391643, acc: 0.952586233616\n",
      "Epoch: 725, loss: 0.174698352814, acc: 0.948814630508\n",
      "Epoch: 726, loss: 0.112357407808, acc: 0.967672407627\n",
      "Epoch: 727, loss: 0.163969784975, acc: 0.95043104887\n",
      "Epoch: 728, loss: 0.200707450509, acc: 0.94288790226\n",
      "Epoch: 729, loss: 0.141399487853, acc: 0.959590494633\n",
      "Epoch: 730, loss: 0.137815415859, acc: 0.962823271751\n",
      "Val: 0.0495689660311\n",
      "Epoch: 731, loss: 0.135954707861, acc: 0.951508641243\n",
      "Epoch: 732, loss: 0.171810671687, acc: 0.947198271751\n",
      "Epoch: 733, loss: 0.169057756662, acc: 0.94773709774\n",
      "Epoch: 734, loss: 0.152872875333, acc: 0.960668087006\n",
      "Epoch: 735, loss: 0.155099004507, acc: 0.956896543503\n",
      "Epoch: 736, loss: 0.125633001328, acc: 0.964439630508\n",
      "Epoch: 737, loss: 0.142427414656, acc: 0.956357777119\n",
      "Epoch: 738, loss: 0.148819565773, acc: 0.95581895113\n",
      "Epoch: 739, loss: 0.143929943442, acc: 0.960129320621\n",
      "Epoch: 740, loss: 0.200392574072, acc: 0.94773709774\n",
      "Val: 0.0425646565855\n",
      "Epoch: 741, loss: 0.174741923809, acc: 0.944504320621\n",
      "Epoch: 742, loss: 0.195452332497, acc: 0.943426728249\n",
      "Epoch: 743, loss: 0.170520409942, acc: 0.95043104887\n",
      "Epoch: 744, loss: 0.144505128264, acc: 0.956357777119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 745, loss: 0.128300100565, acc: 0.961745679379\n",
      "Epoch: 746, loss: 0.14330086112, acc: 0.959590494633\n",
      "Epoch: 747, loss: 0.148216530681, acc: 0.952586233616\n",
      "Epoch: 748, loss: 0.129677399993, acc: 0.959590494633\n",
      "Epoch: 749, loss: 0.160585895181, acc: 0.956357777119\n",
      "Epoch: 750, loss: 0.191491708159, acc: 0.940732777119\n",
      "Val: 0.0425646565855\n",
      "Epoch: 751, loss: 0.159177675843, acc: 0.946659505367\n",
      "Epoch: 752, loss: 0.153463199735, acc: 0.952586233616\n",
      "Epoch: 753, loss: 0.158638089895, acc: 0.954202592373\n",
      "Epoch: 754, loss: 0.136167734861, acc: 0.959590494633\n",
      "Epoch: 755, loss: 0.165362268686, acc: 0.949353456497\n",
      "Epoch: 756, loss: 0.131517335773, acc: 0.957435369492\n",
      "Epoch: 757, loss: 0.171915322542, acc: 0.95043104887\n",
      "Epoch: 758, loss: 0.153605788946, acc: 0.954741358757\n",
      "Epoch: 759, loss: 0.135892182589, acc: 0.960668087006\n",
      "Epoch: 760, loss: 0.118806414306, acc: 0.959051728249\n",
      "Val: 0.0490301735699\n",
      "Epoch: 761, loss: 0.126773446798, acc: 0.95851290226\n",
      "Epoch: 762, loss: 0.149519965053, acc: 0.953125\n",
      "Epoch: 763, loss: 0.122889906168, acc: 0.957974135876\n",
      "Epoch: 764, loss: 0.118333518505, acc: 0.964978456497\n",
      "Epoch: 765, loss: 0.114935331047, acc: 0.967672407627\n",
      "Epoch: 766, loss: 0.143377766013, acc: 0.959051728249\n",
      "Epoch: 767, loss: 0.135451793671, acc: 0.959051728249\n",
      "Epoch: 768, loss: 0.156826585531, acc: 0.961206912994\n",
      "Epoch: 769, loss: 0.174667418003, acc: 0.950969815254\n",
      "Epoch: 770, loss: 0.164701297879, acc: 0.949892222881\n",
      "Val: 0.0490301735699\n",
      "Epoch: 771, loss: 0.14978839457, acc: 0.952047407627\n",
      "Epoch: 772, loss: 0.156384766102, acc: 0.961206912994\n",
      "Epoch: 773, loss: 0.121265642345, acc: 0.968211233616\n",
      "Epoch: 774, loss: 0.130343452096, acc: 0.95851290226\n",
      "Epoch: 775, loss: 0.134439051151, acc: 0.962823271751\n",
      "Epoch: 776, loss: 0.152856394649, acc: 0.95581895113\n",
      "Epoch: 777, loss: 0.149533003569, acc: 0.960668087006\n",
      "Epoch: 778, loss: 0.134151995182, acc: 0.95581895113\n",
      "Epoch: 779, loss: 0.123094387352, acc: 0.964978456497\n",
      "Epoch: 780, loss: 0.147084966302, acc: 0.952586233616\n",
      "Val: 0.0441810339689\n",
      "Epoch: 781, loss: 0.115435212851, acc: 0.966594815254\n",
      "Epoch: 782, loss: 0.140994369984, acc: 0.956896543503\n",
      "Epoch: 783, loss: 0.114406518638, acc: 0.96336209774\n",
      "Epoch: 784, loss: 0.138978347182, acc: 0.959051728249\n",
      "Epoch: 785, loss: 0.137570679188, acc: 0.957435369492\n",
      "Epoch: 786, loss: 0.156625941396, acc: 0.956357777119\n",
      "Epoch: 787, loss: 0.117893770337, acc: 0.964978456497\n",
      "Epoch: 788, loss: 0.140180170536, acc: 0.961206912994\n",
      "Epoch: 789, loss: 0.129086241126, acc: 0.960129320621\n",
      "Epoch: 790, loss: 0.105219468474, acc: 0.96605604887\n",
      "Val: 0.0409482754767\n",
      "Epoch: 791, loss: 0.141631007195, acc: 0.961745679379\n",
      "Epoch: 792, loss: 0.131745070219, acc: 0.957435369492\n",
      "Epoch: 793, loss: 0.133816808462, acc: 0.962284505367\n",
      "Epoch: 794, loss: 0.129291266203, acc: 0.959590494633\n",
      "Epoch: 795, loss: 0.156029850245, acc: 0.951508641243\n",
      "Epoch: 796, loss: 0.131239697337, acc: 0.954202592373\n",
      "Epoch: 797, loss: 0.126714617014, acc: 0.957435369492\n",
      "Epoch: 798, loss: 0.104420155287, acc: 0.96605604887\n",
      "Epoch: 799, loss: 0.130809187889, acc: 0.957974135876\n",
      "Epoch: 800, loss: 0.137581437826, acc: 0.957435369492\n",
      "Val: 0.0495689660311\n",
      "Epoch: 801, loss: 0.143778696656, acc: 0.957435369492\n",
      "Epoch: 802, loss: 0.150735378265, acc: 0.95581895113\n",
      "Epoch: 803, loss: 0.168178498745, acc: 0.94773709774\n",
      "Epoch: 804, loss: 0.144256100059, acc: 0.954741358757\n",
      "Epoch: 805, loss: 0.126662880182, acc: 0.964978456497\n",
      "Epoch: 806, loss: 0.137788847089, acc: 0.959051728249\n",
      "Epoch: 807, loss: 0.139429792762, acc: 0.955280184746\n",
      "Epoch: 808, loss: 0.138070002198, acc: 0.957974135876\n",
      "Epoch: 809, loss: 0.124196209013, acc: 0.95851290226\n",
      "Epoch: 810, loss: 0.124755769968, acc: 0.961745679379\n",
      "Val: 0.0522629320621\n",
      "Epoch: 811, loss: 0.131763190031, acc: 0.961206912994\n",
      "Epoch: 812, loss: 0.0944122821093, acc: 0.972521543503\n",
      "Epoch: 813, loss: 0.11822938174, acc: 0.96336209774\n",
      "Epoch: 814, loss: 0.118236795068, acc: 0.965517222881\n",
      "Epoch: 815, loss: 0.107478454709, acc: 0.961745679379\n",
      "Epoch: 816, loss: 0.101245835423, acc: 0.967672407627\n",
      "Epoch: 817, loss: 0.139798924327, acc: 0.96336209774\n",
      "Epoch: 818, loss: 0.110538639128, acc: 0.96605604887\n",
      "Epoch: 819, loss: 0.140064239502, acc: 0.957435369492\n",
      "Epoch: 820, loss: 0.145915836096, acc: 0.957435369492\n",
      "Val: 0.0479525849223\n",
      "Epoch: 821, loss: 0.147532507777, acc: 0.956357777119\n",
      "Epoch: 822, loss: 0.174746319652, acc: 0.951508641243\n",
      "Epoch: 823, loss: 0.123949624598, acc: 0.957435369492\n",
      "Epoch: 824, loss: 0.124492309988, acc: 0.96605604887\n",
      "Epoch: 825, loss: 0.133088484406, acc: 0.95851290226\n",
      "Epoch: 826, loss: 0.109616547823, acc: 0.963900864124\n",
      "Epoch: 827, loss: 0.159190192819, acc: 0.957435369492\n",
      "Epoch: 828, loss: 0.100180275738, acc: 0.967133641243\n",
      "Epoch: 829, loss: 0.173608317971, acc: 0.954202592373\n",
      "Epoch: 830, loss: 0.14160785079, acc: 0.953663766384\n",
      "Val: 0.0484913811088\n",
      "Epoch: 831, loss: 0.108840554953, acc: 0.964978456497\n",
      "Epoch: 832, loss: 0.114692285657, acc: 0.961745679379\n",
      "Epoch: 833, loss: 0.126219898462, acc: 0.967133641243\n",
      "Epoch: 834, loss: 0.0999736711383, acc: 0.968211233616\n",
      "Epoch: 835, loss: 0.145044416189, acc: 0.957435369492\n",
      "Epoch: 836, loss: 0.109483011067, acc: 0.96605604887\n",
      "Epoch: 837, loss: 0.132780104876, acc: 0.963900864124\n",
      "Epoch: 838, loss: 0.117159344256, acc: 0.963900864124\n",
      "Epoch: 839, loss: 0.137065529823, acc: 0.959051728249\n",
      "Epoch: 840, loss: 0.117379508913, acc: 0.961745679379\n",
      "Val: 0.0452586188912\n",
      "Epoch: 841, loss: 0.135851725936, acc: 0.959051728249\n",
      "Epoch: 842, loss: 0.128484174609, acc: 0.962823271751\n",
      "Epoch: 843, loss: 0.110137067735, acc: 0.962823271751\n",
      "Epoch: 844, loss: 0.134800970554, acc: 0.956896543503\n",
      "Epoch: 845, loss: 0.116169147193, acc: 0.966594815254\n",
      "Epoch: 846, loss: 0.123227529228, acc: 0.962823271751\n",
      "Epoch: 847, loss: 0.126715362072, acc: 0.964439630508\n",
      "Epoch: 848, loss: 0.152454391122, acc: 0.954741358757\n",
      "Epoch: 849, loss: 0.114213466644, acc: 0.962823271751\n",
      "Epoch: 850, loss: 0.133587256074, acc: 0.96336209774\n",
      "Val: 0.0409482754767\n",
      "Epoch: 851, loss: 0.10648688674, acc: 0.962823271751\n",
      "Epoch: 852, loss: 0.127946689725, acc: 0.959051728249\n",
      "Epoch: 853, loss: 0.117750041187, acc: 0.96336209774\n",
      "Epoch: 854, loss: 0.13700312376, acc: 0.964439630508\n",
      "Epoch: 855, loss: 0.12026604265, acc: 0.961745679379\n",
      "Epoch: 856, loss: 0.13695307076, acc: 0.956357777119\n",
      "Epoch: 857, loss: 0.110603012145, acc: 0.964978456497\n",
      "Epoch: 858, loss: 0.126996695995, acc: 0.963900864124\n",
      "Epoch: 859, loss: 0.115615829825, acc: 0.962284505367\n",
      "Epoch: 860, loss: 0.141585916281, acc: 0.961206912994\n",
      "Val: 0.0404094830155\n",
      "Epoch: 861, loss: 0.120308563113, acc: 0.961745679379\n",
      "Epoch: 862, loss: 0.0901681110263, acc: 0.972521543503\n",
      "Epoch: 863, loss: 0.113751426339, acc: 0.966594815254\n",
      "Epoch: 864, loss: 0.126112341881, acc: 0.959590494633\n",
      "Epoch: 865, loss: 0.0807598680258, acc: 0.970905184746\n",
      "Epoch: 866, loss: 0.120036371052, acc: 0.964978456497\n",
      "Epoch: 867, loss: 0.112850539386, acc: 0.96605604887\n",
      "Epoch: 868, loss: 0.109741359949, acc: 0.966594815254\n",
      "Epoch: 869, loss: 0.10855256021, acc: 0.969827592373\n",
      "Epoch: 870, loss: 0.124086901546, acc: 0.967672407627\n",
      "Val: 0.0441810339689\n",
      "Epoch: 871, loss: 0.122217640281, acc: 0.960668087006\n",
      "Epoch: 872, loss: 0.118465565145, acc: 0.964439630508\n",
      "Epoch: 873, loss: 0.118585795164, acc: 0.96605604887\n",
      "Epoch: 874, loss: 0.137575730681, acc: 0.960668087006\n",
      "Epoch: 875, loss: 0.0868417099118, acc: 0.972521543503\n",
      "Epoch: 876, loss: 0.101448133588, acc: 0.967672407627\n",
      "Epoch: 877, loss: 0.122159197927, acc: 0.965517222881\n",
      "Epoch: 878, loss: 0.149490848184, acc: 0.95851290226\n",
      "Epoch: 879, loss: 0.0971467271447, acc: 0.966594815254\n",
      "Epoch: 880, loss: 0.0927595570683, acc: 0.96875\n",
      "Val: 0.0484913811088\n",
      "Epoch: 881, loss: 0.111134313047, acc: 0.964978456497\n",
      "Epoch: 882, loss: 0.106071963906, acc: 0.970905184746\n",
      "Epoch: 883, loss: 0.0953983217478, acc: 0.969288766384\n",
      "Epoch: 884, loss: 0.161126956344, acc: 0.959590494633\n",
      "Epoch: 885, loss: 0.104084923863, acc: 0.969827592373\n",
      "Epoch: 886, loss: 0.0885049626231, acc: 0.973599135876\n",
      "Epoch: 887, loss: 0.108768746257, acc: 0.967672407627\n",
      "Epoch: 888, loss: 0.122038379312, acc: 0.96336209774\n",
      "Epoch: 889, loss: 0.10309547931, acc: 0.961206912994\n",
      "Epoch: 890, loss: 0.136005550623, acc: 0.961745679379\n",
      "Val: 0.0398706905544\n",
      "Epoch: 891, loss: 0.112260870636, acc: 0.969288766384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 892, loss: 0.107112690806, acc: 0.961206912994\n",
      "Epoch: 893, loss: 0.0806533545256, acc: 0.97144395113\n",
      "Epoch: 894, loss: 0.116140395403, acc: 0.965517222881\n",
      "Epoch: 895, loss: 0.0961032137275, acc: 0.969288766384\n",
      "Epoch: 896, loss: 0.121128544211, acc: 0.966594815254\n",
      "Epoch: 897, loss: 0.0887749418616, acc: 0.972521543503\n",
      "Epoch: 898, loss: 0.090291954577, acc: 0.970905184746\n",
      "Epoch: 899, loss: 0.109049424529, acc: 0.969827592373\n",
      "Epoch: 900, loss: 0.105376027524, acc: 0.966594815254\n",
      "Val: 0.0603448264301\n",
      "Epoch: 901, loss: 0.122800663114, acc: 0.964978456497\n",
      "Epoch: 902, loss: 0.116371780634, acc: 0.960668087006\n",
      "Epoch: 903, loss: 0.114247404039, acc: 0.968211233616\n",
      "Epoch: 904, loss: 0.127871379256, acc: 0.962823271751\n",
      "Epoch: 905, loss: 0.134275570512, acc: 0.961206912994\n",
      "Epoch: 906, loss: 0.100059762597, acc: 0.964978456497\n",
      "Epoch: 907, loss: 0.117728322744, acc: 0.964439630508\n",
      "Epoch: 908, loss: 0.118038654327, acc: 0.966594815254\n",
      "Epoch: 909, loss: 0.0986075624824, acc: 0.970905184746\n",
      "Epoch: 910, loss: 0.140459150076, acc: 0.961745679379\n",
      "Val: 0.0425646565855\n",
      "Epoch: 911, loss: 0.113667495549, acc: 0.96605604887\n",
      "Epoch: 912, loss: 0.127124011517, acc: 0.967672407627\n",
      "Epoch: 913, loss: 0.121283903718, acc: 0.966594815254\n",
      "Epoch: 914, loss: 0.117850825191, acc: 0.961745679379\n",
      "Epoch: 915, loss: 0.0826733261347, acc: 0.970366358757\n",
      "Epoch: 916, loss: 0.113107249141, acc: 0.970366358757\n",
      "Epoch: 917, loss: 0.105336591601, acc: 0.96875\n",
      "Epoch: 918, loss: 0.110109627247, acc: 0.967133641243\n",
      "Epoch: 919, loss: 0.0944892391562, acc: 0.97413790226\n",
      "Epoch: 920, loss: 0.111915059388, acc: 0.969288766384\n",
      "Val: 0.0457974150777\n",
      "Epoch: 921, loss: 0.104054316878, acc: 0.972521543503\n",
      "Epoch: 922, loss: 0.106643430889, acc: 0.967672407627\n",
      "Epoch: 923, loss: 0.0965440869331, acc: 0.969827592373\n",
      "Epoch: 924, loss: 0.151306644082, acc: 0.959051728249\n",
      "Epoch: 925, loss: 0.124094933271, acc: 0.962284505367\n",
      "Epoch: 926, loss: 0.146252498031, acc: 0.957435369492\n",
      "Epoch: 927, loss: 0.103581100702, acc: 0.968211233616\n",
      "Epoch: 928, loss: 0.118495471776, acc: 0.964439630508\n",
      "Epoch: 929, loss: 0.108623243868, acc: 0.96336209774\n",
      "Epoch: 930, loss: 0.122286364436, acc: 0.96605604887\n",
      "Val: 0.0393318980932\n",
      "Epoch: 931, loss: 0.11920158565, acc: 0.967133641243\n",
      "Epoch: 932, loss: 0.0906517729163, acc: 0.969827592373\n",
      "Epoch: 933, loss: 0.0954486653209, acc: 0.97144395113\n",
      "Epoch: 934, loss: 0.079890973866, acc: 0.974676728249\n",
      "Epoch: 935, loss: 0.0888781249523, acc: 0.97413790226\n",
      "Epoch: 936, loss: 0.0931937918067, acc: 0.967672407627\n",
      "Epoch: 937, loss: 0.113589301705, acc: 0.962284505367\n",
      "Epoch: 938, loss: 0.072454996407, acc: 0.97898709774\n",
      "Epoch: 939, loss: 0.0997070148587, acc: 0.970366358757\n",
      "Epoch: 940, loss: 0.113726221025, acc: 0.963900864124\n",
      "Val: 0.0457974150777\n",
      "Epoch: 941, loss: 0.0987203866243, acc: 0.965517222881\n",
      "Epoch: 942, loss: 0.0875326991081, acc: 0.973060369492\n",
      "Epoch: 943, loss: 0.141385525465, acc: 0.961206912994\n",
      "Epoch: 944, loss: 0.0809503048658, acc: 0.976831912994\n",
      "Epoch: 945, loss: 0.172486290336, acc: 0.953663766384\n",
      "Epoch: 946, loss: 0.120936885476, acc: 0.964439630508\n",
      "Epoch: 947, loss: 0.0814446434379, acc: 0.976831912994\n",
      "Epoch: 948, loss: 0.107667632401, acc: 0.967133641243\n",
      "Epoch: 949, loss: 0.0908355936408, acc: 0.97413790226\n",
      "Epoch: 950, loss: 0.107733517885, acc: 0.965517222881\n",
      "Val: 0.0382543094456\n",
      "Epoch: 951, loss: 0.117410764098, acc: 0.966594815254\n",
      "Epoch: 952, loss: 0.095291480422, acc: 0.967672407627\n",
      "Epoch: 953, loss: 0.0843805372715, acc: 0.974676728249\n",
      "Epoch: 954, loss: 0.0836481451988, acc: 0.970905184746\n",
      "Epoch: 955, loss: 0.0927064567804, acc: 0.969827592373\n",
      "Epoch: 956, loss: 0.116799049079, acc: 0.968211233616\n",
      "Epoch: 957, loss: 0.0826126933098, acc: 0.969827592373\n",
      "Epoch: 958, loss: 0.112081475556, acc: 0.963900864124\n",
      "Epoch: 959, loss: 0.101542048156, acc: 0.964978456497\n",
      "Epoch: 960, loss: 0.0994417816401, acc: 0.966594815254\n",
      "Val: 0.0425646565855\n",
      "Epoch: 961, loss: 0.0775981172919, acc: 0.97413790226\n",
      "Epoch: 962, loss: 0.103441059589, acc: 0.970905184746\n",
      "Epoch: 963, loss: 0.0878328159451, acc: 0.971982777119\n",
      "Epoch: 964, loss: 0.112520717084, acc: 0.96605604887\n",
      "Epoch: 965, loss: 0.0690508782864, acc: 0.979525864124\n",
      "Epoch: 966, loss: 0.0905317217112, acc: 0.973060369492\n",
      "Epoch: 967, loss: 0.102321475744, acc: 0.968211233616\n",
      "Epoch: 968, loss: 0.0906872078776, acc: 0.970366358757\n",
      "Epoch: 969, loss: 0.070018991828, acc: 0.975215494633\n",
      "Epoch: 970, loss: 0.0913420990109, acc: 0.97144395113\n",
      "Val: 0.0452586188912\n",
      "Epoch: 971, loss: 0.100814133883, acc: 0.96875\n",
      "Epoch: 972, loss: 0.115975305438, acc: 0.962284505367\n",
      "Epoch: 973, loss: 0.0866767242551, acc: 0.974676728249\n",
      "Epoch: 974, loss: 0.108684912324, acc: 0.969827592373\n",
      "Epoch: 975, loss: 0.0872332602739, acc: 0.97144395113\n",
      "Epoch: 976, loss: 0.0945426374674, acc: 0.97413790226\n",
      "Epoch: 977, loss: 0.103987537324, acc: 0.967133641243\n",
      "Epoch: 978, loss: 0.129542171955, acc: 0.965517222881\n",
      "Epoch: 979, loss: 0.122055694461, acc: 0.968211233616\n",
      "Epoch: 980, loss: 0.106007255614, acc: 0.969827592373\n",
      "Val: 0.042025860399\n",
      "Epoch: 981, loss: 0.117238290608, acc: 0.96336209774\n",
      "Epoch: 982, loss: 0.0916498824954, acc: 0.970366358757\n",
      "Epoch: 983, loss: 0.0761144086719, acc: 0.976831912994\n",
      "Epoch: 984, loss: 0.0893896222115, acc: 0.970366358757\n",
      "Epoch: 985, loss: 0.0708220675588, acc: 0.979525864124\n",
      "Epoch: 986, loss: 0.0949388965964, acc: 0.971982777119\n",
      "Epoch: 987, loss: 0.103578031063, acc: 0.967133641243\n",
      "Epoch: 988, loss: 0.0810850337148, acc: 0.973060369492\n",
      "Epoch: 989, loss: 0.10730599612, acc: 0.966594815254\n",
      "Epoch: 990, loss: 0.101070486009, acc: 0.97144395113\n",
      "Val: 0.0587284490466\n",
      "Epoch: 991, loss: 0.0976885929704, acc: 0.966594815254\n",
      "Epoch: 992, loss: 0.0825453549623, acc: 0.973599135876\n",
      "Epoch: 993, loss: 0.0922839939594, acc: 0.970366358757\n",
      "Epoch: 994, loss: 0.133271738887, acc: 0.96875\n",
      "Epoch: 995, loss: 0.115913204849, acc: 0.96605604887\n",
      "Epoch: 996, loss: 0.0763431936502, acc: 0.975754320621\n",
      "Epoch: 997, loss: 0.0939892679453, acc: 0.97144395113\n",
      "Epoch: 998, loss: 0.0772852823138, acc: 0.974676728249\n",
      "Epoch: 999, loss: 0.090572565794, acc: 0.975215494633\n",
      "Epoch: 1000, loss: 0.112660571933, acc: 0.967133641243\n",
      "Val: 0.0441810339689\n"
     ]
    }
   ],
   "source": [
    "inceptionv3.train_and_monitor_with_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "\n",
    "'''50% c 50% unc REP 2 \n",
    "Train generator ready, time elapsed: 18.0930669308\n",
    "...\n",
    "Epoch: 1000, loss: 0.112660571933, acc: 0.967133641243\n",
    "Val: 0.0441810339689'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 19.7664361\n",
      "Epoch: 0, loss: 4.23722171783, acc: 0.0123922415078\n",
      "Val: 0.0215517245233\n",
      "Epoch: 1, loss: 4.03457736969, acc: 0.015625\n",
      "Epoch: 2, loss: 4.22072887421, acc: 0.0210129301995\n",
      "Epoch: 3, loss: 4.07038736343, acc: 0.0220905169845\n",
      "Epoch: 4, loss: 4.07800197601, acc: 0.0172413792461\n",
      "Val: 0.0210129301995\n",
      "Epoch: 5, loss: 4.1047372818, acc: 0.0161637924612\n",
      "Epoch: 6, loss: 4.17403030396, acc: 0.0150862066075\n",
      "Epoch: 7, loss: 4.07617521286, acc: 0.015625\n",
      "Epoch: 8, loss: 4.13574647903, acc: 0.0204741377383\n",
      "Val: 0.0210129301995\n",
      "Epoch: 9, loss: 4.0611615181, acc: 0.0237068962306\n",
      "Epoch: 10, loss: 4.04786300659, acc: 0.0220905169845\n",
      "Epoch: 11, loss: 4.04511451721, acc: 0.0296336207539\n",
      "Epoch: 12, loss: 4.03657579422, acc: 0.030172413215\n",
      "Val: 0.0226293094456\n",
      "Epoch: 13, loss: 4.06992721558, acc: 0.0204741377383\n",
      "Epoch: 14, loss: 4.06176996231, acc: 0.0204741377383\n",
      "Epoch: 15, loss: 4.04542922974, acc: 0.0258620698005\n",
      "Epoch: 16, loss: 4.0660443306, acc: 0.03125\n",
      "Val: 0.0210129301995\n",
      "Epoch: 17, loss: 4.0432062149, acc: 0.0258620698005\n",
      "Epoch: 18, loss: 4.04218292236, acc: 0.0285560339689\n",
      "Epoch: 19, loss: 4.02997541428, acc: 0.0296336207539\n",
      "Epoch: 20, loss: 4.09581851959, acc: 0.03125\n",
      "Val: 0.0161637924612\n",
      "Epoch: 21, loss: 3.95209002495, acc: 0.0237068962306\n",
      "Epoch: 22, loss: 3.85987854004, acc: 0.0188577584922\n",
      "Epoch: 23, loss: 3.84819746017, acc: 0.0242456905544\n",
      "Epoch: 24, loss: 3.84773683548, acc: 0.0274784490466\n",
      "Val: 0.0226293094456\n",
      "Epoch: 25, loss: 3.83838868141, acc: 0.0307112075388\n",
      "Epoch: 26, loss: 3.83422422409, acc: 0.0366379320621\n",
      "Epoch: 27, loss: 3.83425617218, acc: 0.0355603434145\n",
      "Epoch: 28, loss: 3.83575534821, acc: 0.030172413215\n",
      "Val: 0.0226293094456\n",
      "Epoch: 29, loss: 3.83228373528, acc: 0.0317887924612\n",
      "Epoch: 30, loss: 3.81866359711, acc: 0.0350215509534\n",
      "Epoch: 31, loss: 3.81601047516, acc: 0.0382543094456\n",
      "Epoch: 32, loss: 3.81324362755, acc: 0.0398706905544\n",
      "Val: 0.0377155169845\n",
      "Epoch: 33, loss: 3.81135892868, acc: 0.0339439660311\n",
      "Epoch: 34, loss: 3.80165457726, acc: 0.0371767245233\n",
      "Epoch: 35, loss: 3.80266046524, acc: 0.042025860399\n",
      "Epoch: 36, loss: 3.79981255531, acc: 0.0377155169845\n",
      "Val: 0.0226293094456\n",
      "Epoch: 37, loss: 3.79318976402, acc: 0.0387931019068\n",
      "Epoch: 38, loss: 3.78376269341, acc: 0.0495689660311\n",
      "Epoch: 39, loss: 3.79713964462, acc: 0.0366379320621\n",
      "Epoch: 40, loss: 3.78835511208, acc: 0.0452586188912\n",
      "Val: 0.0258620698005\n",
      "Epoch: 41, loss: 3.77624297142, acc: 0.0511853434145\n",
      "Epoch: 42, loss: 3.78148293495, acc: 0.0463362075388\n",
      "Epoch: 43, loss: 3.7866499424, acc: 0.0431034490466\n",
      "Epoch: 44, loss: 3.77677226067, acc: 0.0522629320621\n",
      "Val: 0.0264008622617\n",
      "Epoch: 45, loss: 3.78553199768, acc: 0.0490301735699\n",
      "Epoch: 46, loss: 3.77399015427, acc: 0.0447198264301\n",
      "Epoch: 47, loss: 3.76441669464, acc: 0.0490301735699\n",
      "Epoch: 48, loss: 3.76878261566, acc: 0.0501077584922\n",
      "Val: 0.0285560339689\n",
      "Epoch: 49, loss: 3.75854301453, acc: 0.0490301735699\n",
      "Epoch: 50, loss: 3.76408076286, acc: 0.0522629320621\n",
      "Epoch: 51, loss: 3.75080060959, acc: 0.057650860399\n",
      "Epoch: 52, loss: 3.75224399567, acc: 0.0538793094456\n",
      "Val: 0.030172413215\n",
      "Epoch: 53, loss: 3.75192046165, acc: 0.0571120679379\n",
      "Epoch: 54, loss: 3.74257445335, acc: 0.0565732754767\n",
      "Epoch: 55, loss: 3.75782871246, acc: 0.0528017245233\n",
      "Epoch: 56, loss: 3.74484300613, acc: 0.0592672415078\n",
      "Val: 0.0328663811088\n",
      "Epoch: 57, loss: 3.73389387131, acc: 0.0565732754767\n",
      "Epoch: 58, loss: 3.74056720734, acc: 0.0614224150777\n",
      "Epoch: 59, loss: 3.73504662514, acc: 0.0592672415078\n",
      "Epoch: 60, loss: 3.73217058182, acc: 0.0614224150777\n",
      "Val: 0.0387931019068\n",
      "Epoch: 61, loss: 3.74858164787, acc: 0.0598060339689\n",
      "Epoch: 62, loss: 3.73460412025, acc: 0.0581896565855\n",
      "Epoch: 63, loss: 3.73484992981, acc: 0.0554956905544\n",
      "Epoch: 64, loss: 3.73377943039, acc: 0.0571120679379\n",
      "Val: 0.0377155169845\n",
      "Epoch: 65, loss: 3.71670770645, acc: 0.0614224150777\n",
      "Epoch: 66, loss: 3.72521519661, acc: 0.0614224150777\n",
      "Epoch: 67, loss: 3.71883296967, acc: 0.0651939660311\n",
      "Epoch: 68, loss: 3.72246742249, acc: 0.0608836188912\n",
      "Val: 0.0425646565855\n",
      "Epoch: 69, loss: 3.71813941002, acc: 0.0641163811088\n",
      "Epoch: 70, loss: 3.71320819855, acc: 0.057650860399\n",
      "Epoch: 71, loss: 3.70648765564, acc: 0.0598060339689\n",
      "Epoch: 72, loss: 3.70601892471, acc: 0.0625\n",
      "Val: 0.0404094830155\n",
      "Epoch: 73, loss: 3.71782660484, acc: 0.0641163811088\n",
      "Epoch: 74, loss: 3.70445013046, acc: 0.0641163811088\n",
      "Epoch: 75, loss: 3.70263838768, acc: 0.0619612075388\n",
      "Epoch: 76, loss: 3.69588685036, acc: 0.068426720798\n",
      "Val: 0.0226293094456\n",
      "Epoch: 77, loss: 3.69711995125, acc: 0.0625\n",
      "Epoch: 78, loss: 3.68369150162, acc: 0.0738146528602\n",
      "Epoch: 79, loss: 3.69339776039, acc: 0.0678879320621\n",
      "Epoch: 80, loss: 3.69816374779, acc: 0.0646551698446\n",
      "Val: 0.0264008622617\n",
      "Epoch: 81, loss: 3.68713235855, acc: 0.0668103471398\n",
      "Epoch: 82, loss: 3.67110466957, acc: 0.0689655169845\n",
      "Epoch: 83, loss: 3.67865085602, acc: 0.0727370679379\n",
      "Epoch: 84, loss: 3.67574644089, acc: 0.0711206868291\n",
      "Val: 0.0344827584922\n",
      "Epoch: 85, loss: 3.67765045166, acc: 0.068426720798\n",
      "Epoch: 86, loss: 3.66862559319, acc: 0.0808189660311\n",
      "Epoch: 87, loss: 3.66872596741, acc: 0.0695043131709\n",
      "Epoch: 88, loss: 3.65901350975, acc: 0.0775862038136\n",
      "Val: 0.0328663811088\n",
      "Epoch: 89, loss: 3.67187929153, acc: 0.0765086188912\n",
      "Epoch: 90, loss: 3.66691875458, acc: 0.0716594830155\n",
      "Epoch: 91, loss: 3.66402006149, acc: 0.0727370679379\n",
      "Epoch: 92, loss: 3.65115404129, acc: 0.0754310339689\n",
      "Val: 0.0296336207539\n",
      "Epoch: 93, loss: 3.65460777283, acc: 0.0732758641243\n",
      "Epoch: 94, loss: 3.63752436638, acc: 0.0813577622175\n",
      "Epoch: 95, loss: 3.6505382061, acc: 0.078125\n",
      "Epoch: 96, loss: 3.64078807831, acc: 0.0754310339689\n",
      "Val: 0.0436422415078\n",
      "Epoch: 97, loss: 3.63090324402, acc: 0.0824353471398\n",
      "Epoch: 98, loss: 3.62623262405, acc: 0.0835129320621\n",
      "Epoch: 99, loss: 3.63471913338, acc: 0.0797413811088\n",
      "Epoch: 100, loss: 3.63199663162, acc: 0.0862068980932\n",
      "Val: 0.0382543094456\n",
      "Epoch: 101, loss: 3.6152844429, acc: 0.0824353471398\n",
      "Epoch: 102, loss: 3.61375880241, acc: 0.0818965509534\n",
      "Epoch: 103, loss: 3.59302592278, acc: 0.0824353471398\n",
      "Epoch: 104, loss: 3.6059820652, acc: 0.0835129320621\n",
      "Val: 0.0387931019068\n",
      "Epoch: 105, loss: 3.58484148979, acc: 0.0851293131709\n",
      "Epoch: 106, loss: 3.59716701508, acc: 0.0813577622175\n",
      "Epoch: 107, loss: 3.57921791077, acc: 0.0867456868291\n",
      "Epoch: 108, loss: 3.58859562874, acc: 0.0792025849223\n",
      "Val: 0.0474137924612\n",
      "Epoch: 109, loss: 3.57571864128, acc: 0.0883620679379\n",
      "Epoch: 110, loss: 3.57567238808, acc: 0.0883620679379\n",
      "Epoch: 111, loss: 3.55735445023, acc: 0.09375\n",
      "Epoch: 112, loss: 3.55295467377, acc: 0.0899784490466\n",
      "Val: 0.0366379320621\n",
      "Epoch: 113, loss: 3.55416035652, acc: 0.0932112038136\n",
      "Epoch: 114, loss: 3.54668664932, acc: 0.0969827622175\n",
      "Epoch: 115, loss: 3.55143213272, acc: 0.0921336188912\n",
      "Epoch: 116, loss: 3.55666136742, acc: 0.0910560339689\n",
      "Val: 0.0528017245233\n",
      "Epoch: 117, loss: 3.54941916466, acc: 0.0905172377825\n",
      "Epoch: 118, loss: 3.55446887016, acc: 0.101293101907\n",
      "Epoch: 119, loss: 3.56425738335, acc: 0.0969827622175\n",
      "Epoch: 120, loss: 3.5231654644, acc: 0.0959051698446\n",
      "Val: 0.0371767245233\n",
      "Epoch: 121, loss: 3.51450204849, acc: 0.112068966031\n",
      "Epoch: 122, loss: 3.51138043404, acc: 0.112068966031\n",
      "Epoch: 123, loss: 3.51916909218, acc: 0.104525864124\n",
      "Epoch: 124, loss: 3.49782752991, acc: 0.0991379320621\n",
      "Val: 0.0371767245233\n",
      "Epoch: 125, loss: 3.49240422249, acc: 0.100754313171\n",
      "Epoch: 126, loss: 3.50047826767, acc: 0.103448279202\n",
      "Epoch: 127, loss: 3.4652056694, acc: 0.105603449047\n",
      "Epoch: 128, loss: 3.50881385803, acc: 0.102370686829\n",
      "Val: 0.0409482754767\n",
      "Epoch: 129, loss: 3.4801170826, acc: 0.105603449047\n",
      "Epoch: 130, loss: 3.49092197418, acc: 0.099676720798\n",
      "Epoch: 131, loss: 3.44816923141, acc: 0.11368534714\n",
      "Epoch: 132, loss: 3.46160817146, acc: 0.108836203814\n",
      "Val: 0.0387931019068\n",
      "Epoch: 133, loss: 3.45796561241, acc: 0.109913796186\n",
      "Epoch: 134, loss: 3.44666600227, acc: 0.107219830155\n",
      "Epoch: 135, loss: 3.43080663681, acc: 0.114224135876\n",
      "Epoch: 136, loss: 3.41385245323, acc: 0.119612067938\n",
      "Val: 0.036099139601\n",
      "Epoch: 137, loss: 3.40394949913, acc: 0.122844830155\n",
      "Epoch: 138, loss: 3.41235494614, acc: 0.119073279202\n",
      "Epoch: 139, loss: 3.39986920357, acc: 0.121767237782\n",
      "Epoch: 140, loss: 3.42335534096, acc: 0.12068965286\n",
      "Val: 0.0414870679379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 141, loss: 3.40400838852, acc: 0.130926728249\n",
      "Epoch: 142, loss: 3.36047649384, acc: 0.126077592373\n",
      "Epoch: 143, loss: 3.38038825989, acc: 0.123383618891\n",
      "Epoch: 144, loss: 3.37984013557, acc: 0.13200430572\n",
      "Val: 0.0436422415078\n",
      "Epoch: 145, loss: 3.36404848099, acc: 0.124461203814\n",
      "Epoch: 146, loss: 3.334649086, acc: 0.128771558404\n",
      "Epoch: 147, loss: 3.3154168129, acc: 0.136314660311\n",
      "Epoch: 148, loss: 3.3313498497, acc: 0.130387932062\n",
      "Val: 0.0533405169845\n",
      "Epoch: 149, loss: 3.32868409157, acc: 0.129310339689\n",
      "Epoch: 150, loss: 3.32568144798, acc: 0.140625\n",
      "Epoch: 151, loss: 3.30206513405, acc: 0.129310339689\n",
      "Epoch: 152, loss: 3.27735948563, acc: 0.142241373658\n",
      "Val: 0.0511853434145\n",
      "Epoch: 153, loss: 3.27574396133, acc: 0.139547407627\n",
      "Epoch: 154, loss: 3.24773216248, acc: 0.148168101907\n",
      "Epoch: 155, loss: 3.27129435539, acc: 0.142241373658\n",
      "Epoch: 156, loss: 3.24809241295, acc: 0.148706898093\n",
      "Val: 0.0479525849223\n",
      "Epoch: 157, loss: 3.25841498375, acc: 0.149784475565\n",
      "Epoch: 158, loss: 3.23339486122, acc: 0.144935339689\n",
      "Epoch: 159, loss: 3.2059442997, acc: 0.161099135876\n",
      "Epoch: 160, loss: 3.24731111526, acc: 0.142780169845\n",
      "Val: 0.0436422415078\n",
      "Epoch: 161, loss: 3.21644449234, acc: 0.156788796186\n",
      "Epoch: 162, loss: 3.18640446663, acc: 0.160560339689\n",
      "Epoch: 163, loss: 3.15427470207, acc: 0.170797407627\n",
      "Epoch: 164, loss: 3.15170288086, acc: 0.169719830155\n",
      "Val: 0.0441810339689\n",
      "Epoch: 165, loss: 3.14298343658, acc: 0.169181033969\n",
      "Epoch: 166, loss: 3.13339972496, acc: 0.179956898093\n",
      "Epoch: 167, loss: 3.13922071457, acc: 0.174030169845\n",
      "Epoch: 168, loss: 3.15502882004, acc: 0.164331898093\n",
      "Val: 0.0549568980932\n",
      "Epoch: 169, loss: 3.12563276291, acc: 0.182112067938\n",
      "Epoch: 170, loss: 3.12445521355, acc: 0.172952592373\n",
      "Epoch: 171, loss: 3.05205082893, acc: 0.195043101907\n",
      "Epoch: 172, loss: 3.07739686966, acc: 0.181034475565\n",
      "Val: 0.0474137924612\n",
      "Epoch: 173, loss: 3.0208542347, acc: 0.196659475565\n",
      "Epoch: 174, loss: 3.06263542175, acc: 0.188038796186\n",
      "Epoch: 175, loss: 3.05408120155, acc: 0.190193966031\n",
      "Epoch: 176, loss: 3.06668162346, acc: 0.193965524435\n",
      "Val: 0.0463362075388\n",
      "Epoch: 177, loss: 3.05955910683, acc: 0.203663796186\n",
      "Epoch: 178, loss: 3.04708409309, acc: 0.193426728249\n",
      "Epoch: 179, loss: 3.02601027489, acc: 0.203663796186\n",
      "Epoch: 180, loss: 2.99497652054, acc: 0.214439660311\n",
      "Val: 0.042025860399\n",
      "Epoch: 181, loss: 2.95812773705, acc: 0.216056033969\n",
      "Epoch: 182, loss: 2.96055340767, acc: 0.201508626342\n",
      "Epoch: 183, loss: 2.95773148537, acc: 0.226293101907\n",
      "Epoch: 184, loss: 2.96342730522, acc: 0.206357762218\n",
      "Val: 0.0452586188912\n",
      "Epoch: 185, loss: 2.9519007206, acc: 0.221982762218\n",
      "Epoch: 186, loss: 2.94296360016, acc: 0.216056033969\n",
      "Epoch: 187, loss: 2.88851189613, acc: 0.228987067938\n",
      "Epoch: 188, loss: 2.88036680222, acc: 0.244073271751\n",
      "Val: 0.0495689660311\n",
      "Epoch: 189, loss: 2.91005921364, acc: 0.232758626342\n",
      "Epoch: 190, loss: 2.79724955559, acc: 0.244612067938\n",
      "Epoch: 191, loss: 2.80518627167, acc: 0.246228441596\n",
      "Epoch: 192, loss: 2.8150472641, acc: 0.257004320621\n",
      "Val: 0.0436422415078\n",
      "Epoch: 193, loss: 2.75443959236, acc: 0.267241388559\n",
      "Epoch: 194, loss: 2.83054900169, acc: 0.260775864124\n",
      "Epoch: 195, loss: 2.77749276161, acc: 0.263469815254\n",
      "Epoch: 196, loss: 2.78437542915, acc: 0.249461203814\n",
      "Val: 0.0425646565855\n",
      "Epoch: 197, loss: 2.71821832657, acc: 0.271012932062\n",
      "Epoch: 198, loss: 2.69426631927, acc: 0.275862067938\n",
      "Epoch: 199, loss: 2.75294232368, acc: 0.261853456497\n",
      "Epoch: 200, loss: 2.67788767815, acc: 0.27855604887\n",
      "Val: 0.036099139601\n",
      "Epoch: 201, loss: 2.67804145813, acc: 0.285560339689\n",
      "Epoch: 202, loss: 2.64588356018, acc: 0.290409475565\n",
      "Epoch: 203, loss: 2.63580393791, acc: 0.29418104887\n",
      "Epoch: 204, loss: 2.6391556263, acc: 0.285021543503\n",
      "Val: 0.0479525849223\n",
      "Epoch: 205, loss: 2.62205600739, acc: 0.297413796186\n",
      "Epoch: 206, loss: 2.61266565323, acc: 0.295258611441\n",
      "Epoch: 207, loss: 2.58192610741, acc: 0.313038796186\n",
      "Epoch: 208, loss: 2.63055109978, acc: 0.296336203814\n",
      "Val: 0.0414870679379\n",
      "Epoch: 209, loss: 2.60585832596, acc: 0.299030184746\n",
      "Epoch: 210, loss: 2.55392050743, acc: 0.318426728249\n",
      "Epoch: 211, loss: 2.52821135521, acc: 0.328663796186\n",
      "Epoch: 212, loss: 2.57522082329, acc: 0.306034475565\n",
      "Val: 0.0495689660311\n",
      "Epoch: 213, loss: 2.49998378754, acc: 0.334590524435\n",
      "Epoch: 214, loss: 2.4711523056, acc: 0.344827592373\n",
      "Epoch: 215, loss: 2.44256782532, acc: 0.342133611441\n",
      "Epoch: 216, loss: 2.42808318138, acc: 0.34105604887\n",
      "Val: 0.0495689660311\n",
      "Epoch: 217, loss: 2.4175157547, acc: 0.355064660311\n",
      "Epoch: 218, loss: 2.4522986412, acc: 0.334590524435\n",
      "Epoch: 219, loss: 2.46528148651, acc: 0.351831883192\n",
      "Epoch: 220, loss: 2.4583761692, acc: 0.337284475565\n",
      "Val: 0.0414870679379\n",
      "Epoch: 221, loss: 2.37777400017, acc: 0.376077592373\n",
      "Epoch: 222, loss: 2.27384591103, acc: 0.388469815254\n",
      "Epoch: 223, loss: 2.27655696869, acc: 0.390625\n",
      "Epoch: 224, loss: 2.30853533745, acc: 0.378771543503\n",
      "Val: 0.0393318980932\n",
      "Epoch: 225, loss: 2.3635725975, acc: 0.348599135876\n",
      "Epoch: 226, loss: 2.37726426125, acc: 0.348060339689\n",
      "Epoch: 227, loss: 2.29048919678, acc: 0.378771543503\n",
      "Epoch: 228, loss: 2.29121828079, acc: 0.379310339689\n",
      "Val: 0.0441810339689\n",
      "Epoch: 229, loss: 2.28709721565, acc: 0.371228456497\n",
      "Epoch: 230, loss: 2.22531747818, acc: 0.400323271751\n",
      "Epoch: 231, loss: 2.22717237473, acc: 0.390086203814\n",
      "Epoch: 232, loss: 2.17592906952, acc: 0.42456895113\n",
      "Val: 0.0355603434145\n",
      "Epoch: 233, loss: 2.19950532913, acc: 0.409482747316\n",
      "Epoch: 234, loss: 2.13015460968, acc: 0.410021543503\n",
      "Epoch: 235, loss: 2.09661746025, acc: 0.425646543503\n",
      "Epoch: 236, loss: 2.19784283638, acc: 0.417564660311\n",
      "Val: 0.0452586188912\n",
      "Epoch: 237, loss: 2.09046173096, acc: 0.427262932062\n",
      "Epoch: 238, loss: 2.1440205574, acc: 0.417025864124\n",
      "Epoch: 239, loss: 2.04646062851, acc: 0.441271543503\n",
      "Epoch: 240, loss: 2.14535665512, acc: 0.417025864124\n",
      "Val: 0.0414870679379\n",
      "Epoch: 241, loss: 2.11867833138, acc: 0.436422407627\n",
      "Epoch: 242, loss: 2.13814115524, acc: 0.422952592373\n",
      "Epoch: 243, loss: 2.06383657455, acc: 0.443426728249\n",
      "Epoch: 244, loss: 2.00304174423, acc: 0.459590524435\n",
      "Val: 0.0474137924612\n",
      "Epoch: 245, loss: 2.00863409042, acc: 0.449892252684\n",
      "Epoch: 246, loss: 1.97629642487, acc: 0.461745679379\n",
      "Epoch: 247, loss: 1.97619509697, acc: 0.471982747316\n",
      "Epoch: 248, loss: 1.99079108238, acc: 0.466594815254\n",
      "Val: 0.0463362075388\n",
      "Epoch: 249, loss: 1.9728063345, acc: 0.454741388559\n",
      "Epoch: 250, loss: 1.87863969803, acc: 0.477909475565\n",
      "Epoch: 251, loss: 1.96285378933, acc: 0.455280184746\n",
      "Epoch: 252, loss: 1.92897057533, acc: 0.496228456497\n",
      "Val: 0.0425646565855\n",
      "Epoch: 253, loss: 1.93268120289, acc: 0.473599135876\n",
      "Epoch: 254, loss: 1.91208302975, acc: 0.484913796186\n",
      "Epoch: 255, loss: 1.89016854763, acc: 0.490840524435\n",
      "Epoch: 256, loss: 1.893856287, acc: 0.485991388559\n",
      "Val: 0.0398706905544\n",
      "Epoch: 257, loss: 1.78587734699, acc: 0.504849135876\n",
      "Epoch: 258, loss: 1.76497745514, acc: 0.51293104887\n",
      "Epoch: 259, loss: 1.74965071678, acc: 0.530711233616\n",
      "Epoch: 260, loss: 1.76217544079, acc: 0.515086233616\n",
      "Val: 0.0431034490466\n",
      "Epoch: 261, loss: 1.78779387474, acc: 0.511853456497\n",
      "Epoch: 262, loss: 1.77713859081, acc: 0.504310369492\n",
      "Epoch: 263, loss: 1.79241919518, acc: 0.513469815254\n",
      "Epoch: 264, loss: 1.64328360558, acc: 0.559267222881\n",
      "Val: 0.0441810339689\n",
      "Epoch: 265, loss: 1.7694978714, acc: 0.511853456497\n",
      "Epoch: 266, loss: 1.64721310139, acc: 0.539331912994\n",
      "Epoch: 267, loss: 1.62643861771, acc: 0.55711209774\n",
      "Epoch: 268, loss: 1.62944674492, acc: 0.556034505367\n",
      "Val: 0.0377155169845\n",
      "Epoch: 269, loss: 1.62350511551, acc: 0.55226290226\n",
      "Epoch: 270, loss: 1.62756812572, acc: 0.546336233616\n",
      "Epoch: 271, loss: 1.58699715137, acc: 0.570581912994\n",
      "Epoch: 272, loss: 1.66285431385, acc: 0.524784505367\n",
      "Val: 0.051724139601\n",
      "Epoch: 273, loss: 1.62024736404, acc: 0.550107777119\n",
      "Epoch: 274, loss: 1.58043050766, acc: 0.552801728249\n",
      "Epoch: 275, loss: 1.58435308933, acc: 0.563038766384\n",
      "Epoch: 276, loss: 1.57164359093, acc: 0.574892222881\n",
      "Val: 0.042025860399\n",
      "Epoch: 277, loss: 1.59806013107, acc: 0.564655184746\n",
      "Epoch: 278, loss: 1.4538410902, acc: 0.592133641243\n",
      "Epoch: 279, loss: 1.49466335773, acc: 0.584590494633\n",
      "Epoch: 280, loss: 1.59052360058, acc: 0.558189630508\n",
      "Val: 0.0501077584922\n",
      "Epoch: 281, loss: 1.45219540596, acc: 0.598060369492\n",
      "Epoch: 282, loss: 1.45953464508, acc: 0.585129320621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283, loss: 1.42781198025, acc: 0.605603456497\n",
      "Epoch: 284, loss: 1.48812067509, acc: 0.601293087006\n",
      "Val: 0.0393318980932\n",
      "Epoch: 285, loss: 1.45282709599, acc: 0.587823271751\n",
      "Epoch: 286, loss: 1.37049615383, acc: 0.610452592373\n",
      "Epoch: 287, loss: 1.31911873817, acc: 0.634698271751\n",
      "Epoch: 288, loss: 1.41940450668, acc: 0.598060369492\n",
      "Val: 0.0382543094456\n",
      "Epoch: 289, loss: 1.38410210609, acc: 0.607219815254\n",
      "Epoch: 290, loss: 1.32548213005, acc: 0.62230604887\n",
      "Epoch: 291, loss: 1.36306977272, acc: 0.61961209774\n",
      "Epoch: 292, loss: 1.356757164, acc: 0.628232777119\n",
      "Val: 0.0436422415078\n",
      "Epoch: 293, loss: 1.35281801224, acc: 0.626616358757\n",
      "Epoch: 294, loss: 1.33074939251, acc: 0.636314630508\n",
      "Epoch: 295, loss: 1.28860116005, acc: 0.638469815254\n",
      "Epoch: 296, loss: 1.34109532833, acc: 0.626616358757\n",
      "Val: 0.0436422415078\n",
      "Epoch: 297, loss: 1.28661954403, acc: 0.63523709774\n",
      "Epoch: 298, loss: 1.28202283382, acc: 0.638469815254\n",
      "Epoch: 299, loss: 1.28150546551, acc: 0.65086209774\n",
      "Epoch: 300, loss: 1.33344745636, acc: 0.631465494633\n",
      "Val: 0.0463362075388\n",
      "Epoch: 301, loss: 1.31903588772, acc: 0.626077592373\n",
      "Epoch: 302, loss: 1.27903592587, acc: 0.639008641243\n",
      "Epoch: 303, loss: 1.20449852943, acc: 0.660021543503\n",
      "Epoch: 304, loss: 1.18802368641, acc: 0.66648709774\n",
      "Val: 0.0538793094456\n",
      "Epoch: 305, loss: 1.16396594048, acc: 0.667564630508\n",
      "Epoch: 306, loss: 1.16076660156, acc: 0.677801728249\n",
      "Epoch: 307, loss: 1.14792346954, acc: 0.670258641243\n",
      "Epoch: 308, loss: 1.20170998573, acc: 0.648168087006\n",
      "Val: 0.0511853434145\n",
      "Epoch: 309, loss: 1.20567345619, acc: 0.673491358757\n",
      "Epoch: 310, loss: 1.138677001, acc: 0.667564630508\n",
      "Epoch: 311, loss: 1.02369236946, acc: 0.711206912994\n",
      "Epoch: 312, loss: 1.0942530632, acc: 0.685883641243\n",
      "Val: 0.0479525849223\n",
      "Epoch: 313, loss: 1.13867986202, acc: 0.68211209774\n",
      "Epoch: 314, loss: 1.06267774105, acc: 0.696120679379\n",
      "Epoch: 315, loss: 1.05956959724, acc: 0.690732777119\n",
      "Epoch: 316, loss: 1.10535645485, acc: 0.684267222881\n",
      "Val: 0.0463362075388\n",
      "Epoch: 317, loss: 1.06665158272, acc: 0.684267222881\n",
      "Epoch: 318, loss: 1.03446340561, acc: 0.717672407627\n",
      "Epoch: 319, loss: 1.08483850956, acc: 0.697198271751\n",
      "Epoch: 320, loss: 1.0400583744, acc: 0.703125\n",
      "Val: 0.042025860399\n",
      "Epoch: 321, loss: 1.04266762733, acc: 0.707974135876\n",
      "Epoch: 322, loss: 1.02545177937, acc: 0.709051728249\n",
      "Epoch: 323, loss: 1.04610657692, acc: 0.707435369492\n",
      "Epoch: 324, loss: 1.02665269375, acc: 0.704202592373\n",
      "Val: 0.0549568980932\n",
      "Epoch: 325, loss: 1.01973998547, acc: 0.700969815254\n",
      "Epoch: 326, loss: 1.04051542282, acc: 0.706896543503\n",
      "Epoch: 327, loss: 0.962914168835, acc: 0.726293087006\n",
      "Epoch: 328, loss: 1.00181412697, acc: 0.712823271751\n",
      "Val: 0.046875\n",
      "Epoch: 329, loss: 0.935763835907, acc: 0.720366358757\n",
      "Epoch: 330, loss: 0.91467165947, acc: 0.722521543503\n",
      "Epoch: 331, loss: 0.923097670078, acc: 0.739224135876\n",
      "Epoch: 332, loss: 0.927576899529, acc: 0.732758641243\n",
      "Val: 0.0495689660311\n",
      "Epoch: 333, loss: 0.992237627506, acc: 0.722521543503\n",
      "Epoch: 334, loss: 0.90477669239, acc: 0.733836233616\n",
      "Epoch: 335, loss: 0.941491603851, acc: 0.730064630508\n",
      "Epoch: 336, loss: 0.869005441666, acc: 0.746228456497\n",
      "Val: 0.0495689660311\n",
      "Epoch: 337, loss: 0.898731887341, acc: 0.733297407627\n",
      "Epoch: 338, loss: 0.882406055927, acc: 0.748922407627\n",
      "Epoch: 339, loss: 0.881929516792, acc: 0.740840494633\n",
      "Epoch: 340, loss: 0.955733597279, acc: 0.72144395113\n",
      "Val: 0.046875\n",
      "Epoch: 341, loss: 0.784375965595, acc: 0.767780184746\n",
      "Epoch: 342, loss: 0.878779947758, acc: 0.751077592373\n",
      "Epoch: 343, loss: 0.817434668541, acc: 0.76023709774\n",
      "Epoch: 344, loss: 0.879681646824, acc: 0.743534505367\n",
      "Val: 0.0398706905544\n",
      "Epoch: 345, loss: 0.894353926182, acc: 0.739224135876\n",
      "Epoch: 346, loss: 0.881938755512, acc: 0.757004320621\n",
      "Epoch: 347, loss: 0.784930050373, acc: 0.776400864124\n",
      "Epoch: 348, loss: 0.835642755032, acc: 0.761314630508\n",
      "Val: 0.0431034490466\n",
      "Epoch: 349, loss: 0.856865167618, acc: 0.741918087006\n",
      "Epoch: 350, loss: 0.859530389309, acc: 0.753232777119\n",
      "Epoch: 351, loss: 0.753743767738, acc: 0.773168087006\n",
      "Epoch: 352, loss: 0.779141128063, acc: 0.77586209774\n",
      "Val: 0.042025860399\n",
      "Epoch: 353, loss: 0.755710124969, acc: 0.774245679379\n",
      "Epoch: 354, loss: 0.776283621788, acc: 0.774784505367\n",
      "Epoch: 355, loss: 0.701169967651, acc: 0.806573271751\n",
      "Epoch: 356, loss: 0.768937170506, acc: 0.775323271751\n",
      "Val: 0.0490301735699\n",
      "Epoch: 357, loss: 0.828141987324, acc: 0.766702592373\n",
      "Epoch: 358, loss: 0.695116102695, acc: 0.801724135876\n",
      "Epoch: 359, loss: 0.738958895206, acc: 0.785021543503\n",
      "Epoch: 360, loss: 0.818707704544, acc: 0.754849135876\n",
      "Val: 0.0506465509534\n",
      "Epoch: 361, loss: 0.741369903088, acc: 0.78663790226\n",
      "Epoch: 362, loss: 0.758145451546, acc: 0.774784505367\n",
      "Epoch: 363, loss: 0.747756481171, acc: 0.776400864124\n",
      "Epoch: 364, loss: 0.77186268568, acc: 0.774784505367\n",
      "Val: 0.0484913811088\n",
      "Epoch: 365, loss: 0.709343791008, acc: 0.806573271751\n",
      "Epoch: 366, loss: 0.753463387489, acc: 0.782866358757\n",
      "Epoch: 367, loss: 0.721592128277, acc: 0.78125\n",
      "Epoch: 368, loss: 0.699947297573, acc: 0.787176728249\n",
      "Val: 0.0538793094456\n",
      "Epoch: 369, loss: 0.722058534622, acc: 0.786099135876\n",
      "Epoch: 370, loss: 0.699621558189, acc: 0.792564630508\n",
      "Epoch: 371, loss: 0.691870391369, acc: 0.806573271751\n",
      "Epoch: 372, loss: 0.707161009312, acc: 0.803340494633\n",
      "Val: 0.046875\n",
      "Epoch: 373, loss: 0.682619690895, acc: 0.800107777119\n",
      "Epoch: 374, loss: 0.621037662029, acc: 0.816810369492\n",
      "Epoch: 375, loss: 0.700836122036, acc: 0.788793087006\n",
      "Epoch: 376, loss: 0.686061739922, acc: 0.795797407627\n",
      "Val: 0.0431034490466\n",
      "Epoch: 377, loss: 0.614320576191, acc: 0.816810369492\n",
      "Epoch: 378, loss: 0.637895405293, acc: 0.811422407627\n",
      "Epoch: 379, loss: 0.664915502071, acc: 0.798491358757\n",
      "Epoch: 380, loss: 0.708821237087, acc: 0.793103456497\n",
      "Val: 0.0409482754767\n",
      "Epoch: 381, loss: 0.564604699612, acc: 0.831357777119\n",
      "Epoch: 382, loss: 0.64717066288, acc: 0.816271543503\n",
      "Epoch: 383, loss: 0.653706371784, acc: 0.814655184746\n",
      "Epoch: 384, loss: 0.661833405495, acc: 0.799030184746\n",
      "Val: 0.0646551698446\n",
      "Epoch: 385, loss: 0.664213895798, acc: 0.80980604887\n",
      "Epoch: 386, loss: 0.628363549709, acc: 0.8125\n",
      "Epoch: 387, loss: 0.588377714157, acc: 0.826508641243\n",
      "Epoch: 388, loss: 0.593740403652, acc: 0.82543104887\n",
      "Val: 0.0474137924612\n",
      "Epoch: 389, loss: 0.599925518036, acc: 0.828663766384\n",
      "Epoch: 390, loss: 0.571926295757, acc: 0.834590494633\n",
      "Epoch: 391, loss: 0.523870944977, acc: 0.852909505367\n",
      "Epoch: 392, loss: 0.573059856892, acc: 0.839978456497\n",
      "Val: 0.0511853434145\n",
      "Epoch: 393, loss: 0.62370878458, acc: 0.824353456497\n",
      "Epoch: 394, loss: 0.634532809258, acc: 0.81788790226\n",
      "Epoch: 395, loss: 0.597507536411, acc: 0.830280184746\n",
      "Epoch: 396, loss: 0.513375163078, acc: 0.842672407627\n",
      "Val: 0.0490301735699\n",
      "Epoch: 397, loss: 0.572174608707, acc: 0.829202592373\n",
      "Epoch: 398, loss: 0.560328543186, acc: 0.840517222881\n",
      "Epoch: 399, loss: 0.597620785236, acc: 0.827586233616\n",
      "Epoch: 400, loss: 0.548543989658, acc: 0.84375\n",
      "Val: 0.0452586188912\n",
      "Epoch: 401, loss: 0.508994698524, acc: 0.84644395113\n",
      "Epoch: 402, loss: 0.540149509907, acc: 0.841594815254\n",
      "Epoch: 403, loss: 0.525880992413, acc: 0.842133641243\n",
      "Epoch: 404, loss: 0.513109445572, acc: 0.841594815254\n",
      "Val: 0.0441810339689\n",
      "Epoch: 405, loss: 0.563930392265, acc: 0.832974135876\n",
      "Epoch: 406, loss: 0.529419958591, acc: 0.84105604887\n",
      "Epoch: 407, loss: 0.501258671284, acc: 0.850754320621\n",
      "Epoch: 408, loss: 0.574567079544, acc: 0.835668087006\n",
      "Val: 0.0501077584922\n",
      "Epoch: 409, loss: 0.473991960287, acc: 0.85668104887\n",
      "Epoch: 410, loss: 0.50223582983, acc: 0.84644395113\n",
      "Epoch: 411, loss: 0.5151450634, acc: 0.839978456497\n",
      "Epoch: 412, loss: 0.520045161247, acc: 0.844827592373\n",
      "Val: 0.0544181019068\n",
      "Epoch: 413, loss: 0.526237010956, acc: 0.83836209774\n",
      "Epoch: 414, loss: 0.551181077957, acc: 0.835129320621\n",
      "Epoch: 415, loss: 0.562383890152, acc: 0.84105604887\n",
      "Epoch: 416, loss: 0.470434486866, acc: 0.858297407627\n",
      "Val: 0.0549568980932\n",
      "Epoch: 417, loss: 0.534228503704, acc: 0.851293087006\n",
      "Epoch: 418, loss: 0.499733835459, acc: 0.85398709774\n",
      "Epoch: 419, loss: 0.468747884035, acc: 0.866918087006\n",
      "Epoch: 420, loss: 0.486880272627, acc: 0.851293087006\n",
      "Val: 0.0528017245233\n",
      "Epoch: 421, loss: 0.676086902618, acc: 0.804418087006\n",
      "Epoch: 422, loss: 0.496779948473, acc: 0.851293087006\n",
      "Epoch: 423, loss: 0.467680811882, acc: 0.863146543503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424, loss: 0.447373837233, acc: 0.869073271751\n",
      "Val: 0.0447198264301\n",
      "Epoch: 425, loss: 0.466628402472, acc: 0.857758641243\n",
      "Epoch: 426, loss: 0.495437055826, acc: 0.852370679379\n",
      "Epoch: 427, loss: 0.460766226053, acc: 0.860991358757\n",
      "Epoch: 428, loss: 0.488625615835, acc: 0.858836233616\n",
      "Val: 0.0479525849223\n",
      "Epoch: 429, loss: 0.485174834728, acc: 0.85668104887\n",
      "Epoch: 430, loss: 0.440932571888, acc: 0.86961209774\n",
      "Epoch: 431, loss: 0.433484494686, acc: 0.862607777119\n",
      "Epoch: 432, loss: 0.417945235968, acc: 0.873383641243\n",
      "Val: 0.0414870679379\n",
      "Epoch: 433, loss: 0.447765082121, acc: 0.864224135876\n",
      "Epoch: 434, loss: 0.536363124847, acc: 0.852370679379\n",
      "Epoch: 435, loss: 0.421780586243, acc: 0.869073271751\n",
      "Epoch: 436, loss: 0.494464278221, acc: 0.856142222881\n",
      "Val: 0.0463362075388\n",
      "Epoch: 437, loss: 0.442852139473, acc: 0.867995679379\n",
      "Epoch: 438, loss: 0.453730851412, acc: 0.862607777119\n",
      "Epoch: 439, loss: 0.4605294168, acc: 0.859375\n",
      "Epoch: 440, loss: 0.410606384277, acc: 0.877155184746\n",
      "Val: 0.0511853434145\n",
      "Epoch: 441, loss: 0.418675452471, acc: 0.87230604887\n",
      "Epoch: 442, loss: 0.463546395302, acc: 0.860991358757\n",
      "Epoch: 443, loss: 0.420194506645, acc: 0.873383641243\n",
      "Epoch: 444, loss: 0.409285306931, acc: 0.878771543503\n",
      "Val: 0.0511853434145\n",
      "Epoch: 445, loss: 0.454308778048, acc: 0.859375\n",
      "Epoch: 446, loss: 0.37706014514, acc: 0.884159505367\n",
      "Epoch: 447, loss: 0.418109327555, acc: 0.876077592373\n",
      "Epoch: 448, loss: 0.3741517663, acc: 0.889547407627\n",
      "Val: 0.0425646565855\n",
      "Epoch: 449, loss: 0.374228239059, acc: 0.888469815254\n",
      "Epoch: 450, loss: 0.460845291615, acc: 0.87230604887\n",
      "Epoch: 451, loss: 0.328218251467, acc: 0.901939630508\n",
      "Epoch: 452, loss: 0.390640556812, acc: 0.883081912994\n",
      "Val: 0.0463362075388\n",
      "Epoch: 453, loss: 0.443849742413, acc: 0.86206895113\n",
      "Epoch: 454, loss: 0.430375218391, acc: 0.878771543503\n",
      "Epoch: 455, loss: 0.410943001509, acc: 0.88038790226\n",
      "Epoch: 456, loss: 0.344827771187, acc: 0.89331895113\n",
      "Val: 0.0501077584922\n",
      "Epoch: 457, loss: 0.40216550231, acc: 0.877155184746\n",
      "Epoch: 458, loss: 0.35330709815, acc: 0.891163766384\n",
      "Epoch: 459, loss: 0.397030472755, acc: 0.88038790226\n",
      "Epoch: 460, loss: 0.403368443251, acc: 0.881465494633\n",
      "Val: 0.0608836188912\n",
      "Epoch: 461, loss: 0.396061718464, acc: 0.880926728249\n",
      "Epoch: 462, loss: 0.345744818449, acc: 0.882543087006\n",
      "Epoch: 463, loss: 0.402159065008, acc: 0.878232777119\n",
      "Epoch: 464, loss: 0.388676851988, acc: 0.88038790226\n",
      "Val: 0.0457974150777\n",
      "Epoch: 465, loss: 0.357192218304, acc: 0.896551728249\n",
      "Epoch: 466, loss: 0.356885254383, acc: 0.882004320621\n",
      "Epoch: 467, loss: 0.371473133564, acc: 0.888469815254\n",
      "Epoch: 468, loss: 0.379313856363, acc: 0.88038790226\n",
      "Val: 0.0452586188912\n",
      "Epoch: 469, loss: 0.327436029911, acc: 0.899784505367\n",
      "Epoch: 470, loss: 0.333223313093, acc: 0.901939630508\n",
      "Epoch: 471, loss: 0.366732597351, acc: 0.884159505367\n",
      "Epoch: 472, loss: 0.337128460407, acc: 0.905711233616\n",
      "Val: 0.0474137924612\n",
      "Epoch: 473, loss: 0.339360415936, acc: 0.897629320621\n",
      "Epoch: 474, loss: 0.388622224331, acc: 0.876616358757\n",
      "Epoch: 475, loss: 0.348609745502, acc: 0.889547407627\n",
      "Epoch: 476, loss: 0.384310156107, acc: 0.88523709774\n",
      "Val: 0.0522629320621\n",
      "Epoch: 477, loss: 0.408891648054, acc: 0.882543087006\n",
      "Epoch: 478, loss: 0.426330596209, acc: 0.878771543503\n",
      "Epoch: 479, loss: 0.366641610861, acc: 0.893857777119\n",
      "Epoch: 480, loss: 0.355294197798, acc: 0.892780184746\n",
      "Val: 0.0484913811088\n",
      "Epoch: 481, loss: 0.34756949544, acc: 0.897629320621\n",
      "Epoch: 482, loss: 0.296173661947, acc: 0.905172407627\n",
      "Epoch: 483, loss: 0.391376107931, acc: 0.881465494633\n",
      "Epoch: 484, loss: 0.333876311779, acc: 0.900323271751\n",
      "Val: 0.0554956905544\n",
      "Epoch: 485, loss: 0.36010569334, acc: 0.897629320621\n",
      "Epoch: 486, loss: 0.332167506218, acc: 0.899784505367\n",
      "Epoch: 487, loss: 0.283839851618, acc: 0.905172407627\n",
      "Epoch: 488, loss: 0.307625681162, acc: 0.901939630508\n",
      "Val: 0.0549568980932\n",
      "Epoch: 489, loss: 0.35979026556, acc: 0.901939630508\n",
      "Epoch: 490, loss: 0.304595321417, acc: 0.904633641243\n",
      "Epoch: 491, loss: 0.387216180563, acc: 0.882004320621\n",
      "Epoch: 492, loss: 0.330285996199, acc: 0.905172407627\n",
      "Val: 0.0490301735699\n",
      "Epoch: 493, loss: 0.316562592983, acc: 0.906788766384\n",
      "Epoch: 494, loss: 0.371717333794, acc: 0.890625\n",
      "Epoch: 495, loss: 0.343983739614, acc: 0.890625\n",
      "Epoch: 496, loss: 0.260384619236, acc: 0.91648709774\n",
      "Val: 0.0565732754767\n",
      "Epoch: 497, loss: 0.313417345285, acc: 0.904094815254\n",
      "Epoch: 498, loss: 0.369278371334, acc: 0.89601290226\n",
      "Epoch: 499, loss: 0.332588165998, acc: 0.899245679379\n",
      "Epoch: 500, loss: 0.265505254269, acc: 0.915409505367\n",
      "Val: 0.0447198264301\n",
      "Epoch: 501, loss: 0.292662322521, acc: 0.911099135876\n",
      "Epoch: 502, loss: 0.332839041948, acc: 0.898168087006\n",
      "Epoch: 503, loss: 0.307897776365, acc: 0.907327592373\n",
      "Epoch: 504, loss: 0.310345977545, acc: 0.913793087006\n",
      "Val: 0.0528017245233\n",
      "Epoch: 505, loss: 0.314988911152, acc: 0.904633641243\n",
      "Epoch: 506, loss: 0.307552874088, acc: 0.905172407627\n",
      "Epoch: 507, loss: 0.268442720175, acc: 0.915948271751\n",
      "Epoch: 508, loss: 0.327704310417, acc: 0.899245679379\n",
      "Val: 0.0501077584922\n",
      "Epoch: 509, loss: 0.300188183784, acc: 0.907327592373\n",
      "Epoch: 510, loss: 0.353750556707, acc: 0.896551728249\n",
      "Epoch: 511, loss: 0.296983450651, acc: 0.904633641243\n",
      "Epoch: 512, loss: 0.290845602751, acc: 0.920258641243\n",
      "Val: 0.0441810339689\n",
      "Epoch: 513, loss: 0.317247092724, acc: 0.903017222881\n",
      "Epoch: 514, loss: 0.268221110106, acc: 0.918103456497\n",
      "Epoch: 515, loss: 0.248312726617, acc: 0.921336233616\n",
      "Epoch: 516, loss: 0.294762253761, acc: 0.918642222881\n",
      "Val: 0.0447198264301\n",
      "Epoch: 517, loss: 0.280177861452, acc: 0.91918104887\n",
      "Epoch: 518, loss: 0.27460026741, acc: 0.915948271751\n",
      "Epoch: 519, loss: 0.349984556437, acc: 0.898706912994\n",
      "Epoch: 520, loss: 0.259057015181, acc: 0.92726290226\n",
      "Val: 0.0495689660311\n",
      "Epoch: 521, loss: 0.272166073322, acc: 0.926185369492\n",
      "Epoch: 522, loss: 0.324551522732, acc: 0.901939630508\n",
      "Epoch: 523, loss: 0.270891219378, acc: 0.918642222881\n",
      "Epoch: 524, loss: 0.270922422409, acc: 0.917025864124\n",
      "Val: 0.0511853434145\n",
      "Epoch: 525, loss: 0.296999394894, acc: 0.911099135876\n",
      "Epoch: 526, loss: 0.280290424824, acc: 0.922952592373\n",
      "Epoch: 527, loss: 0.29414704442, acc: 0.910560369492\n",
      "Epoch: 528, loss: 0.280145764351, acc: 0.913793087006\n",
      "Val: 0.0533405169845\n",
      "Epoch: 529, loss: 0.20680989325, acc: 0.936422407627\n",
      "Epoch: 530, loss: 0.25756084919, acc: 0.917564630508\n",
      "Epoch: 531, loss: 0.362645030022, acc: 0.892780184746\n",
      "Epoch: 532, loss: 0.286693483591, acc: 0.915409505367\n",
      "Val: 0.0511853434145\n",
      "Epoch: 533, loss: 0.252799600363, acc: 0.927801728249\n",
      "Epoch: 534, loss: 0.300551593304, acc: 0.91163790226\n",
      "Epoch: 535, loss: 0.257230192423, acc: 0.918642222881\n",
      "Epoch: 536, loss: 0.25937756896, acc: 0.922413766384\n",
      "Val: 0.051724139601\n",
      "Epoch: 537, loss: 0.362302333117, acc: 0.890625\n",
      "Epoch: 538, loss: 0.307537734509, acc: 0.902478456497\n",
      "Epoch: 539, loss: 0.277291744947, acc: 0.918642222881\n",
      "Epoch: 540, loss: 0.310735344887, acc: 0.901400864124\n",
      "Val: 0.051724139601\n",
      "Epoch: 541, loss: 0.244005545974, acc: 0.92456895113\n",
      "Epoch: 542, loss: 0.25357016921, acc: 0.920258641243\n",
      "Epoch: 543, loss: 0.296817272902, acc: 0.903017222881\n",
      "Epoch: 544, loss: 0.223267495632, acc: 0.93211209774\n",
      "Val: 0.0571120679379\n",
      "Epoch: 545, loss: 0.289133936167, acc: 0.914870679379\n",
      "Epoch: 546, loss: 0.262143373489, acc: 0.927801728249\n",
      "Epoch: 547, loss: 0.26817932725, acc: 0.918103456497\n",
      "Epoch: 548, loss: 0.243707880378, acc: 0.926724135876\n",
      "Val: 0.0511853434145\n",
      "Epoch: 549, loss: 0.25093716383, acc: 0.92456895113\n",
      "Epoch: 550, loss: 0.267556279898, acc: 0.914870679379\n",
      "Epoch: 551, loss: 0.243570640683, acc: 0.926185369492\n",
      "Epoch: 552, loss: 0.260469377041, acc: 0.918103456497\n",
      "Val: 0.0538793094456\n",
      "Epoch: 553, loss: 0.22757896781, acc: 0.931034505367\n",
      "Epoch: 554, loss: 0.27222058177, acc: 0.924030184746\n",
      "Epoch: 555, loss: 0.276668667793, acc: 0.920797407627\n",
      "Epoch: 556, loss: 0.24289020896, acc: 0.922952592373\n",
      "Val: 0.0501077584922\n",
      "Epoch: 557, loss: 0.240863963962, acc: 0.921336233616\n",
      "Epoch: 558, loss: 0.249233543873, acc: 0.926185369492\n",
      "Epoch: 559, loss: 0.271242886782, acc: 0.921336233616\n",
      "Epoch: 560, loss: 0.202612891793, acc: 0.934267222881\n",
      "Val: 0.0549568980932\n",
      "Epoch: 561, loss: 0.23075132072, acc: 0.933728456497\n",
      "Epoch: 562, loss: 0.213636130095, acc: 0.935883641243\n",
      "Epoch: 563, loss: 0.242862164974, acc: 0.929956912994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 564, loss: 0.258855313063, acc: 0.929956912994\n",
      "Val: 0.046875\n",
      "Epoch: 565, loss: 0.243123784661, acc: 0.926185369492\n",
      "Epoch: 566, loss: 0.252356559038, acc: 0.92726290226\n",
      "Epoch: 567, loss: 0.220071807504, acc: 0.933189630508\n",
      "Epoch: 568, loss: 0.213685899973, acc: 0.933728456497\n",
      "Val: 0.0490301735699\n",
      "Epoch: 569, loss: 0.216223850846, acc: 0.928879320621\n",
      "Epoch: 570, loss: 0.259409844875, acc: 0.915948271751\n",
      "Epoch: 571, loss: 0.252701073885, acc: 0.918103456497\n",
      "Epoch: 572, loss: 0.227901905775, acc: 0.928879320621\n",
      "Val: 0.0560344830155\n",
      "Epoch: 573, loss: 0.234574973583, acc: 0.925107777119\n",
      "Epoch: 574, loss: 0.211833223701, acc: 0.934267222881\n",
      "Epoch: 575, loss: 0.217342570424, acc: 0.931573271751\n",
      "Epoch: 576, loss: 0.250392496586, acc: 0.924030184746\n",
      "Val: 0.0662715509534\n",
      "Epoch: 577, loss: 0.204415157437, acc: 0.935883641243\n",
      "Epoch: 578, loss: 0.260613620281, acc: 0.91648709774\n",
      "Epoch: 579, loss: 0.228376537561, acc: 0.928879320621\n",
      "Epoch: 580, loss: 0.242538303137, acc: 0.925646543503\n",
      "Val: 0.0495689660311\n",
      "Epoch: 581, loss: 0.232517361641, acc: 0.927801728249\n",
      "Epoch: 582, loss: 0.254914045334, acc: 0.921875\n",
      "Epoch: 583, loss: 0.262621194124, acc: 0.921875\n",
      "Epoch: 584, loss: 0.25104996562, acc: 0.92456895113\n",
      "Val: 0.0544181019068\n",
      "Epoch: 585, loss: 0.226333469152, acc: 0.933728456497\n",
      "Epoch: 586, loss: 0.178479224443, acc: 0.941271543503\n",
      "Epoch: 587, loss: 0.200261846185, acc: 0.944504320621\n",
      "Epoch: 588, loss: 0.221720844507, acc: 0.931573271751\n",
      "Val: 0.0592672415078\n",
      "Epoch: 589, loss: 0.215686172247, acc: 0.933189630508\n",
      "Epoch: 590, loss: 0.187824308872, acc: 0.945043087006\n",
      "Epoch: 591, loss: 0.276601701975, acc: 0.918642222881\n",
      "Epoch: 592, loss: 0.232362851501, acc: 0.931573271751\n",
      "Val: 0.0447198264301\n",
      "Epoch: 593, loss: 0.216160088778, acc: 0.934267222881\n",
      "Epoch: 594, loss: 0.233809381723, acc: 0.932650864124\n",
      "Epoch: 595, loss: 0.251562416553, acc: 0.921336233616\n",
      "Epoch: 596, loss: 0.183127835393, acc: 0.945043087006\n",
      "Val: 0.0538793094456\n",
      "Epoch: 597, loss: 0.23501573503, acc: 0.928879320621\n",
      "Epoch: 598, loss: 0.240759387612, acc: 0.932650864124\n",
      "Epoch: 599, loss: 0.254811495543, acc: 0.926185369492\n",
      "Epoch: 600, loss: 0.229888051748, acc: 0.926185369492\n",
      "Val: 0.051724139601\n",
      "Epoch: 601, loss: 0.226616114378, acc: 0.926185369492\n",
      "Epoch: 602, loss: 0.19988873601, acc: 0.943426728249\n",
      "Epoch: 603, loss: 0.223976403475, acc: 0.936422407627\n",
      "Epoch: 604, loss: 0.207003772259, acc: 0.939116358757\n",
      "Val: 0.0495689660311\n",
      "Epoch: 605, loss: 0.196400359273, acc: 0.941810369492\n",
      "Epoch: 606, loss: 0.208411380649, acc: 0.939116358757\n",
      "Epoch: 607, loss: 0.224321052432, acc: 0.933728456497\n",
      "Epoch: 608, loss: 0.201682925224, acc: 0.936961233616\n",
      "Val: 0.0457974150777\n",
      "Epoch: 609, loss: 0.229213654995, acc: 0.927801728249\n",
      "Epoch: 610, loss: 0.213674083352, acc: 0.933728456497\n",
      "Epoch: 611, loss: 0.230988711119, acc: 0.934267222881\n",
      "Epoch: 612, loss: 0.187212094665, acc: 0.940732777119\n",
      "Val: 0.0457974150777\n",
      "Epoch: 613, loss: 0.198361441493, acc: 0.938038766384\n",
      "Epoch: 614, loss: 0.203179448843, acc: 0.936961233616\n",
      "Epoch: 615, loss: 0.164137005806, acc: 0.948275864124\n",
      "Epoch: 616, loss: 0.225090309978, acc: 0.926185369492\n",
      "Val: 0.0490301735699\n",
      "Epoch: 617, loss: 0.195517212152, acc: 0.9375\n",
      "Epoch: 618, loss: 0.175955250859, acc: 0.943426728249\n",
      "Epoch: 619, loss: 0.215343207121, acc: 0.939116358757\n",
      "Epoch: 620, loss: 0.185322120786, acc: 0.94288790226\n",
      "Val: 0.0522629320621\n",
      "Epoch: 621, loss: 0.194250866771, acc: 0.943426728249\n",
      "Epoch: 622, loss: 0.174225687981, acc: 0.94019395113\n",
      "Epoch: 623, loss: 0.181333869696, acc: 0.941810369492\n",
      "Epoch: 624, loss: 0.204931095243, acc: 0.939116358757\n",
      "Val: 0.0474137924612\n",
      "Epoch: 625, loss: 0.165830388665, acc: 0.95043104887\n",
      "Epoch: 626, loss: 0.214252531528, acc: 0.932650864124\n",
      "Epoch: 627, loss: 0.184731915593, acc: 0.948814630508\n",
      "Epoch: 628, loss: 0.218343943357, acc: 0.93480604887\n",
      "Val: 0.0387931019068\n",
      "Epoch: 629, loss: 0.202139690518, acc: 0.94019395113\n",
      "Epoch: 630, loss: 0.245579719543, acc: 0.927801728249\n",
      "Epoch: 631, loss: 0.189245954156, acc: 0.944504320621\n",
      "Epoch: 632, loss: 0.140235751867, acc: 0.957974135876\n",
      "Val: 0.0587284490466\n",
      "Epoch: 633, loss: 0.204226136208, acc: 0.938038766384\n",
      "Epoch: 634, loss: 0.191480338573, acc: 0.941271543503\n",
      "Epoch: 635, loss: 0.146436870098, acc: 0.955280184746\n",
      "Epoch: 636, loss: 0.177272498608, acc: 0.944504320621\n",
      "Val: 0.051724139601\n",
      "Epoch: 637, loss: 0.184503614902, acc: 0.945581912994\n",
      "Epoch: 638, loss: 0.24221162498, acc: 0.927801728249\n",
      "Epoch: 639, loss: 0.222979560494, acc: 0.935344815254\n",
      "Epoch: 640, loss: 0.195579648018, acc: 0.943426728249\n",
      "Val: 0.0495689660311\n",
      "Epoch: 641, loss: 0.202643960714, acc: 0.941271543503\n",
      "Epoch: 642, loss: 0.183439195156, acc: 0.948275864124\n",
      "Epoch: 643, loss: 0.192949265242, acc: 0.939116358757\n",
      "Epoch: 644, loss: 0.220250934362, acc: 0.9375\n",
      "Val: 0.0506465509534\n",
      "Epoch: 645, loss: 0.234607100487, acc: 0.932650864124\n",
      "Epoch: 646, loss: 0.205935314298, acc: 0.941271543503\n",
      "Epoch: 647, loss: 0.206799149513, acc: 0.93480604887\n",
      "Epoch: 648, loss: 0.199839383364, acc: 0.94288790226\n",
      "Val: 0.0490301735699\n",
      "Epoch: 649, loss: 0.183314621449, acc: 0.943426728249\n",
      "Epoch: 650, loss: 0.167621791363, acc: 0.94773709774\n",
      "Epoch: 651, loss: 0.166750371456, acc: 0.947198271751\n",
      "Epoch: 652, loss: 0.236192002892, acc: 0.935344815254\n",
      "Val: 0.0495689660311\n",
      "Epoch: 653, loss: 0.189533770084, acc: 0.942349135876\n",
      "Epoch: 654, loss: 0.194407731295, acc: 0.943426728249\n",
      "Epoch: 655, loss: 0.228728786111, acc: 0.932650864124\n",
      "Epoch: 656, loss: 0.183993458748, acc: 0.943426728249\n",
      "Val: 0.0533405169845\n",
      "Epoch: 657, loss: 0.176088392735, acc: 0.940732777119\n",
      "Epoch: 658, loss: 0.184410437942, acc: 0.94019395113\n",
      "Epoch: 659, loss: 0.171469643712, acc: 0.945581912994\n",
      "Epoch: 660, loss: 0.223328694701, acc: 0.936422407627\n",
      "Val: 0.0404094830155\n",
      "Epoch: 661, loss: 0.189462289214, acc: 0.944504320621\n",
      "Epoch: 662, loss: 0.153337597847, acc: 0.950969815254\n",
      "Epoch: 663, loss: 0.198645070195, acc: 0.940732777119\n",
      "Epoch: 664, loss: 0.202581509948, acc: 0.938038766384\n",
      "Val: 0.0511853434145\n",
      "Epoch: 665, loss: 0.192467853427, acc: 0.945043087006\n",
      "Epoch: 666, loss: 0.16822899878, acc: 0.948814630508\n",
      "Epoch: 667, loss: 0.176686540246, acc: 0.948814630508\n",
      "Epoch: 668, loss: 0.187229052186, acc: 0.946120679379\n",
      "Val: 0.0511853434145\n",
      "Epoch: 669, loss: 0.210919365287, acc: 0.941810369492\n",
      "Epoch: 670, loss: 0.175252184272, acc: 0.947198271751\n",
      "Epoch: 671, loss: 0.161648496985, acc: 0.946120679379\n",
      "Epoch: 672, loss: 0.202516838908, acc: 0.945581912994\n",
      "Val: 0.0490301735699\n",
      "Epoch: 673, loss: 0.153818801045, acc: 0.954202592373\n",
      "Epoch: 674, loss: 0.174532592297, acc: 0.947198271751\n",
      "Epoch: 675, loss: 0.194756925106, acc: 0.943426728249\n",
      "Epoch: 676, loss: 0.168567538261, acc: 0.947198271751\n",
      "Val: 0.0452586188912\n",
      "Epoch: 677, loss: 0.18743288517, acc: 0.94019395113\n",
      "Epoch: 678, loss: 0.158305853605, acc: 0.945581912994\n",
      "Epoch: 679, loss: 0.159723460674, acc: 0.947198271751\n",
      "Epoch: 680, loss: 0.18833181262, acc: 0.940732777119\n",
      "Val: 0.0528017245233\n",
      "Epoch: 681, loss: 0.128859162331, acc: 0.960668087006\n",
      "Epoch: 682, loss: 0.191108807921, acc: 0.942349135876\n",
      "Epoch: 683, loss: 0.180824026465, acc: 0.94019395113\n",
      "Epoch: 684, loss: 0.135509505868, acc: 0.956896543503\n",
      "Val: 0.0528017245233\n",
      "Epoch: 685, loss: 0.200049027801, acc: 0.941810369492\n",
      "Epoch: 686, loss: 0.133078202605, acc: 0.962823271751\n",
      "Epoch: 687, loss: 0.165438622236, acc: 0.95043104887\n",
      "Epoch: 688, loss: 0.155452683568, acc: 0.948275864124\n",
      "Val: 0.046875\n",
      "Epoch: 689, loss: 0.159066900611, acc: 0.954202592373\n",
      "Epoch: 690, loss: 0.175142273307, acc: 0.946120679379\n",
      "Epoch: 691, loss: 0.171047493815, acc: 0.952047407627\n",
      "Epoch: 692, loss: 0.148086935282, acc: 0.948814630508\n",
      "Val: 0.0544181019068\n",
      "Epoch: 693, loss: 0.168013110757, acc: 0.947198271751\n",
      "Epoch: 694, loss: 0.148752883077, acc: 0.952586233616\n",
      "Epoch: 695, loss: 0.176900997758, acc: 0.940732777119\n",
      "Epoch: 696, loss: 0.176774799824, acc: 0.945043087006\n",
      "Val: 0.0560344830155\n",
      "Epoch: 697, loss: 0.143982544541, acc: 0.952586233616\n",
      "Epoch: 698, loss: 0.135104075074, acc: 0.960668087006\n",
      "Epoch: 699, loss: 0.134259432554, acc: 0.959051728249\n",
      "Epoch: 700, loss: 0.190735146403, acc: 0.945043087006\n",
      "Val: 0.0511853434145\n",
      "Epoch: 701, loss: 0.163242697716, acc: 0.953663766384\n",
      "Epoch: 702, loss: 0.162072554231, acc: 0.949353456497\n",
      "Epoch: 703, loss: 0.160052031279, acc: 0.951508641243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 704, loss: 0.173399060965, acc: 0.949892222881\n",
      "Val: 0.0484913811088\n",
      "Epoch: 705, loss: 0.176064923406, acc: 0.947198271751\n",
      "Epoch: 706, loss: 0.152169421315, acc: 0.955280184746\n",
      "Epoch: 707, loss: 0.16932824254, acc: 0.948814630508\n",
      "Epoch: 708, loss: 0.151986300945, acc: 0.951508641243\n",
      "Val: 0.0495689660311\n",
      "Epoch: 709, loss: 0.14276444912, acc: 0.956357777119\n",
      "Epoch: 710, loss: 0.169008612633, acc: 0.950969815254\n",
      "Epoch: 711, loss: 0.188741192222, acc: 0.938038766384\n",
      "Epoch: 712, loss: 0.164585009217, acc: 0.952047407627\n",
      "Val: 0.0495689660311\n",
      "Epoch: 713, loss: 0.155234634876, acc: 0.949353456497\n",
      "Epoch: 714, loss: 0.113944716752, acc: 0.95851290226\n",
      "Epoch: 715, loss: 0.141470238566, acc: 0.95581895113\n",
      "Epoch: 716, loss: 0.174821406603, acc: 0.94773709774\n",
      "Val: 0.0598060339689\n",
      "Epoch: 717, loss: 0.172383636236, acc: 0.944504320621\n",
      "Epoch: 718, loss: 0.146854966879, acc: 0.957974135876\n",
      "Epoch: 719, loss: 0.120486155152, acc: 0.960129320621\n",
      "Epoch: 720, loss: 0.135390624404, acc: 0.956357777119\n",
      "Val: 0.0501077584922\n",
      "Epoch: 721, loss: 0.163591772318, acc: 0.953125\n",
      "Epoch: 722, loss: 0.14295502007, acc: 0.955280184746\n",
      "Epoch: 723, loss: 0.191019549966, acc: 0.943426728249\n",
      "Epoch: 724, loss: 0.160956189036, acc: 0.946120679379\n",
      "Val: 0.0511853434145\n",
      "Epoch: 725, loss: 0.135366275907, acc: 0.957974135876\n",
      "Epoch: 726, loss: 0.137608096004, acc: 0.953663766384\n",
      "Epoch: 727, loss: 0.159796625376, acc: 0.948814630508\n",
      "Epoch: 728, loss: 0.129019349813, acc: 0.95581895113\n",
      "Val: 0.0452586188912\n",
      "Epoch: 729, loss: 0.16978892684, acc: 0.94773709774\n",
      "Epoch: 730, loss: 0.157146722078, acc: 0.953663766384\n",
      "Epoch: 731, loss: 0.145528748631, acc: 0.952047407627\n",
      "Epoch: 732, loss: 0.110807627439, acc: 0.956896543503\n",
      "Val: 0.0544181019068\n",
      "Epoch: 733, loss: 0.147832110524, acc: 0.956896543503\n",
      "Epoch: 734, loss: 0.168113037944, acc: 0.949353456497\n",
      "Epoch: 735, loss: 0.159021660686, acc: 0.951508641243\n",
      "Epoch: 736, loss: 0.14824706316, acc: 0.954741358757\n",
      "Val: 0.0490301735699\n",
      "Epoch: 737, loss: 0.167680725455, acc: 0.95043104887\n",
      "Epoch: 738, loss: 0.128719210625, acc: 0.961206912994\n",
      "Epoch: 739, loss: 0.161056518555, acc: 0.94773709774\n",
      "Epoch: 740, loss: 0.141127705574, acc: 0.952586233616\n",
      "Val: 0.0501077584922\n",
      "Epoch: 741, loss: 0.16116335988, acc: 0.952586233616\n",
      "Epoch: 742, loss: 0.142587855458, acc: 0.95581895113\n",
      "Epoch: 743, loss: 0.152950033545, acc: 0.955280184746\n",
      "Epoch: 744, loss: 0.160588800907, acc: 0.954741358757\n",
      "Val: 0.0490301735699\n",
      "Epoch: 745, loss: 0.192232072353, acc: 0.943426728249\n",
      "Epoch: 746, loss: 0.134004876018, acc: 0.956896543503\n",
      "Epoch: 747, loss: 0.165547192097, acc: 0.953125\n",
      "Epoch: 748, loss: 0.143312230706, acc: 0.957435369492\n",
      "Val: 0.0511853434145\n",
      "Epoch: 749, loss: 0.137811839581, acc: 0.95581895113\n",
      "Epoch: 750, loss: 0.140688985586, acc: 0.962284505367\n",
      "Epoch: 751, loss: 0.115565605462, acc: 0.960129320621\n",
      "Epoch: 752, loss: 0.152431339025, acc: 0.952047407627\n",
      "Val: 0.0619612075388\n",
      "Epoch: 753, loss: 0.145511880517, acc: 0.957435369492\n",
      "Epoch: 754, loss: 0.134782776237, acc: 0.95851290226\n",
      "Epoch: 755, loss: 0.151509389281, acc: 0.953125\n",
      "Epoch: 756, loss: 0.15858989954, acc: 0.949353456497\n",
      "Val: 0.0544181019068\n",
      "Epoch: 757, loss: 0.159255728126, acc: 0.952586233616\n",
      "Epoch: 758, loss: 0.162242889404, acc: 0.948814630508\n",
      "Epoch: 759, loss: 0.126752495766, acc: 0.960129320621\n",
      "Epoch: 760, loss: 0.126921400428, acc: 0.965517222881\n",
      "Val: 0.0598060339689\n",
      "Epoch: 761, loss: 0.124623946846, acc: 0.962284505367\n",
      "Epoch: 762, loss: 0.13586011529, acc: 0.960668087006\n",
      "Epoch: 763, loss: 0.148957759142, acc: 0.952047407627\n",
      "Epoch: 764, loss: 0.15896192193, acc: 0.95043104887\n",
      "Val: 0.0522629320621\n",
      "Epoch: 765, loss: 0.148824185133, acc: 0.953125\n",
      "Epoch: 766, loss: 0.16116052866, acc: 0.950969815254\n",
      "Epoch: 767, loss: 0.10050522536, acc: 0.96336209774\n",
      "Epoch: 768, loss: 0.1299302876, acc: 0.962823271751\n",
      "Val: 0.0490301735699\n",
      "Epoch: 769, loss: 0.177161976695, acc: 0.946659505367\n",
      "Epoch: 770, loss: 0.165679648519, acc: 0.956896543503\n",
      "Epoch: 771, loss: 0.133393079042, acc: 0.957435369492\n",
      "Epoch: 772, loss: 0.119939245284, acc: 0.965517222881\n",
      "Val: 0.0565732754767\n",
      "Epoch: 773, loss: 0.146087482572, acc: 0.956896543503\n",
      "Epoch: 774, loss: 0.138835445046, acc: 0.954741358757\n",
      "Epoch: 775, loss: 0.167822495103, acc: 0.949353456497\n",
      "Epoch: 776, loss: 0.133519634604, acc: 0.953663766384\n",
      "Val: 0.051724139601\n",
      "Epoch: 777, loss: 0.134083583951, acc: 0.957435369492\n",
      "Epoch: 778, loss: 0.137393683195, acc: 0.956896543503\n",
      "Epoch: 779, loss: 0.138840511441, acc: 0.95043104887\n",
      "Epoch: 780, loss: 0.147883027792, acc: 0.961206912994\n",
      "Val: 0.0598060339689\n",
      "Epoch: 781, loss: 0.139821529388, acc: 0.955280184746\n",
      "Epoch: 782, loss: 0.120442472398, acc: 0.962823271751\n",
      "Epoch: 783, loss: 0.133627697825, acc: 0.956357777119\n",
      "Epoch: 784, loss: 0.11227196455, acc: 0.962823271751\n",
      "Val: 0.0678879320621\n",
      "Epoch: 785, loss: 0.112760618329, acc: 0.962284505367\n",
      "Epoch: 786, loss: 0.152103409171, acc: 0.95581895113\n",
      "Epoch: 787, loss: 0.148247525096, acc: 0.950969815254\n",
      "Epoch: 788, loss: 0.144795358181, acc: 0.953125\n",
      "Val: 0.0538793094456\n",
      "Epoch: 789, loss: 0.115876957774, acc: 0.962284505367\n",
      "Epoch: 790, loss: 0.115554101765, acc: 0.96336209774\n",
      "Epoch: 791, loss: 0.165249526501, acc: 0.950969815254\n",
      "Epoch: 792, loss: 0.12813089788, acc: 0.959051728249\n",
      "Val: 0.0522629320621\n",
      "Epoch: 793, loss: 0.138371273875, acc: 0.955280184746\n",
      "Epoch: 794, loss: 0.114155903459, acc: 0.964978456497\n",
      "Epoch: 795, loss: 0.150354132056, acc: 0.961206912994\n",
      "Epoch: 796, loss: 0.11467204988, acc: 0.964978456497\n",
      "Val: 0.0479525849223\n",
      "Epoch: 797, loss: 0.131064504385, acc: 0.957974135876\n",
      "Epoch: 798, loss: 0.132842868567, acc: 0.95851290226\n",
      "Epoch: 799, loss: 0.139507904649, acc: 0.959590494633\n",
      "Epoch: 800, loss: 0.131750181317, acc: 0.962284505367\n",
      "Val: 0.0495689660311\n",
      "Epoch: 801, loss: 0.136563390493, acc: 0.956896543503\n",
      "Epoch: 802, loss: 0.121569216251, acc: 0.96336209774\n",
      "Epoch: 803, loss: 0.14391040802, acc: 0.957435369492\n",
      "Epoch: 804, loss: 0.120016738772, acc: 0.964439630508\n",
      "Val: 0.0592672415078\n",
      "Epoch: 805, loss: 0.171384200454, acc: 0.953663766384\n",
      "Epoch: 806, loss: 0.149558350444, acc: 0.956896543503\n",
      "Epoch: 807, loss: 0.14952352643, acc: 0.953663766384\n",
      "Epoch: 808, loss: 0.106979310513, acc: 0.960668087006\n",
      "Val: 0.0538793094456\n",
      "Epoch: 809, loss: 0.134652361274, acc: 0.95851290226\n",
      "Epoch: 810, loss: 0.125758633018, acc: 0.957974135876\n",
      "Epoch: 811, loss: 0.131779327989, acc: 0.959590494633\n",
      "Epoch: 812, loss: 0.146272853017, acc: 0.952047407627\n",
      "Val: 0.0474137924612\n",
      "Epoch: 813, loss: 0.111707225442, acc: 0.96605604887\n",
      "Epoch: 814, loss: 0.112385764718, acc: 0.96605604887\n",
      "Epoch: 815, loss: 0.107823289931, acc: 0.964439630508\n",
      "Epoch: 816, loss: 0.123476549983, acc: 0.960129320621\n",
      "Val: 0.0554956905544\n",
      "Epoch: 817, loss: 0.118991263211, acc: 0.962823271751\n",
      "Epoch: 818, loss: 0.125177234411, acc: 0.96336209774\n",
      "Epoch: 819, loss: 0.120690137148, acc: 0.963900864124\n",
      "Epoch: 820, loss: 0.107277080417, acc: 0.961206912994\n",
      "Val: 0.0614224150777\n",
      "Epoch: 821, loss: 0.141747295856, acc: 0.960129320621\n",
      "Epoch: 822, loss: 0.12661151588, acc: 0.963900864124\n",
      "Epoch: 823, loss: 0.125108361244, acc: 0.961745679379\n",
      "Epoch: 824, loss: 0.132485032082, acc: 0.960129320621\n",
      "Val: 0.0528017245233\n",
      "Epoch: 825, loss: 0.116253398359, acc: 0.961206912994\n",
      "Epoch: 826, loss: 0.123027011752, acc: 0.961206912994\n",
      "Epoch: 827, loss: 0.126337438822, acc: 0.956896543503\n",
      "Epoch: 828, loss: 0.13657541573, acc: 0.959590494633\n",
      "Val: 0.0404094830155\n",
      "Epoch: 829, loss: 0.121632620692, acc: 0.969827592373\n",
      "Epoch: 830, loss: 0.134978830814, acc: 0.953125\n",
      "Epoch: 831, loss: 0.118008680642, acc: 0.96336209774\n",
      "Epoch: 832, loss: 0.139729231596, acc: 0.954202592373\n",
      "Val: 0.0474137924612\n",
      "Epoch: 833, loss: 0.0840468555689, acc: 0.971982777119\n",
      "Epoch: 834, loss: 0.148653998971, acc: 0.95581895113\n",
      "Epoch: 835, loss: 0.169035866857, acc: 0.954202592373\n",
      "Epoch: 836, loss: 0.168753400445, acc: 0.949353456497\n",
      "Val: 0.0549568980932\n",
      "Epoch: 837, loss: 0.134483292699, acc: 0.959051728249\n",
      "Epoch: 838, loss: 0.118184693158, acc: 0.965517222881\n",
      "Epoch: 839, loss: 0.114314854145, acc: 0.966594815254\n",
      "Epoch: 840, loss: 0.150672718883, acc: 0.95581895113\n",
      "Val: 0.0549568980932\n",
      "Epoch: 841, loss: 0.123853944242, acc: 0.957974135876\n",
      "Epoch: 842, loss: 0.11171412468, acc: 0.962823271751\n",
      "Epoch: 843, loss: 0.0960046723485, acc: 0.964978456497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 844, loss: 0.126095160842, acc: 0.961206912994\n",
      "Val: 0.0495689660311\n",
      "Epoch: 845, loss: 0.138250827789, acc: 0.95851290226\n",
      "Epoch: 846, loss: 0.105720944703, acc: 0.967672407627\n",
      "Epoch: 847, loss: 0.147515505552, acc: 0.953663766384\n",
      "Epoch: 848, loss: 0.124396130443, acc: 0.962823271751\n",
      "Val: 0.0495689660311\n",
      "Epoch: 849, loss: 0.0989119485021, acc: 0.971982777119\n",
      "Epoch: 850, loss: 0.101234406233, acc: 0.964439630508\n",
      "Epoch: 851, loss: 0.121496662498, acc: 0.960668087006\n",
      "Epoch: 852, loss: 0.153327032924, acc: 0.951508641243\n",
      "Val: 0.0441810339689\n",
      "Epoch: 853, loss: 0.123002320528, acc: 0.964978456497\n",
      "Epoch: 854, loss: 0.125493079424, acc: 0.961206912994\n",
      "Epoch: 855, loss: 0.115887179971, acc: 0.969288766384\n",
      "Epoch: 856, loss: 0.123298935592, acc: 0.957435369492\n",
      "Val: 0.0457974150777\n",
      "Epoch: 857, loss: 0.109112337232, acc: 0.96605604887\n",
      "Epoch: 858, loss: 0.130911156535, acc: 0.962823271751\n",
      "Epoch: 859, loss: 0.10415353626, acc: 0.964439630508\n",
      "Epoch: 860, loss: 0.141099959612, acc: 0.954202592373\n",
      "Val: 0.0452586188912\n",
      "Epoch: 861, loss: 0.0834166109562, acc: 0.975754320621\n",
      "Epoch: 862, loss: 0.115920469165, acc: 0.966594815254\n",
      "Epoch: 863, loss: 0.108049601316, acc: 0.965517222881\n",
      "Epoch: 864, loss: 0.0800816863775, acc: 0.970905184746\n",
      "Val: 0.0479525849223\n",
      "Epoch: 865, loss: 0.136454030871, acc: 0.961206912994\n",
      "Epoch: 866, loss: 0.142454832792, acc: 0.960668087006\n",
      "Epoch: 867, loss: 0.121545419097, acc: 0.961745679379\n",
      "Epoch: 868, loss: 0.126599892974, acc: 0.960668087006\n",
      "Val: 0.0581896565855\n",
      "Epoch: 869, loss: 0.135725349188, acc: 0.957435369492\n",
      "Epoch: 870, loss: 0.106046393514, acc: 0.96605604887\n",
      "Epoch: 871, loss: 0.13520732522, acc: 0.96336209774\n",
      "Epoch: 872, loss: 0.146108880639, acc: 0.959051728249\n",
      "Val: 0.0484913811088\n",
      "Epoch: 873, loss: 0.113719940186, acc: 0.964978456497\n",
      "Epoch: 874, loss: 0.125623822212, acc: 0.960668087006\n",
      "Epoch: 875, loss: 0.104184955359, acc: 0.970366358757\n",
      "Epoch: 876, loss: 0.127079814672, acc: 0.960668087006\n",
      "Val: 0.0436422415078\n",
      "Epoch: 877, loss: 0.140160247684, acc: 0.948814630508\n",
      "Epoch: 878, loss: 0.122974418104, acc: 0.962823271751\n",
      "Epoch: 879, loss: 0.136182770133, acc: 0.956896543503\n",
      "Epoch: 880, loss: 0.113306283951, acc: 0.967672407627\n",
      "Val: 0.0414870679379\n",
      "Epoch: 881, loss: 0.115861728787, acc: 0.962823271751\n",
      "Epoch: 882, loss: 0.0843469128013, acc: 0.97413790226\n",
      "Epoch: 883, loss: 0.119633205235, acc: 0.965517222881\n",
      "Epoch: 884, loss: 0.110321871936, acc: 0.965517222881\n",
      "Val: 0.0549568980932\n",
      "Epoch: 885, loss: 0.123651325703, acc: 0.961745679379\n",
      "Epoch: 886, loss: 0.111107885838, acc: 0.967133641243\n",
      "Epoch: 887, loss: 0.140472605824, acc: 0.955280184746\n",
      "Epoch: 888, loss: 0.115330472589, acc: 0.96336209774\n",
      "Val: 0.0452586188912\n",
      "Epoch: 889, loss: 0.101176582277, acc: 0.969288766384\n",
      "Epoch: 890, loss: 0.117335855961, acc: 0.968211233616\n",
      "Epoch: 891, loss: 0.0941037908196, acc: 0.971982777119\n",
      "Epoch: 892, loss: 0.140543758869, acc: 0.957974135876\n",
      "Val: 0.0484913811088\n",
      "Epoch: 893, loss: 0.163179576397, acc: 0.95581895113\n",
      "Epoch: 894, loss: 0.0868159458041, acc: 0.973060369492\n",
      "Epoch: 895, loss: 0.12282499671, acc: 0.961206912994\n",
      "Epoch: 896, loss: 0.123385861516, acc: 0.959051728249\n",
      "Val: 0.0495689660311\n",
      "Epoch: 897, loss: 0.114514507353, acc: 0.959590494633\n",
      "Epoch: 898, loss: 0.092100687325, acc: 0.968211233616\n",
      "Epoch: 899, loss: 0.0912990644574, acc: 0.970366358757\n",
      "Epoch: 900, loss: 0.123010367155, acc: 0.967133641243\n",
      "Val: 0.0463362075388\n",
      "Epoch: 901, loss: 0.114414982498, acc: 0.963900864124\n",
      "Epoch: 902, loss: 0.10983274132, acc: 0.968211233616\n",
      "Epoch: 903, loss: 0.139737665653, acc: 0.959051728249\n",
      "Epoch: 904, loss: 0.107206642628, acc: 0.973060369492\n",
      "Val: 0.0495689660311\n",
      "Epoch: 905, loss: 0.126410573721, acc: 0.961745679379\n",
      "Epoch: 906, loss: 0.106790713966, acc: 0.968211233616\n",
      "Epoch: 907, loss: 0.0945488885045, acc: 0.96605604887\n",
      "Epoch: 908, loss: 0.116447500885, acc: 0.966594815254\n",
      "Val: 0.0506465509534\n",
      "Epoch: 909, loss: 0.11013905704, acc: 0.967672407627\n",
      "Epoch: 910, loss: 0.0987143665552, acc: 0.973599135876\n",
      "Epoch: 911, loss: 0.10632006079, acc: 0.96875\n",
      "Epoch: 912, loss: 0.102872982621, acc: 0.964978456497\n",
      "Val: 0.0544181019068\n",
      "Epoch: 913, loss: 0.107156813145, acc: 0.96605604887\n",
      "Epoch: 914, loss: 0.126328974962, acc: 0.96605604887\n",
      "Epoch: 915, loss: 0.115845367312, acc: 0.963900864124\n",
      "Epoch: 916, loss: 0.122912451625, acc: 0.959051728249\n",
      "Val: 0.0549568980932\n",
      "Epoch: 917, loss: 0.107804581523, acc: 0.964439630508\n",
      "Epoch: 918, loss: 0.125996410847, acc: 0.962823271751\n",
      "Epoch: 919, loss: 0.0852479711175, acc: 0.96875\n",
      "Epoch: 920, loss: 0.11797849834, acc: 0.964978456497\n",
      "Val: 0.0501077584922\n",
      "Epoch: 921, loss: 0.12610155344, acc: 0.95851290226\n",
      "Epoch: 922, loss: 0.123306378722, acc: 0.968211233616\n",
      "Epoch: 923, loss: 0.0951559320092, acc: 0.967672407627\n",
      "Epoch: 924, loss: 0.103072345257, acc: 0.96605604887\n",
      "Val: 0.0414870679379\n",
      "Epoch: 925, loss: 0.125406324863, acc: 0.961206912994\n",
      "Epoch: 926, loss: 0.139151737094, acc: 0.963900864124\n",
      "Epoch: 927, loss: 0.0807053744793, acc: 0.976831912994\n",
      "Epoch: 928, loss: 0.108654052019, acc: 0.964978456497\n",
      "Val: 0.0495689660311\n",
      "Epoch: 929, loss: 0.125020414591, acc: 0.96336209774\n",
      "Epoch: 930, loss: 0.0869109556079, acc: 0.973599135876\n",
      "Epoch: 931, loss: 0.114460997283, acc: 0.961745679379\n",
      "Epoch: 932, loss: 0.101151198149, acc: 0.966594815254\n",
      "Val: 0.0533405169845\n",
      "Epoch: 933, loss: 0.10361725837, acc: 0.967133641243\n",
      "Epoch: 934, loss: 0.0863673090935, acc: 0.967133641243\n",
      "Epoch: 935, loss: 0.100598141551, acc: 0.967133641243\n",
      "Epoch: 936, loss: 0.0937637984753, acc: 0.973599135876\n",
      "Val: 0.0528017245233\n",
      "Epoch: 937, loss: 0.125965729356, acc: 0.96336209774\n",
      "Epoch: 938, loss: 0.0985043495893, acc: 0.969827592373\n",
      "Epoch: 939, loss: 0.116553537548, acc: 0.96605604887\n",
      "Epoch: 940, loss: 0.0812748074532, acc: 0.971982777119\n",
      "Val: 0.0598060339689\n",
      "Epoch: 941, loss: 0.0974808782339, acc: 0.966594815254\n",
      "Epoch: 942, loss: 0.0985220000148, acc: 0.964439630508\n",
      "Epoch: 943, loss: 0.109309278429, acc: 0.96336209774\n",
      "Epoch: 944, loss: 0.0933030545712, acc: 0.970366358757\n",
      "Val: 0.0554956905544\n",
      "Epoch: 945, loss: 0.147067800164, acc: 0.962284505367\n",
      "Epoch: 946, loss: 0.0901774838567, acc: 0.973060369492\n",
      "Epoch: 947, loss: 0.101736277342, acc: 0.970366358757\n",
      "Epoch: 948, loss: 0.115438230336, acc: 0.964978456497\n",
      "Val: 0.0474137924612\n",
      "Epoch: 949, loss: 0.108833156526, acc: 0.960668087006\n",
      "Epoch: 950, loss: 0.0714283809066, acc: 0.975215494633\n",
      "Epoch: 951, loss: 0.121516399086, acc: 0.962823271751\n",
      "Epoch: 952, loss: 0.0984116047621, acc: 0.973060369492\n",
      "Val: 0.0463362075388\n",
      "Epoch: 953, loss: 0.137599125504, acc: 0.964978456497\n",
      "Epoch: 954, loss: 0.109090216458, acc: 0.966594815254\n",
      "Epoch: 955, loss: 0.0922576785088, acc: 0.967133641243\n",
      "Epoch: 956, loss: 0.102290660143, acc: 0.96605604887\n",
      "Val: 0.0533405169845\n",
      "Epoch: 957, loss: 0.107813864946, acc: 0.96605604887\n",
      "Epoch: 958, loss: 0.0955133140087, acc: 0.965517222881\n",
      "Epoch: 959, loss: 0.123762533069, acc: 0.963900864124\n",
      "Epoch: 960, loss: 0.06110233441, acc: 0.976831912994\n",
      "Val: 0.051724139601\n",
      "Epoch: 961, loss: 0.0975226387382, acc: 0.971982777119\n",
      "Epoch: 962, loss: 0.0788826420903, acc: 0.978448271751\n",
      "Epoch: 963, loss: 0.0725587978959, acc: 0.977370679379\n",
      "Epoch: 964, loss: 0.104277275503, acc: 0.969827592373\n",
      "Val: 0.0560344830155\n",
      "Epoch: 965, loss: 0.0975605398417, acc: 0.966594815254\n",
      "Epoch: 966, loss: 0.0896844342351, acc: 0.974676728249\n",
      "Epoch: 967, loss: 0.100024804473, acc: 0.961745679379\n",
      "Epoch: 968, loss: 0.10266829282, acc: 0.973599135876\n",
      "Val: 0.0522629320621\n",
      "Epoch: 969, loss: 0.108692809939, acc: 0.967133641243\n",
      "Epoch: 970, loss: 0.0849837660789, acc: 0.974676728249\n",
      "Epoch: 971, loss: 0.10029759258, acc: 0.969827592373\n",
      "Epoch: 972, loss: 0.103594094515, acc: 0.97144395113\n",
      "Val: 0.0533405169845\n",
      "Epoch: 973, loss: 0.0911835134029, acc: 0.97144395113\n",
      "Epoch: 974, loss: 0.0730934292078, acc: 0.976293087006\n",
      "Epoch: 975, loss: 0.0844333916903, acc: 0.970905184746\n",
      "Epoch: 976, loss: 0.0976266413927, acc: 0.962823271751\n",
      "Val: 0.0479525849223\n",
      "Epoch: 977, loss: 0.0999979525805, acc: 0.968211233616\n",
      "Epoch: 978, loss: 0.120126143098, acc: 0.96336209774\n",
      "Epoch: 979, loss: 0.0716091319919, acc: 0.976831912994\n",
      "Epoch: 980, loss: 0.0983524397016, acc: 0.969827592373\n",
      "Val: 0.0501077584922\n",
      "Epoch: 981, loss: 0.0882267206907, acc: 0.971982777119\n",
      "Epoch: 982, loss: 0.113153062761, acc: 0.963900864124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 983, loss: 0.0856137573719, acc: 0.975215494633\n",
      "Epoch: 984, loss: 0.139146909118, acc: 0.964439630508\n",
      "Val: 0.0581896565855\n",
      "Epoch: 985, loss: 0.118464767933, acc: 0.964439630508\n",
      "Epoch: 986, loss: 0.0951532796025, acc: 0.970905184746\n",
      "Epoch: 987, loss: 0.0696032643318, acc: 0.97898709774\n",
      "Epoch: 988, loss: 0.0873599126935, acc: 0.973060369492\n",
      "Val: 0.0501077584922\n",
      "Epoch: 989, loss: 0.124139614403, acc: 0.960129320621\n",
      "Epoch: 990, loss: 0.123172648251, acc: 0.964439630508\n",
      "Epoch: 991, loss: 0.112481966615, acc: 0.968211233616\n",
      "Epoch: 992, loss: 0.112447097898, acc: 0.963900864124\n",
      "Val: 0.0565732754767\n",
      "Epoch: 993, loss: 0.106271222234, acc: 0.967133641243\n",
      "Epoch: 994, loss: 0.0817582681775, acc: 0.974676728249\n",
      "Epoch: 995, loss: 0.112483918667, acc: 0.967133641243\n",
      "Epoch: 996, loss: 0.0852271392941, acc: 0.970366358757\n",
      "Val: 0.0538793094456\n",
      "Epoch: 997, loss: 0.0900985747576, acc: 0.973599135876\n",
      "Epoch: 998, loss: 0.121361903846, acc: 0.956357777119\n",
      "Epoch: 999, loss: 0.142417728901, acc: 0.957974135876\n",
      "Epoch: 1000, loss: 0.107264339924, acc: 0.964978456497\n",
      "Val: 0.0495689660311\n"
     ]
    }
   ],
   "source": [
    "inceptionv3.train_and_monitor_with_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "'''\n",
    "50% corrupted 50% uncorrupted REP 1 \n",
    "\n",
    "Train generator ready, time elapsed: 19.7664361\n",
    "Epoch: 0, loss: 4.23722171783, acc: 0.0123922415078\n",
    "...\n",
    "Epoch: 1000, loss: 0.107264339924, acc: 0.964978456497\n",
    "Val: 0.0495689660311'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [0.0267857145518, 0.0267857145518, 0.0178571436554, 0.0178571436554, 0.0178571436554, 0.0178571436554, 0.0189732145518, 0.015625, 0.0178571436554, 0.0234375, 0.0223214291036, 0.0245535708964, 0.0145089281723, 0.0234375, 0.0279017854482, 0.015625, 0.0189732145518, 0.0167410708964, 0.0200892854482, 0.0212053563446, 0.0256696436554, 0.0189732145518, 0.0234375, 0.0212053563446, 0.0189732145518, 0.0167410708964, 0.0234375, 0.0212053563446, 0.0189732145518, 0.0167410708964, 0.0189732145518, 0.0223214291036, 0.0189732145518, 0.0178571436554, 0.0234375, 0.0290178563446, 0.0212053563446, 0.0223214291036, 0.0223214291036, 0.0212053563446, 0.0178571436554, 0.0200892854482, 0.0133928572759, 0.0167410708964, 0.0178571436554, 0.0256696436554, 0.0267857145518, 0.0267857145518, 0.0145089281723, 0.0223214291036, 0.0223214291036, 0.0212053563446, 0.0234375, 0.015625, 0.0234375, 0.0256696436554, 0.0212053563446, 0.0178571436554, 0.0167410708964, 0.0279017854482, 0.0279017854482, 0.0267857145518, 0.0267857145518, 0.0279017854482, 0.0279017854482, 0.0279017854482, 0.0334821417928, 0.0301339291036, 0.0290178563446, 0.0290178563446, 0.0301339291036, 0.0345982126892, 0.03125, 0.0200892854482, 0.0256696436554, 0.0323660708964, 0.0234375, 0.0345982126892, 0.0256696436554, 0.0279017854482, 0.0245535708964, 0.0267857145518, 0.0234375, 0.0234375, 0.0301339291036, 0.0357142873108, 0.0334821417928, 0.0290178563446, 0.0301339291036, 0.0256696436554, 0.0167410708964, 0.0212053563446, 0.0301339291036, 0.0267857145518, 0.0368303582072, 0.0390625, 0.0513392873108, 0.0334821417928, 0.0323660708964, 0.0368303582072, 0.0334821417928, 0.0435267873108, 0.0279017854482, 0.0412946417928, 0.0412946417928, 0.0290178563446, 0.0323660708964, 0.0379464291036, 0.0323660708964, 0.0412946417928, 0.0379464291036, 0.0379464291036, 0.0379464291036, 0.0301339291036, 0.0558035708964, 0.0457589291036, 0.046875, 0.0357142873108, 0.0379464291036, 0.0357142873108, 0.0446428582072, 0.0491071417928, 0.046875, 0.0345982126892, 0.0479910708964, 0.0457589291036, 0.0479910708964, 0.046875, 0.0446428582072, 0.0424107126892, 0.0401785708964, 0.0435267873108, 0.0502232126892, 0.0435267873108, 0.0457589291036, 0.0435267873108, 0.0479910708964, 0.0479910708964, 0.0401785708964, 0.0479910708964, 0.0345982126892, 0.0535714291036, 0.0591517873108, 0.0513392873108, 0.0546875, 0.0502232126892, 0.0535714291036, 0.0580357126892, 0.0546875, 0.0546875, 0.0569196417928, 0.0658482164145, 0.0658482164145, 0.046875, 0.0524553582072, 0.0680803582072, 0.0725446417928, 0.0613839291036, 0.0513392873108, 0.0535714291036, 0.0602678582072, 0.0825892835855, 0.0636160746217, 0.0703125, 0.0691964253783, 0.0825892835855, 0.0792410746217, 0.0758928582072, 0.0859375, 0.0825892835855, 0.0848214253783, 0.0848214253783, 0.0892857164145, 0.0948660746217, 0.0892857164145, 0.104910716414, 0.106026783586, 0.0982142835855, 0.0970982164145, 0.108258925378, 0.09375, 0.122767858207, 0.104910716414, 0.108258925378, 0.106026783586, 0.0926339253783, 0.111607141793, 0.0948660746217, 0.137276783586, 0.121651783586, 0.130580350757, 0.108258925378, 0.112723216414, 0.137276783586, 0.139508932829, 0.130580350757, 0.129464283586, 0.125, 0.135044649243, 0.170758932829, 0.170758932829, 0.159598216414, 0.175223216414, 0.185267850757, 0.175223216414, 0.165178567171, 0.178571432829, 0.159598216414, 0.159598216414, 0.138392850757, 0.155133932829, 0.196428567171, 0.200892850757, 0.214285716414, 0.207589283586, 0.216517850757, 0.213169649243, 0.2265625, 0.193080350757, 0.225446432829, 0.193080350757, 0.224330350757, 0.189732149243, 0.227678567171, 0.198660716414, 0.219866067171, 0.231026783586, 0.235491067171, 0.216517850757, 0.253348201513, 0.252232134342, 0.246651783586, 0.290178567171, 0.263392865658, 0.256696432829, 0.286830365658, 0.229910716414, 0.276785701513, 0.256696432829, 0.265625, 0.292410701513, 0.243303567171, 0.280133932829, 0.272321432829, 0.3046875, 0.25, 0.315848201513, 0.318080365658, 0.266741067171, 0.328125, 0.309151798487, 0.28125, 0.292410701513, 0.337053567171, 0.300223201513, 0.309151798487, 0.266741067171, 0.318080365658, 0.359375, 0.387276798487, 0.350446432829, 0.314732134342, 0.318080365658, 0.3671875, 0.329241067171, 0.306919634342, 0.310267865658, 0.341517865658, 0.408482134342, 0.368303567171, 0.396205365658, 0.360491067171, 0.388392865658, 0.339285701513, 0.397321432829, 0.371651798487, 0.391741067171, 0.357142865658, 0.375, 0.390625, 0.412946432829, 0.40625, 0.401785701513, 0.396205365658, 0.371651798487, 0.408482134342, 0.400669634342, 0.337053567171, 0.438616067171, 0.405133932829, 0.407366067171, 0.4140625, 0.459821432829, 0.368303567171, 0.408482134342, 0.4296875, 0.439732134342, 0.435267865658, 0.456473201513, 0.431919634342, 0.448660701513, 0.458705365658, 0.430803567171, 0.440848201513, 0.430803567171, 0.419642865658, 0.5, 0.376116067171, 0.450892865658, 0.402901798487, 0.452008932829, 0.464285701513, 0.477678567171, 0.508928596973, 0.5078125, 0.483258932829, 0.479910701513, 0.488839298487, 0.482142865658, 0.522321403027, 0.4609375, 0.497767865658, 0.422991067171, 0.485491067171, 0.427455365658, 0.481026798487, 0.497767865658, 0.487723201513, 0.508928596973, 0.495535701513, 0.513392865658, 0.550223231316, 0.521205365658, 0.502232134342, 0.493303567171, 0.473214298487, 0.581473231316, 0.458705365658, 0.515625, 0.541294634342, 0.508928596973, 0.545758903027, 0.518973231316, 0.505580365658, 0.483258932829, 0.501116096973, 0.535714268684, 0.522321403027, 0.577008903027, 0.565848231316, 0.502232134342, 0.542410731316, 0.53125, 0.526785731316, 0.577008903027, 0.498883932829, 0.514508903027, 0.555803596973, 0.6015625, 0.582589268684, 0.556919634342, 0.572544634342, 0.547991096973, 0.558035731316, 0.536830365658, 0.556919634342, 0.550223231316, 0.598214268684, 0.595982134342, 0.572544634342, 0.590401768684, 0.573660731316, 0.543526768684, 0.580357134342, 0.578125, 0.578125, 0.620535731316, 0.560267865658, 0.579241096973, 0.595982134342, 0.613839268684, 0.59375, 0.479910701513, 0.563616096973, 0.581473231316, 0.581473231316, 0.587053596973, 0.623883903027, 0.612723231316, 0.612723231316, 0.555803596973, 0.635044634342, 0.613839268684, 0.606026768684, 0.513392865658, 0.6171875, 0.535714268684, 0.5546875, 0.587053596973, 0.636160731316, 0.595982134342, 0.591517865658, 0.589285731316, 0.628348231316, 0.639508903027, 0.619419634342, 0.59375, 0.569196403027, 0.569196403027, 0.639508903027, 0.65625, 0.619419634342, 0.583705365658, 0.591517865658, 0.626116096973, 0.641741096973, 0.608258903027, 0.631696403027, 0.607142865658, 0.649553596973, 0.609375, 0.579241096973, 0.623883903027, 0.640625, 0.612723231316, 0.503348231316, 0.602678596973, 0.628348231316, 0.620535731316, 0.577008903027, 0.686383903027, 0.631696403027, 0.660714268684, 0.657366096973, 0.59375, 0.602678596973, 0.599330365658, 0.638392865658, 0.689732134342, 0.699776768684, 0.623883903027, 0.613839268684, 0.625, 0.655133903027, 0.598214268684, 0.598214268684, 0.564732134342, 0.626116096973, 0.630580365658, 0.685267865658, 0.661830365658, 0.652901768684, 0.625, 0.674107134342, 0.654017865658, 0.640625, 0.635044634342, 0.671875, 0.689732134342, 0.639508903027, 0.645089268684, 0.621651768684, 0.669642865658, 0.584821403027, 0.676339268684, 0.665178596973, 0.671875, 0.59375, 0.667410731316, 0.685267865658, 0.619419634342, 0.640625, 0.699776768684, 0.635044634342, 0.635044634342, 0.633928596973, 0.621651768684, 0.666294634342, 0.669642865658, 0.65625, 0.706473231316, 0.622767865658, 0.693080365658, 0.668526768684, 0.702008903027, 0.683035731316, 0.722098231316, 0.714285731316, 0.637276768684, 0.65625, 0.667410731316, 0.662946403027, 0.699776768684, 0.694196403027, 0.628348231316, 0.693080365658, 0.697544634342, 0.706473231316, 0.597098231316, 0.703125, 0.707589268684, 0.686383903027, 0.724330365658, 0.677455365658, 0.6796875, 0.654017865658, 0.685267865658, 0.75, 0.685267865658, 0.668526768684, 0.666294634342, 0.705357134342, 0.697544634342, 0.646205365658, 0.713169634342, 0.690848231316, 0.681919634342, 0.680803596973, 0.647321403027, 0.660714268684, 0.729910731316, 0.705357134342, 0.6953125, 0.641741096973, 0.6796875, 0.689732134342, 0.735491096973, 0.684151768684, 0.690848231316, 0.706473231316, 0.690848231316, 0.717633903027, 0.7421875, 0.7265625, 0.752232134342, 0.669642865658, 0.714285731316, 0.665178596973, 0.65625, 0.674107134342, 0.678571403027, 0.727678596973, 0.6953125, 0.753348231316, 0.734375, 0.736607134342, 0.708705365658, 0.720982134342, 0.733258903027, 0.727678596973, 0.652901768684, 0.668526768684, 0.801339268684, 0.713169634342, 0.733258903027, 0.685267865658, 0.720982134342, 0.732142865658, 0.793526768684, 0.744419634342, 0.739955365658, 0.697544634342, 0.777901768684, 0.731026768684, 0.697544634342, 0.738839268684, 0.744419634342, 0.796875, 0.736607134342, 0.723214268684, 0.6796875, 0.699776768684, 0.758928596973, 0.78125, 0.743303596973, 0.739955365658, 0.727678596973, 0.7421875, 0.734375, 0.764508903027, 0.727678596973, 0.694196403027, 0.763392865658, 0.694196403027, 0.776785731316, 0.729910731316, 0.729910731316, 0.717633903027, 0.681919634342, 0.735491096973, 0.729910731316, 0.784598231316, 0.783482134342, 0.731026768684, 0.738839268684, 0.748883903027, 0.733258903027, 0.755580365658, 0.697544634342, 0.678571403027, 0.696428596973, 0.753348231316, 0.782366096973, 0.783482134342, 0.795758903027, 0.723214268684, 0.71875, 0.728794634342, 0.717633903027, 0.640625, 0.733258903027, 0.715401768684, 0.720982134342, 0.763392865658, 0.741071403027, 0.758928596973, 0.751116096973, 0.714285731316, 0.777901768684, 0.731026768684, 0.758928596973, 0.736607134342, 0.763392865658, 0.752232134342, 0.758928596973, 0.775669634342, 0.765625, 0.761160731316, 0.681919634342, 0.720982134342, 0.794642865658, 0.809151768684, 0.811383903027, 0.747767865658, 0.814732134342, 0.784598231316, 0.772321403027, 0.720982134342, 0.767857134342, 0.752232134342, 0.731026768684, 0.752232134342, 0.754464268684, 0.758928596973, 0.744419634342, 0.732142865658, 0.739955365658, 0.770089268684, 0.777901768684, 0.814732134342, 0.761160731316, 0.709821403027, 0.729910731316, 0.755580365658, 0.75, 0.746651768684, 0.785714268684, 0.743303596973, 0.71875, 0.751116096973, 0.731026768684, 0.734375, 0.734375, 0.761160731316, 0.779017865658, 0.792410731316, 0.803571403027, 0.786830365658, 0.810267865658, 0.803571403027, 0.767857134342, 0.723214268684, 0.7890625, 0.763392865658, 0.658482134342, 0.747767865658, 0.722098231316, 0.823660731316, 0.799107134342, 0.776785731316, 0.819196403027, 0.809151768684, 0.779017865658, 0.7890625, 0.764508903027, 0.765625, 0.782366096973, 0.786830365658, 0.811383903027, 0.806919634342, 0.801339268684, 0.783482134342, 0.756696403027, 0.761160731316, 0.706473231316, 0.770089268684, 0.729910731316, 0.838169634342, 0.780133903027, 0.784598231316, 0.799107134342, 0.733258903027, 0.760044634342, 0.764508903027, 0.815848231316, 0.801339268684, 0.828125, 0.830357134342, 0.768973231316, 0.767857134342, 0.760044634342, 0.7890625, 0.816964268684, 0.774553596973, 0.766741096973, 0.803571403027, 0.784598231316, 0.810267865658, 0.7578125, 0.821428596973, 0.850446403027, 0.790178596973, 0.707589268684, 0.78125, 0.821428596973, 0.803571403027, 0.784598231316, 0.761160731316, 0.774553596973, 0.796875, 0.783482134342, 0.805803596973, 0.827008903027, 0.774553596973, 0.7734375, 0.760044634342, 0.828125, 0.774553596973, 0.78125, 0.7265625, 0.758928596973, 0.809151768684, 0.785714268684, 0.708705365658, 0.810267865658, 0.722098231316, 0.739955365658, 0.739955365658, 0.751116096973, 0.847098231316, 0.810267865658, 0.731026768684, 0.777901768684, 0.768973231316, 0.796875, 0.870535731316, 0.811383903027, 0.758928596973, 0.809151768684, 0.794642865658, 0.787946403027, 0.824776768684, 0.799107134342, 0.838169634342, 0.7734375, 0.771205365658, 0.799107134342, 0.830357134342, 0.760044634342, 0.823660731316, 0.739955365658, 0.828125, 0.819196403027, 0.8125, 0.833705365658, 0.801339268684, 0.8046875, 0.8203125, 0.758928596973, 0.834821403027, 0.8046875, 0.791294634342, 0.806919634342, 0.857142865658, 0.821428596973, 0.801339268684, 0.771205365658, 0.862723231316, 0.825892865658, 0.814732134342, 0.839285731316, 0.797991096973, 0.777901768684, 0.806919634342, 0.818080365658, 0.815848231316, 0.8046875, 0.861607134342, 0.828125, 0.780133903027, 0.768973231316, 0.716517865658, 0.752232134342, 0.738839268684, 0.808035731316, 0.828125, 0.821428596973, 0.839285731316, 0.815848231316, 0.822544634342, 0.823660731316, 0.784598231316, 0.803571403027, 0.827008903027, 0.806919634342, 0.850446403027, 0.845982134342, 0.7890625, 0.776785731316, 0.766741096973, 0.699776768684, 0.813616096973, 0.8125, 0.831473231316, 0.808035731316, 0.805803596973, 0.823660731316, 0.754464268684, 0.829241096973, 0.840401768684, 0.806919634342, 0.822544634342, 0.816964268684, 0.840401768684, 0.849330365658, 0.745535731316, 0.8203125, 0.811383903027, 0.823660731316, 0.764508903027, 0.821428596973, 0.784598231316, 0.829241096973, 0.844866096973, 0.791294634342, 0.783482134342, 0.8046875, 0.857142865658, 0.803571403027, 0.848214268684, 0.866071403027, 0.840401768684, 0.791294634342, 0.767857134342, 0.816964268684, 0.793526768684, 0.818080365658, 0.823660731316, 0.823660731316, 0.830357134342, 0.864955365658, 0.853794634342, 0.833705365658, 0.840401768684, 0.848214268684, 0.832589268684, 0.779017865658, 0.845982134342, 0.833705365658, 0.847098231316, 0.8515625, 0.813616096973, 0.795758903027, 0.830357134342, 0.840401768684, 0.822544634342, 0.844866096973, 0.803571403027, 0.814732134342, 0.842633903027, 0.825892865658, 0.800223231316, 0.797991096973, 0.847098231316, 0.828125, 0.839285731316, 0.790178596973, 0.791294634342, 0.808035731316, 0.821428596973, 0.818080365658, 0.751116096973, 0.782366096973, 0.8359375, 0.819196403027, 0.782366096973, 0.787946403027, 0.839285731316, 0.864955365658, 0.864955365658, 0.864955365658, 0.837053596973, 0.823660731316, 0.786830365658, 0.827008903027, 0.821428596973, 0.861607134342, 0.8515625, 0.864955365658, 0.862723231316, 0.838169634342, 0.828125, 0.8515625, 0.830357134342, 0.815848231316, 0.838169634342, 0.853794634342, 0.859375, 0.849330365658, 0.859375, 0.784598231316, 0.787946403027, 0.859375, 0.848214268684, 0.78125, 0.833705365658, 0.832589268684, 0.811383903027, 0.860491096973, 0.796875, 0.801339268684, 0.821428596973, 0.828125, 0.84375, 0.860491096973, 0.832589268684, 0.834821403027, 0.791294634342, 0.831473231316, 0.830357134342, 0.822544634342, 0.839285731316, 0.848214268684, 0.809151768684, 0.791294634342, 0.828125, 0.841517865658, 0.852678596973, 0.810267865658, 0.748883903027, 0.811383903027, 0.816964268684, 0.782366096973, 0.792410731316, 0.831473231316, 0.805803596973, 0.776785731316, 0.845982134342, 0.793526768684, 0.814732134342, 0.837053596973, 0.802455365658, 0.802455365658, 0.833705365658, 0.803571403027, 0.854910731316, 0.770089268684, 0.806919634342, 0.834821403027, 0.822544634342, 0.847098231316, 0.832589268684, 0.833705365658, 0.8203125, 0.808035731316, 0.821428596973, 0.829241096973, 0.810267865658, 0.818080365658, 0.840401768684, 0.832589268684, 0.830357134342, 0.782366096973, 0.828125, 0.837053596973, 0.859375, 0.842633903027, 0.806919634342, 0.860491096973, 0.849330365658, 0.833705365658, 0.84375, 0.858258903027, 0.8203125, 0.838169634342, 0.859375, 0.852678596973, 0.794642865658, 0.833705365658, 0.794642865658, 0.864955365658]\n",
    "uncorr = [0.0166666675359, 0.0166666675359, 0.0250000003725, 0.0250000003725, 0.0229166671634, 0.0250000003725, 0.0250000003725, 0.0281249992549, 0.0250000003725, 0.0197916664183, 0.0208333339542, 0.0218749996275, 0.0229166671634, 0.0187500007451, 0.0177083332092, 0.03125, 0.015625, 0.0250000003725, 0.015625, 0.0197916664183, 0.0197916664183, 0.0145833333954, 0.0166666675359, 0.015625, 0.0250000003725, 0.0250000003725, 0.0229166671634, 0.0250000003725, 0.0250000003725, 0.0333333350718, 0.03125, 0.046875, 0.0489583350718, 0.046875, 0.0343750007451, 0.0354166664183, 0.0354166664183, 0.0447916649282, 0.0270833335817, 0.0322916656733, 0.0229166671634, 0.0447916649282, 0.0333333350718, 0.0302083324641, 0.0302083324641, 0.0375000014901, 0.0447916649282, 0.046875, 0.03125, 0.0406249985099, 0.0385416671634, 0.0322916656733, 0.0416666679084, 0.0343750007451, 0.0635416656733, 0.0489583350718, 0.0395833328366, 0.0500000007451, 0.0406249985099, 0.0479166656733, 0.0531250014901, 0.0520833320916, 0.0479166656733, 0.0520833320916, 0.0437499992549, 0.0562499985099, 0.0593749992549, 0.0604166649282, 0.0760416686535, 0.0645833313465, 0.0666666701436, 0.0625, 0.0614583343267, 0.0479166656733, 0.0437499992549, 0.0708333328366, 0.0302083324641, 0.0750000029802, 0.0593749992549, 0.0531250014901, 0.0562499985099, 0.0572916679084, 0.0666666701436, 0.0729166641831, 0.0635416656733, 0.0916666686535, 0.0729166641831, 0.0760416686535, 0.0812499970198, 0.0500000007451, 0.0395833328366, 0.0447916649282, 0.0697916671634, 0.0729166641831, 0.0791666656733, 0.078125, 0.101041667163, 0.0874999985099, 0.10625000298, 0.122916668653, 0.0895833298564, 0.0791666656733, 0.0625, 0.11249999702, 0.125, 0.0791666656733, 0.10000000149, 0.0916666686535, 0.117708332837, 0.12187500298, 0.0989583358169, 0.0979166701436, 0.0989583358169, 0.107291668653, 0.126041665673, 0.102083332837, 0.12812499702, 0.122916668653, 0.104166664183, 0.132291659713, 0.127083331347, 0.12187500298, 0.13750000298, 0.116666667163, 0.114583335817, 0.130208328366, 0.151041671634, 0.114583335817, 0.113541670144, 0.110416665673, 0.122916668653, 0.117708332837, 0.13124999404, 0.119791664183, 0.151041671634, 0.135416671634, 0.132291659713, 0.13437500596, 0.135416671634, 0.12187500298, 0.12812499702, 0.147916659713, 0.14687499404, 0.151041671634, 0.147916659713, 0.15000000596, 0.185416668653, 0.154166668653, 0.161458328366, 0.17812499404, 0.135416671634, 0.158333331347, 0.17812499404, 0.119791664183, 0.13437500596, 0.157291665673, 0.172916665673, 0.160416662693, 0.148958340287, 0.139583334327, 0.13750000298, 0.183333337307, 0.167708337307, 0.22499999404, 0.189583331347, 0.19062499702, 0.19687500596, 0.183333337307, 0.22812500596, 0.18125000596, 0.179166659713, 0.201041668653, 0.16562500596, 0.208333328366, 0.202083334327, 0.210416659713, 0.191666662693, 0.227083340287, 0.19374999404, 0.203125, 0.20000000298, 0.233333334327, 0.22187499702, 0.198958337307, 0.22187499702, 0.205208331347, 0.219791665673, 0.17812499404, 0.238541662693, 0.22812500596, 0.25624999404, 0.236458331347, 0.217708334327, 0.229166671634, 0.242708340287, 0.222916662693, 0.223958328366, 0.229166671634, 0.267708331347, 0.279166668653, 0.315625011921, 0.277083337307, 0.279166668653, 0.324999988079, 0.268750011921, 0.267708331347, 0.296875, 0.260416656733, 0.21562500298, 0.255208343267, 0.265625, 0.288541674614, 0.323958337307, 0.33750000596, 0.326041668653, 0.352083325386, 0.351041674614, 0.330208331347, 0.297916680574, 0.33750000596, 0.311458319426, 0.342708319426, 0.282291680574, 0.347916662693, 0.297916680574, 0.332291662693, 0.311458319426, 0.352083325386, 0.363541662693, 0.33750000596, 0.35312500596, 0.351041674614, 0.395833343267, 0.36875000596, 0.348958343267, 0.367708325386, 0.319791674614, 0.380208343267, 0.38437500596, 0.38437500596, 0.393750011921, 0.366666674614, 0.382291674614, 0.364583343267, 0.405208319426, 0.332291662693, 0.41249999404, 0.442708343267, 0.367708325386, 0.389583319426, 0.416666656733, 0.394791662693, 0.409375011921, 0.46250000596, 0.391666680574, 0.39687499404, 0.33437499404, 0.40000000596, 0.451041668653, 0.476041674614, 0.452083319426, 0.449999988079, 0.434374988079, 0.467708319426, 0.413541674614, 0.421875, 0.355208337307, 0.390625, 0.49062499404, 0.440625011921, 0.464583337307, 0.43125000596, 0.489583343267, 0.40000000596, 0.47812500596, 0.435416668653, 0.501041650772, 0.429166674614, 0.486458331347, 0.492708325386, 0.505208313465, 0.484375, 0.479166656733, 0.506250023842, 0.429166674614, 0.483333319426, 0.482291668653, 0.38437500596, 0.520833313465, 0.485416680574, 0.436458319426, 0.512499988079, 0.472916662693, 0.456250011921, 0.477083325386, 0.503125011921, 0.535416662693, 0.512499988079, 0.517708361149, 0.503125011921, 0.481249988079, 0.539583325386, 0.497916668653, 0.49062499404, 0.510416686535, 0.521875023842, 0.547916650772, 0.453125, 0.532291650772, 0.476041674614, 0.519791662693, 0.497916668653, 0.545833349228, 0.583333313465, 0.557291686535, 0.550000011921, 0.536458313465, 0.555208325386, 0.497916668653, 0.559374988079, 0.495833337307, 0.544791638851, 0.506250023842, 0.517708361149, 0.471875011921, 0.532291650772, 0.545833349228, 0.519791662693, 0.565625011921, 0.546875, 0.582291662693, 0.586458325386, 0.571874976158, 0.541666686535, 0.540624976158, 0.563541650772, 0.611458361149, 0.521875023842, 0.592708349228, 0.594791650772, 0.561458349228, 0.617708325386, 0.546875, 0.573958337307, 0.572916686535, 0.578125, 0.609375, 0.560416638851, 0.625, 0.607291638851, 0.535416662693, 0.591666638851, 0.568750023842, 0.583333313465, 0.620833337307, 0.536458313465, 0.587499976158, 0.588541686535, 0.649999976158, 0.627083361149, 0.626041650772, 0.618749976158, 0.606249988079, 0.614583313465, 0.620833337307, 0.627083361149, 0.610416650772, 0.659375011921, 0.638541638851, 0.645833313465, 0.655208349228, 0.651041686535, 0.601041674614, 0.623958349228, 0.590624988079, 0.597916662693, 0.672916650772, 0.648958325386, 0.644791662693, 0.648958325386, 0.652083337307, 0.663541674614, 0.555208325386, 0.640625, 0.640625, 0.625, 0.643750011921, 0.660416662693, 0.622916638851, 0.646875023842, 0.608333349228, 0.677083313465, 0.671875, 0.684374988079, 0.556249976158, 0.690625011921, 0.596875011921, 0.606249988079, 0.625, 0.667708337307, 0.661458313465, 0.644791662693, 0.653124988079, 0.645833313465, 0.670833349228, 0.669791638851, 0.671875, 0.608333349228, 0.657291650772, 0.675000011921, 0.683333337307, 0.676041662693, 0.630208313465, 0.627083361149, 0.685416638851, 0.678125023842, 0.660416662693, 0.701041638851, 0.680208325386, 0.706250011921, 0.663541674614, 0.646875023842, 0.654166638851, 0.688541650772, 0.680208325386, 0.606249988079, 0.604166686535, 0.653124988079, 0.636458337307, 0.598958313465, 0.702083349228, 0.6875, 0.704166650772, 0.698958337307, 0.65625, 0.616666674614, 0.646875023842, 0.689583361149, 0.710416674614, 0.725000023842, 0.683333337307, 0.662500023842, 0.622916638851, 0.726041674614, 0.642708361149, 0.666666686535, 0.612500011921, 0.680208325386, 0.663541674614, 0.734375, 0.711458325386, 0.712499976158, 0.682291686535, 0.714583337307, 0.683333337307, 0.675000011921, 0.653124988079, 0.728124976158, 0.709375023842, 0.719791650772, 0.708333313465, 0.690625011921, 0.719791650772, 0.646875023842, 0.706250011921, 0.688541650772, 0.691666662693, 0.660416662693, 0.719791650772, 0.708333313465, 0.665624976158, 0.698958337307, 0.730208337307, 0.673958361149, 0.671875, 0.692708313465, 0.694791674614, 0.683333337307, 0.716666638851, 0.701041638851, 0.734375, 0.619791686535, 0.738541662693, 0.695833325386, 0.771875023842, 0.732291638851, 0.778124988079, 0.738541662693, 0.676041662693, 0.704166650772, 0.721875011921, 0.695833325386, 0.732291638851, 0.736458361149, 0.661458313465, 0.729166686535, 0.75, 0.739583313465, 0.677083313465, 0.751041650772, 0.763541638851, 0.728124976158, 0.760416686535, 0.740625023842, 0.703125, 0.686458349228, 0.699999988079, 0.792708337307, 0.719791650772, 0.711458325386, 0.739583313465, 0.746874988079, 0.729166686535, 0.715624988079, 0.789583325386, 0.746874988079, 0.722916662693, 0.740625023842, 0.676041662693, 0.736458361149, 0.774999976158, 0.752083361149, 0.770833313465, 0.711458325386, 0.706250011921, 0.719791650772, 0.780208349228, 0.715624988079, 0.694791674614, 0.78125, 0.742708325386, 0.763541638851, 0.779166638851, 0.786458313465, 0.787500023842, 0.727083325386, 0.785416662693, 0.722916662693, 0.694791674614, 0.729166686535, 0.706250011921, 0.732291638851, 0.741666674614, 0.807291686535, 0.787500023842, 0.778124988079, 0.766666650772, 0.748958349228, 0.782291650772, 0.756250023842, 0.697916686535, 0.734375, 0.806249976158, 0.751041650772, 0.757291674614, 0.721875011921, 0.759374976158, 0.772916674614, 0.797916650772, 0.788541674614, 0.791666686535, 0.725000023842, 0.794791638851, 0.78125, 0.766666650772, 0.795833349228, 0.773958325386, 0.821874976158, 0.758333325386, 0.772916674614, 0.701041638851, 0.730208337307, 0.793749988079, 0.815625011921, 0.765625, 0.792708337307, 0.762499988079, 0.773958325386, 0.752083361149, 0.803125023842, 0.744791686535, 0.758333325386, 0.827083349228, 0.739583313465, 0.790624976158, 0.807291686535, 0.787500023842, 0.745833337307, 0.732291638851, 0.769791662693, 0.769791662693, 0.821874976158, 0.815625011921, 0.778124988079, 0.780208349228, 0.778124988079, 0.759374976158, 0.826041638851, 0.759374976158, 0.755208313465, 0.747916638851, 0.785416662693, 0.814583361149, 0.819791674614, 0.824999988079, 0.745833337307, 0.735416650772, 0.744791686535, 0.765625, 0.699999988079, 0.784375011921, 0.745833337307, 0.765625, 0.777083337307, 0.770833313465, 0.795833349228, 0.780208349228, 0.734375, 0.792708337307, 0.751041650772, 0.802083313465, 0.772916674614, 0.816666662693, 0.819791674614, 0.780208349228, 0.804166674614, 0.804166674614, 0.766666650772, 0.726041674614, 0.763541638851, 0.808333337307, 0.830208361149, 0.810416638851, 0.787500023842, 0.828125, 0.808333337307, 0.807291686535, 0.767708361149, 0.797916650772, 0.801041662693, 0.789583325386, 0.769791662693, 0.795833349228, 0.792708337307, 0.801041662693, 0.747916638851, 0.769791662693, 0.813541650772, 0.819791674614, 0.842708349228, 0.779166638851, 0.722916662693, 0.793749988079, 0.809374988079, 0.794791638851, 0.800000011921, 0.822916686535, 0.779166638851, 0.752083361149, 0.804166674614, 0.784375011921, 0.786458313465, 0.774999976158, 0.808333337307, 0.800000011921, 0.809374988079, 0.817708313465, 0.798958361149, 0.835416674614, 0.845833361149, 0.811458349228, 0.756250023842, 0.817708313465, 0.809374988079, 0.773958325386, 0.78125, 0.758333325386, 0.831250011921, 0.859375, 0.791666686535, 0.832291662693, 0.855208337307, 0.800000011921, 0.822916686535, 0.784375011921, 0.793749988079, 0.824999988079, 0.814583361149, 0.815625011921, 0.832291662693, 0.834375023842, 0.817708313465, 0.803125023842, 0.788541674614, 0.764583349228, 0.790624976158, 0.767708361149, 0.831250011921, 0.822916686535, 0.800000011921, 0.789583325386, 0.754166662693, 0.755208313465, 0.805208325386, 0.831250011921, 0.811458349228, 0.845833361149, 0.817708313465, 0.770833313465, 0.787500023842, 0.788541674614, 0.817708313465, 0.820833325386, 0.792708337307, 0.763541638851, 0.804166674614, 0.788541674614, 0.805208325386, 0.786458313465, 0.829166650772, 0.856249988079, 0.822916686535, 0.772916674614, 0.794791638851, 0.819791674614, 0.810416638851, 0.819791674614, 0.774999976158, 0.809374988079, 0.831250011921, 0.815625011921, 0.814583361149, 0.842708349228, 0.786458313465, 0.795833349228, 0.783333361149, 0.844791650772, 0.778124988079, 0.824999988079, 0.778124988079, 0.791666686535, 0.829166650772, 0.8125, 0.774999976158, 0.824999988079, 0.729166686535, 0.773958325386, 0.746874988079, 0.804166674614, 0.858333349228, 0.802083313465, 0.732291638851, 0.824999988079, 0.793749988079, 0.824999988079, 0.892708361149, 0.826041638851, 0.808333337307, 0.809374988079, 0.840624988079, 0.837499976158, 0.869791686535, 0.84375, 0.842708349228, 0.801041662693, 0.809374988079, 0.830208361149, 0.856249988079, 0.814583361149, 0.856249988079, 0.796875, 0.827083349228, 0.840624988079, 0.823958337307, 0.826041638851, 0.84375, 0.798958361149, 0.838541686535, 0.771875023842, 0.841666638851, 0.824999988079, 0.837499976158, 0.818750023842, 0.862500011921, 0.848958313465, 0.836458325386, 0.816666662693, 0.854166686535, 0.853124976158, 0.851041674614, 0.866666674614, 0.806249976158, 0.802083313465, 0.822916686535, 0.829166650772, 0.827083349228, 0.810416638851, 0.869791686535, 0.867708325386, 0.829166650772, 0.815625011921, 0.760416686535, 0.811458349228, 0.787500023842, 0.852083325386, 0.852083325386, 0.872916638851, 0.867708325386, 0.838541686535, 0.830208361149, 0.855208337307, 0.827083349228, 0.796875, 0.848958313465, 0.826041638851, 0.867708325386, 0.838541686535, 0.815625011921, 0.786458313465, 0.789583325386, 0.759374976158, 0.813541650772, 0.795833349228, 0.857291638851, 0.839583337307, 0.8125, 0.855208337307, 0.767708361149, 0.827083349228, 0.828125, 0.829166650772, 0.850000023842, 0.834375023842, 0.816666662693, 0.869791686535, 0.788541674614, 0.840624988079, 0.830208361149, 0.822916686535, 0.788541674614, 0.827083349228, 0.808333337307, 0.867708325386, 0.856249988079, 0.828125, 0.820833325386, 0.823958337307, 0.860416650772, 0.827083349228, 0.851041674614, 0.864583313465, 0.854166686535, 0.8125, 0.820833325386, 0.824999988079, 0.790624976158, 0.852083325386, 0.814583361149, 0.837499976158, 0.870833337307, 0.872916638851, 0.877083361149, 0.850000023842, 0.882291674614, 0.873958349228, 0.852083325386, 0.816666662693, 0.878125011921, 0.868749976158, 0.859375, 0.847916662693, 0.841666638851, 0.821874976158, 0.84375, 0.832291662693, 0.821874976158, 0.842708349228, 0.840624988079, 0.836458325386, 0.863541662693, 0.842708349228, 0.852083325386, 0.837499976158, 0.871874988079, 0.869791686535, 0.867708325386, 0.834375023842, 0.836458325386, 0.830208361149, 0.826041638851, 0.836458325386, 0.805208325386, 0.829166650772, 0.830208361149, 0.842708349228, 0.828125, 0.826041638851, 0.859375, 0.886458337307, 0.850000023842, 0.864583313465, 0.871874988079, 0.836458325386, 0.830208361149, 0.850000023842, 0.834375023842, 0.891666650772, 0.868749976158, 0.870833337307, 0.898958325386, 0.861458361149, 0.858333349228, 0.864583313465, 0.824999988079, 0.848958313465, 0.846875011921, 0.876041650772, 0.860416650772, 0.859375, 0.886458337307, 0.822916686535, 0.829166650772, 0.857291638851, 0.865625023842, 0.832291662693, 0.866666674614, 0.859375, 0.861458361149, 0.862500011921, 0.796875, 0.846875011921, 0.852083325386, 0.839583337307, 0.864583313465, 0.868749976158, 0.848958313465, 0.830208361149, 0.787500023842, 0.839583337307, 0.861458361149, 0.848958313465, 0.875, 0.873958349228, 0.828125, 0.851041674614, 0.856249988079, 0.862500011921, 0.823958337307, 0.852083325386, 0.791666686535, 0.832291662693, 0.824999988079, 0.811458349228, 0.830208361149, 0.829166650772, 0.821874976158, 0.797916650772, 0.855208337307, 0.822916686535, 0.832291662693, 0.871874988079, 0.861458361149, 0.845833361149, 0.844791650772, 0.840624988079, 0.895833313465, 0.806249976158, 0.815625011921, 0.844791650772, 0.823958337307, 0.852083325386, 0.847916662693, 0.851041674614, 0.859375, 0.84375, 0.856249988079, 0.84375, 0.827083349228, 0.887499988079, 0.835416674614, 0.823958337307, 0.838541686535, 0.809374988079, 0.851041674614, 0.857291638851, 0.877083361149, 0.847916662693, 0.838541686535, 0.879166662693, 0.872916638851, 0.861458361149, 0.863541662693, 0.895833313465, 0.850000023842, 0.856249988079, 0.861458361149, 0.855208337307, 0.842708349228, 0.834375023842, 0.787500023842, 0.861458361149]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0f3d408c90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f3d583510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(uncorr)\n",
    "plt.plot(corr)\n",
    "plt.legend(['uncorrupted', '50% label noise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 404.054769993\n",
      "Epoch: 0, loss: 4.22428750992, acc: 0.0188577584922\n",
      "Val: 0.0210129301995\n",
      "Epoch: 1, loss: 4.22030496597, acc: 0.0193965509534\n",
      "Epoch: 2, loss: 4.51411581039, acc: 0.0193965509534\n",
      "Epoch: 3, loss: 4.15990781784, acc: 0.0183189660311\n",
      "Epoch: 4, loss: 4.16516113281, acc: 0.0199353452772\n",
      "Val: 0.0204741377383\n",
      "Epoch: 5, loss: 4.27531576157, acc: 0.0226293094456\n",
      "Epoch: 6, loss: 4.13281440735, acc: 0.0145474141464\n",
      "Epoch: 7, loss: 4.15033435822, acc: 0.0193965509534\n",
      "Epoch: 8, loss: 4.19442939758, acc: 0.0183189660311\n",
      "Val: 0.0215517245233\n",
      "Epoch: 9, loss: 4.09881639481, acc: 0.0183189660311\n",
      "Epoch: 10, loss: 4.0555062294, acc: 0.0183189660311\n",
      "Epoch: 11, loss: 3.92427492142, acc: 0.016702586785\n",
      "Epoch: 12, loss: 3.86186766624, acc: 0.0226293094456\n",
      "Val: 0.0215517245233\n",
      "Epoch: 13, loss: 3.85072779655, acc: 0.0237068962306\n",
      "Epoch: 14, loss: 3.85028243065, acc: 0.0193965509534\n",
      "Epoch: 15, loss: 3.8462908268, acc: 0.0242456905544\n",
      "Epoch: 16, loss: 3.84946656227, acc: 0.0247844830155\n",
      "Val: 0.0274784490466\n",
      "Epoch: 17, loss: 3.85012817383, acc: 0.030172413215\n",
      "Epoch: 18, loss: 3.84430456161, acc: 0.0264008622617\n",
      "Epoch: 19, loss: 3.8388645649, acc: 0.030172413215\n",
      "Epoch: 20, loss: 3.83618998528, acc: 0.0269396547228\n",
      "Val: 0.0226293094456\n",
      "Epoch: 21, loss: 3.83645606041, acc: 0.0274784490466\n",
      "Epoch: 22, loss: 3.83966803551, acc: 0.0328663811088\n",
      "Epoch: 23, loss: 3.83566141129, acc: 0.03125\n",
      "Epoch: 24, loss: 3.83255815506, acc: 0.0355603434145\n",
      "Val: 0.0247844830155\n",
      "Epoch: 25, loss: 3.82965803146, acc: 0.036099139601\n",
      "Epoch: 26, loss: 3.83088922501, acc: 0.0344827584922\n",
      "Epoch: 27, loss: 3.83163809776, acc: 0.0296336207539\n",
      "Epoch: 28, loss: 3.83125400543, acc: 0.0296336207539\n",
      "Val: 0.0247844830155\n",
      "Epoch: 29, loss: 3.82301044464, acc: 0.0344827584922\n",
      "Epoch: 30, loss: 3.82163381577, acc: 0.03125\n",
      "Epoch: 31, loss: 3.82251191139, acc: 0.0350215509534\n",
      "Epoch: 32, loss: 3.82498693466, acc: 0.036099139601\n",
      "Val: 0.0269396547228\n",
      "Epoch: 33, loss: 3.8198287487, acc: 0.0344827584922\n",
      "Epoch: 34, loss: 3.82170295715, acc: 0.0307112075388\n",
      "Epoch: 35, loss: 3.81915688515, acc: 0.0355603434145\n",
      "Epoch: 36, loss: 3.81497550011, acc: 0.0366379320621\n",
      "Val: 0.0183189660311\n",
      "Epoch: 37, loss: 3.810359478, acc: 0.0344827584922\n",
      "Epoch: 38, loss: 3.81817293167, acc: 0.0355603434145\n",
      "Epoch: 39, loss: 3.81750702858, acc: 0.030172413215\n",
      "Epoch: 40, loss: 3.80859875679, acc: 0.0328663811088\n",
      "Val: 0.0177801717073\n",
      "Epoch: 41, loss: 3.81108093262, acc: 0.0323275849223\n",
      "Epoch: 42, loss: 3.81060028076, acc: 0.03125\n",
      "Epoch: 43, loss: 3.80674552917, acc: 0.0344827584922\n",
      "Epoch: 44, loss: 3.81927800179, acc: 0.0339439660311\n",
      "Val: 0.0237068962306\n",
      "Epoch: 45, loss: 3.80796909332, acc: 0.0334051735699\n",
      "Epoch: 46, loss: 3.81221079826, acc: 0.0334051735699\n",
      "Epoch: 47, loss: 3.80530691147, acc: 0.0371767245233\n",
      "Epoch: 48, loss: 3.79830169678, acc: 0.0377155169845\n",
      "Val: 0.0226293094456\n",
      "Epoch: 49, loss: 3.80511164665, acc: 0.0350215509534\n",
      "Epoch: 50, loss: 3.80625939369, acc: 0.0323275849223\n",
      "Epoch: 51, loss: 3.80574345589, acc: 0.042025860399\n",
      "Epoch: 52, loss: 3.79765295982, acc: 0.0393318980932\n",
      "Val: 0.0204741377383\n",
      "Epoch: 53, loss: 3.79627752304, acc: 0.036099139601\n",
      "Epoch: 54, loss: 3.80360746384, acc: 0.0366379320621\n",
      "Epoch: 55, loss: 3.79074621201, acc: 0.0382543094456\n",
      "Epoch: 56, loss: 3.79114460945, acc: 0.0371767245233\n",
      "Val: 0.0204741377383\n",
      "Epoch: 57, loss: 3.78474164009, acc: 0.0382543094456\n",
      "Epoch: 58, loss: 3.79290962219, acc: 0.0393318980932\n",
      "Epoch: 59, loss: 3.79482793808, acc: 0.0371767245233\n",
      "Epoch: 60, loss: 3.78723740578, acc: 0.0387931019068\n",
      "Val: 0.0264008622617\n",
      "Epoch: 61, loss: 3.79141807556, acc: 0.0447198264301\n",
      "Epoch: 62, loss: 3.78799462318, acc: 0.0436422415078\n",
      "Epoch: 63, loss: 3.78814339638, acc: 0.0431034490466\n",
      "Epoch: 64, loss: 3.77486348152, acc: 0.0431034490466\n",
      "Val: 0.0220905169845\n",
      "Epoch: 65, loss: 3.7776055336, acc: 0.0436422415078\n",
      "Epoch: 66, loss: 3.78056931496, acc: 0.0387931019068\n",
      "Epoch: 67, loss: 3.78817152977, acc: 0.0398706905544\n",
      "Epoch: 68, loss: 3.78336906433, acc: 0.0382543094456\n",
      "Val: 0.0226293094456\n",
      "Epoch: 69, loss: 3.78039956093, acc: 0.0404094830155\n",
      "Epoch: 70, loss: 3.78126311302, acc: 0.0409482754767\n",
      "Epoch: 71, loss: 3.77947711945, acc: 0.0398706905544\n",
      "Epoch: 72, loss: 3.77797698975, acc: 0.0409482754767\n",
      "Val: 0.0237068962306\n",
      "Epoch: 73, loss: 3.77078795433, acc: 0.0398706905544\n",
      "Epoch: 74, loss: 3.75861215591, acc: 0.046875\n",
      "Epoch: 75, loss: 3.76752686501, acc: 0.0344827584922\n",
      "Epoch: 76, loss: 3.77161073685, acc: 0.0393318980932\n",
      "Val: 0.0296336207539\n",
      "Epoch: 77, loss: 3.77800321579, acc: 0.0414870679379\n",
      "Epoch: 78, loss: 3.76342463493, acc: 0.042025860399\n",
      "Epoch: 79, loss: 3.7705373764, acc: 0.0334051735699\n",
      "Epoch: 80, loss: 3.77445244789, acc: 0.0441810339689\n",
      "Val: 0.0280172415078\n",
      "Epoch: 81, loss: 3.76493024826, acc: 0.0447198264301\n",
      "Epoch: 82, loss: 3.75567698479, acc: 0.046875\n",
      "Epoch: 83, loss: 3.74512076378, acc: 0.0441810339689\n",
      "Epoch: 84, loss: 3.74724054337, acc: 0.042025860399\n",
      "Val: 0.0231681037694\n",
      "Epoch: 85, loss: 3.74086356163, acc: 0.0484913811088\n",
      "Epoch: 86, loss: 3.75288820267, acc: 0.0452586188912\n",
      "Epoch: 87, loss: 3.74190068245, acc: 0.0490301735699\n",
      "Epoch: 88, loss: 3.73590517044, acc: 0.0490301735699\n",
      "Val: 0.0242456905544\n",
      "Epoch: 89, loss: 3.74453139305, acc: 0.0425646565855\n",
      "Epoch: 90, loss: 3.7387740612, acc: 0.0474137924612\n",
      "Epoch: 91, loss: 3.7375793457, acc: 0.0463362075388\n",
      "Epoch: 92, loss: 3.74822425842, acc: 0.0560344830155\n",
      "Val: 0.0129310349002\n",
      "Epoch: 93, loss: 3.72618865967, acc: 0.0501077584922\n",
      "Epoch: 94, loss: 3.71738195419, acc: 0.0474137924612\n",
      "Epoch: 95, loss: 3.71987247467, acc: 0.0490301735699\n",
      "Epoch: 96, loss: 3.72284007072, acc: 0.0490301735699\n",
      "Val: 0.0204741377383\n",
      "Epoch: 97, loss: 3.72473192215, acc: 0.0463362075388\n",
      "Epoch: 98, loss: 3.71244740486, acc: 0.0474137924612\n",
      "Epoch: 99, loss: 3.70562386513, acc: 0.0490301735699\n",
      "Epoch: 100, loss: 3.71056175232, acc: 0.0544181019068\n",
      "Val: 0.0226293094456\n",
      "Epoch: 101, loss: 3.70485091209, acc: 0.0571120679379\n",
      "Epoch: 102, loss: 3.72569465637, acc: 0.0495689660311\n",
      "Epoch: 103, loss: 3.72303771973, acc: 0.0490301735699\n",
      "Epoch: 104, loss: 3.70244884491, acc: 0.0571120679379\n",
      "Val: 0.0253232754767\n",
      "Epoch: 105, loss: 3.70391082764, acc: 0.0581896565855\n",
      "Epoch: 106, loss: 3.69139671326, acc: 0.0608836188912\n",
      "Epoch: 107, loss: 3.68110108376, acc: 0.051724139601\n",
      "Epoch: 108, loss: 3.67497944832, acc: 0.0560344830155\n",
      "Val: 0.0258620698005\n",
      "Epoch: 109, loss: 3.67315816879, acc: 0.0598060339689\n",
      "Epoch: 110, loss: 3.67193603516, acc: 0.057650860399\n",
      "Epoch: 111, loss: 3.69115638733, acc: 0.0544181019068\n",
      "Epoch: 112, loss: 3.6528646946, acc: 0.0662715509534\n",
      "Val: 0.0210129301995\n",
      "Epoch: 113, loss: 3.66786122322, acc: 0.0603448264301\n",
      "Epoch: 114, loss: 3.65873742104, acc: 0.0641163811088\n",
      "Epoch: 115, loss: 3.65176892281, acc: 0.0603448264301\n",
      "Epoch: 116, loss: 3.65233063698, acc: 0.0587284490466\n",
      "Val: 0.0183189660311\n",
      "Epoch: 117, loss: 3.63939929008, acc: 0.0689655169845\n",
      "Epoch: 118, loss: 3.64568138123, acc: 0.0608836188912\n",
      "Epoch: 119, loss: 3.6298801899, acc: 0.0657327622175\n",
      "Epoch: 120, loss: 3.62789916992, acc: 0.0662715509534\n",
      "Val: 0.0193965509534\n",
      "Epoch: 121, loss: 3.64416790009, acc: 0.0625\n",
      "Epoch: 122, loss: 3.62621474266, acc: 0.0705818980932\n",
      "Epoch: 123, loss: 3.622143507, acc: 0.0689655169845\n",
      "Epoch: 124, loss: 3.61461019516, acc: 0.057650860399\n",
      "Val: 0.0188577584922\n",
      "Epoch: 125, loss: 3.61603879929, acc: 0.0651939660311\n",
      "Epoch: 126, loss: 3.60158395767, acc: 0.0668103471398\n",
      "Epoch: 127, loss: 3.62234926224, acc: 0.0695043131709\n",
      "Epoch: 128, loss: 3.59381103516, acc: 0.0786637961864\n",
      "Val: 0.0231681037694\n",
      "Epoch: 129, loss: 3.58055877686, acc: 0.0727370679379\n",
      "Epoch: 130, loss: 3.59063768387, acc: 0.0705818980932\n",
      "Epoch: 131, loss: 3.57777667046, acc: 0.078125\n",
      "Epoch: 132, loss: 3.5833671093, acc: 0.0716594830155\n",
      "Val: 0.0247844830155\n",
      "Epoch: 133, loss: 3.57850170135, acc: 0.0738146528602\n",
      "Epoch: 134, loss: 3.54747819901, acc: 0.0862068980932\n",
      "Epoch: 135, loss: 3.55866265297, acc: 0.0867456868291\n",
      "Epoch: 136, loss: 3.53150391579, acc: 0.0905172377825\n",
      "Val: 0.0226293094456\n",
      "Epoch: 137, loss: 3.52830433846, acc: 0.09375\n",
      "Epoch: 138, loss: 3.54806733131, acc: 0.0856681019068\n",
      "Epoch: 139, loss: 3.53343772888, acc: 0.0899784490466\n",
      "Epoch: 140, loss: 3.54140615463, acc: 0.0813577622175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.0285560339689\n",
      "Epoch: 141, loss: 3.53777742386, acc: 0.0894396528602\n",
      "Epoch: 142, loss: 3.50850582123, acc: 0.0905172377825\n",
      "Epoch: 143, loss: 3.51272416115, acc: 0.100215516984\n",
      "Epoch: 144, loss: 3.49921751022, acc: 0.0872844830155\n",
      "Val: 0.0231681037694\n",
      "Epoch: 145, loss: 3.49783229828, acc: 0.0905172377825\n",
      "Epoch: 146, loss: 3.47144770622, acc: 0.106142237782\n",
      "Epoch: 147, loss: 3.44987273216, acc: 0.109375\n",
      "Epoch: 148, loss: 3.47796726227, acc: 0.106681033969\n",
      "Val: 0.0204741377383\n",
      "Epoch: 149, loss: 3.45994091034, acc: 0.110452584922\n",
      "Epoch: 150, loss: 3.4471077919, acc: 0.102909483016\n",
      "Epoch: 151, loss: 3.42360520363, acc: 0.103448279202\n",
      "Epoch: 152, loss: 3.45651221275, acc: 0.103987067938\n",
      "Val: 0.0253232754767\n",
      "Epoch: 153, loss: 3.44260883331, acc: 0.108836203814\n",
      "Epoch: 154, loss: 3.43124437332, acc: 0.107219830155\n",
      "Epoch: 155, loss: 3.39706611633, acc: 0.118534483016\n",
      "Epoch: 156, loss: 3.40936541557, acc: 0.115840516984\n",
      "Val: 0.0210129301995\n",
      "Epoch: 157, loss: 3.37646794319, acc: 0.120150864124\n",
      "Epoch: 158, loss: 3.40570545197, acc: 0.120150864124\n",
      "Epoch: 159, loss: 3.40266633034, acc: 0.125\n",
      "Epoch: 160, loss: 3.33829975128, acc: 0.129310339689\n",
      "Val: 0.0215517245233\n",
      "Epoch: 161, loss: 3.38494682312, acc: 0.109913796186\n",
      "Epoch: 162, loss: 3.39557957649, acc: 0.123383618891\n",
      "Epoch: 163, loss: 3.38499283791, acc: 0.115840516984\n",
      "Epoch: 164, loss: 3.35253739357, acc: 0.135237067938\n",
      "Val: 0.0183189660311\n",
      "Epoch: 165, loss: 3.32934308052, acc: 0.129849135876\n",
      "Epoch: 166, loss: 3.32152962685, acc: 0.134698271751\n",
      "Epoch: 167, loss: 3.31823682785, acc: 0.128232762218\n",
      "Epoch: 168, loss: 3.30361127853, acc: 0.135237067938\n",
      "Val: 0.0204741377383\n",
      "Epoch: 169, loss: 3.26336312294, acc: 0.14762930572\n",
      "Epoch: 170, loss: 3.31726264954, acc: 0.139547407627\n",
      "Epoch: 171, loss: 3.2518453598, acc: 0.148168101907\n",
      "Epoch: 172, loss: 3.23016214371, acc: 0.153017237782\n",
      "Val: 0.0140086207539\n",
      "Epoch: 173, loss: 3.26884126663, acc: 0.143857762218\n",
      "Epoch: 174, loss: 3.23354482651, acc: 0.159482762218\n",
      "Epoch: 175, loss: 3.23042321205, acc: 0.161099135876\n",
      "Epoch: 176, loss: 3.27323865891, acc: 0.143318966031\n",
      "Val: 0.0177801717073\n",
      "Epoch: 177, loss: 3.27089452744, acc: 0.152478441596\n",
      "Epoch: 178, loss: 3.18203806877, acc: 0.161637932062\n",
      "Epoch: 179, loss: 3.19446110725, acc: 0.165948271751\n",
      "Epoch: 180, loss: 3.11760926247, acc: 0.184267237782\n",
      "Val: 0.0188577584922\n",
      "Epoch: 181, loss: 3.21490740776, acc: 0.166487067938\n",
      "Epoch: 182, loss: 3.13905882835, acc: 0.173491373658\n",
      "Epoch: 183, loss: 3.1763484478, acc: 0.168103441596\n",
      "Epoch: 184, loss: 3.1545085907, acc: 0.193965524435\n",
      "Val: 0.0177801717073\n",
      "Epoch: 185, loss: 3.08727741241, acc: 0.18049569428\n",
      "Epoch: 186, loss: 3.18495321274, acc: 0.167564660311\n",
      "Epoch: 187, loss: 3.14102125168, acc: 0.185883626342\n",
      "Epoch: 188, loss: 3.13782906532, acc: 0.18049569428\n",
      "Val: 0.016702586785\n",
      "Epoch: 189, loss: 3.12281250954, acc: 0.188038796186\n",
      "Epoch: 190, loss: 3.06628513336, acc: 0.205280169845\n",
      "Epoch: 191, loss: 3.072463274, acc: 0.197737067938\n",
      "Epoch: 192, loss: 3.0627989769, acc: 0.203663796186\n",
      "Val: 0.0193965509534\n",
      "Epoch: 193, loss: 3.03108429909, acc: 0.215517237782\n",
      "Epoch: 194, loss: 3.01781225204, acc: 0.216594830155\n",
      "Epoch: 195, loss: 2.97522807121, acc: 0.215517237782\n",
      "Epoch: 196, loss: 2.95075488091, acc: 0.230064660311\n",
      "Val: 0.0199353452772\n",
      "Epoch: 197, loss: 2.9927675724, acc: 0.219827592373\n",
      "Epoch: 198, loss: 3.02169704437, acc: 0.215517237782\n",
      "Epoch: 199, loss: 2.95737791061, acc: 0.230064660311\n",
      "Epoch: 200, loss: 2.95374131203, acc: 0.221982762218\n",
      "Val: 0.0199353452772\n",
      "Epoch: 201, loss: 2.86802577972, acc: 0.24137930572\n",
      "Epoch: 202, loss: 2.97770500183, acc: 0.216594830155\n",
      "Epoch: 203, loss: 2.90199351311, acc: 0.24137930572\n",
      "Epoch: 204, loss: 2.8884510994, acc: 0.25\n",
      "Val: 0.0307112075388\n",
      "Epoch: 205, loss: 2.87483215332, acc: 0.240301728249\n",
      "Epoch: 206, loss: 2.83897447586, acc: 0.260775864124\n",
      "Epoch: 207, loss: 2.86244416237, acc: 0.239762932062\n",
      "Epoch: 208, loss: 2.8829100132, acc: 0.251077592373\n",
      "Val: 0.0172413792461\n",
      "Epoch: 209, loss: 2.8145699501, acc: 0.268857747316\n",
      "Epoch: 210, loss: 2.87690925598, acc: 0.259159475565\n",
      "Epoch: 211, loss: 2.85717439651, acc: 0.259698271751\n",
      "Epoch: 212, loss: 2.83601808548, acc: 0.261314660311\n",
      "Val: 0.0215517245233\n",
      "Epoch: 213, loss: 2.76185202599, acc: 0.28125\n",
      "Epoch: 214, loss: 2.78810691833, acc: 0.286099135876\n",
      "Epoch: 215, loss: 2.74631929398, acc: 0.275862067938\n",
      "Epoch: 216, loss: 2.73443055153, acc: 0.283405184746\n",
      "Val: 0.0204741377383\n",
      "Epoch: 217, loss: 2.73478388786, acc: 0.280711203814\n",
      "Epoch: 218, loss: 2.76756334305, acc: 0.272629320621\n",
      "Epoch: 219, loss: 2.69120836258, acc: 0.293642252684\n",
      "Epoch: 220, loss: 2.67675232887, acc: 0.311422407627\n",
      "Val: 0.0172413792461\n",
      "Epoch: 221, loss: 2.70697069168, acc: 0.292025864124\n",
      "Epoch: 222, loss: 2.69423270226, acc: 0.307112067938\n",
      "Epoch: 223, loss: 2.66882228851, acc: 0.303879320621\n",
      "Epoch: 224, loss: 2.7390768528, acc: 0.288254320621\n",
      "Val: 0.0220905169845\n",
      "Epoch: 225, loss: 2.63406729698, acc: 0.301724135876\n",
      "Epoch: 226, loss: 2.53367567062, acc: 0.329202592373\n",
      "Epoch: 227, loss: 2.58486676216, acc: 0.319504320621\n",
      "Epoch: 228, loss: 2.58307552338, acc: 0.320581883192\n",
      "Val: 0.0317887924612\n",
      "Epoch: 229, loss: 2.59787583351, acc: 0.316810339689\n",
      "Epoch: 230, loss: 2.51192760468, acc: 0.34644395113\n",
      "Epoch: 231, loss: 2.54947495461, acc: 0.329202592373\n",
      "Epoch: 232, loss: 2.54193735123, acc: 0.339978456497\n",
      "Val: 0.016702586785\n",
      "Epoch: 233, loss: 2.4321680069, acc: 0.359375\n",
      "Epoch: 234, loss: 2.47866988182, acc: 0.335129320621\n",
      "Epoch: 235, loss: 2.58567237854, acc: 0.327047407627\n",
      "Epoch: 236, loss: 2.43167662621, acc: 0.363146543503\n",
      "Val: 0.0199353452772\n",
      "Epoch: 237, loss: 2.44604301453, acc: 0.349137932062\n",
      "Epoch: 238, loss: 2.39970183372, acc: 0.357758611441\n",
      "Epoch: 239, loss: 2.36124372482, acc: 0.374461203814\n",
      "Epoch: 240, loss: 2.47647809982, acc: 0.349137932062\n",
      "Val: 0.0210129301995\n",
      "Epoch: 241, loss: 2.47902703285, acc: 0.363146543503\n",
      "Epoch: 242, loss: 2.38684558868, acc: 0.377155184746\n",
      "Epoch: 243, loss: 2.39951181412, acc: 0.359375\n",
      "Epoch: 244, loss: 2.336114645, acc: 0.376077592373\n",
      "Val: 0.0307112075388\n",
      "Epoch: 245, loss: 2.29580926895, acc: 0.383620679379\n",
      "Epoch: 246, loss: 2.35221028328, acc: 0.384159475565\n",
      "Epoch: 247, loss: 2.28349852562, acc: 0.397090524435\n",
      "Epoch: 248, loss: 2.31799530983, acc: 0.393857747316\n",
      "Val: 0.0193965509534\n",
      "Epoch: 249, loss: 2.26697945595, acc: 0.396551728249\n",
      "Epoch: 250, loss: 2.22031474113, acc: 0.414331883192\n",
      "Epoch: 251, loss: 2.31816935539, acc: 0.390086203814\n",
      "Epoch: 252, loss: 2.21319127083, acc: 0.425646543503\n",
      "Val: 0.0253232754767\n",
      "Epoch: 253, loss: 2.19687795639, acc: 0.413793116808\n",
      "Epoch: 254, loss: 2.15457749367, acc: 0.422952592373\n",
      "Epoch: 255, loss: 2.24434089661, acc: 0.403017252684\n",
      "Epoch: 256, loss: 2.1805536747, acc: 0.436422407627\n",
      "Val: 0.0253232754767\n",
      "Epoch: 257, loss: 2.11009979248, acc: 0.450969815254\n",
      "Epoch: 258, loss: 2.11930036545, acc: 0.431034475565\n",
      "Epoch: 259, loss: 2.07311677933, acc: 0.449892252684\n",
      "Epoch: 260, loss: 2.13328742981, acc: 0.436422407627\n",
      "Val: 0.0242456905544\n",
      "Epoch: 261, loss: 2.11319971085, acc: 0.438038796186\n",
      "Epoch: 262, loss: 2.08348989487, acc: 0.444504320621\n",
      "Epoch: 263, loss: 2.0715546608, acc: 0.454741388559\n",
      "Epoch: 264, loss: 2.02776646614, acc: 0.452047407627\n",
      "Val: 0.0226293094456\n",
      "Epoch: 265, loss: 2.04079222679, acc: 0.464978456497\n",
      "Epoch: 266, loss: 1.96090900898, acc: 0.474137932062\n",
      "Epoch: 267, loss: 1.93625295162, acc: 0.483297407627\n",
      "Epoch: 268, loss: 2.01230382919, acc: 0.462284475565\n",
      "Val: 0.0264008622617\n",
      "Epoch: 269, loss: 1.88197159767, acc: 0.505926728249\n",
      "Epoch: 270, loss: 1.93039619923, acc: 0.474676728249\n",
      "Epoch: 271, loss: 1.90423429012, acc: 0.496767252684\n",
      "Epoch: 272, loss: 1.9572044611, acc: 0.484375\n",
      "Val: 0.0253232754767\n",
      "Epoch: 273, loss: 1.94222426414, acc: 0.484913796186\n",
      "Epoch: 274, loss: 1.95766425133, acc: 0.476831883192\n",
      "Epoch: 275, loss: 1.9370418787, acc: 0.490301728249\n",
      "Epoch: 276, loss: 1.80939900875, acc: 0.514008641243\n",
      "Val: 0.0247844830155\n",
      "Epoch: 277, loss: 1.8053085804, acc: 0.51023709774\n",
      "Epoch: 278, loss: 1.88519001007, acc: 0.494612067938\n",
      "Epoch: 279, loss: 1.78403353691, acc: 0.526400864124\n",
      "Epoch: 280, loss: 1.74742937088, acc: 0.52586209774\n",
      "Val: 0.0296336207539\n",
      "Epoch: 281, loss: 1.81624531746, acc: 0.514008641243\n",
      "Epoch: 282, loss: 1.77469587326, acc: 0.513469815254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283, loss: 1.75084471703, acc: 0.526939630508\n",
      "Epoch: 284, loss: 1.77694618702, acc: 0.52855604887\n",
      "Val: 0.0323275849223\n",
      "Epoch: 285, loss: 1.70115947723, acc: 0.542025864124\n",
      "Epoch: 286, loss: 1.6149532795, acc: 0.560344815254\n",
      "Epoch: 287, loss: 1.73158526421, acc: 0.535560369492\n",
      "Epoch: 288, loss: 1.63914322853, acc: 0.556034505367\n",
      "Val: 0.0269396547228\n",
      "Epoch: 289, loss: 1.60352957249, acc: 0.56519395113\n",
      "Epoch: 290, loss: 1.63767027855, acc: 0.565732777119\n",
      "Epoch: 291, loss: 1.66333866119, acc: 0.551724135876\n",
      "Epoch: 292, loss: 1.60304617882, acc: 0.577047407627\n",
      "Val: 0.0264008622617\n",
      "Epoch: 293, loss: 1.49819850922, acc: 0.600754320621\n",
      "Epoch: 294, loss: 1.61941742897, acc: 0.572198271751\n",
      "Epoch: 295, loss: 1.55114901066, acc: 0.573814630508\n",
      "Epoch: 296, loss: 1.5690600872, acc: 0.578125\n",
      "Val: 0.0274784490466\n",
      "Epoch: 297, loss: 1.45770823956, acc: 0.595905184746\n",
      "Epoch: 298, loss: 1.59028744698, acc: 0.564116358757\n",
      "Epoch: 299, loss: 1.43663859367, acc: 0.614224135876\n",
      "Epoch: 300, loss: 1.41123819351, acc: 0.600754320621\n",
      "Val: 0.0204741377383\n",
      "Epoch: 301, loss: 1.43966889381, acc: 0.61206895113\n",
      "Epoch: 302, loss: 1.44469547272, acc: 0.599676728249\n",
      "Epoch: 303, loss: 1.4271299839, acc: 0.608297407627\n",
      "Epoch: 304, loss: 1.39337313175, acc: 0.619073271751\n",
      "Val: 0.0258620698005\n",
      "Epoch: 305, loss: 1.34444117546, acc: 0.626077592373\n",
      "Epoch: 306, loss: 1.34992134571, acc: 0.627155184746\n",
      "Epoch: 307, loss: 1.44732034206, acc: 0.602370679379\n",
      "Epoch: 308, loss: 1.39120781422, acc: 0.605603456497\n",
      "Val: 0.0264008622617\n",
      "Epoch: 309, loss: 1.33084869385, acc: 0.625\n",
      "Epoch: 310, loss: 1.35288667679, acc: 0.627155184746\n",
      "Epoch: 311, loss: 1.2910040617, acc: 0.644935369492\n",
      "Epoch: 312, loss: 1.32013618946, acc: 0.628232777119\n",
      "Val: 0.0290948282927\n",
      "Epoch: 313, loss: 1.35809993744, acc: 0.62769395113\n",
      "Epoch: 314, loss: 1.31081676483, acc: 0.640625\n",
      "Epoch: 315, loss: 1.28103363514, acc: 0.647090494633\n",
      "Epoch: 316, loss: 1.26282072067, acc: 0.655172407627\n",
      "Val: 0.0231681037694\n",
      "Epoch: 317, loss: 1.24837017059, acc: 0.653017222881\n",
      "Epoch: 318, loss: 1.22775757313, acc: 0.664870679379\n",
      "Epoch: 319, loss: 1.26401865482, acc: 0.642780184746\n",
      "Epoch: 320, loss: 1.30958402157, acc: 0.640625\n",
      "Val: 0.0177801717073\n",
      "Epoch: 321, loss: 1.25627923012, acc: 0.65894395113\n",
      "Epoch: 322, loss: 1.24100041389, acc: 0.646551728249\n",
      "Epoch: 323, loss: 1.18336582184, acc: 0.670797407627\n",
      "Epoch: 324, loss: 1.09080660343, acc: 0.702586233616\n",
      "Val: 0.0199353452772\n",
      "Epoch: 325, loss: 1.10532975197, acc: 0.673491358757\n",
      "Epoch: 326, loss: 1.20534086227, acc: 0.665409505367\n",
      "Epoch: 327, loss: 1.14685893059, acc: 0.665409505367\n",
      "Epoch: 328, loss: 1.20528936386, acc: 0.671336233616\n",
      "Val: 0.030172413215\n",
      "Epoch: 329, loss: 1.1031897068, acc: 0.689116358757\n",
      "Epoch: 330, loss: 1.05495417118, acc: 0.6875\n",
      "Epoch: 331, loss: 1.08771455288, acc: 0.700969815254\n",
      "Epoch: 332, loss: 1.2218773365, acc: 0.664331912994\n",
      "Val: 0.030172413215\n",
      "Epoch: 333, loss: 1.046402812, acc: 0.711206912994\n",
      "Epoch: 334, loss: 1.05809497833, acc: 0.702047407627\n",
      "Epoch: 335, loss: 1.04646861553, acc: 0.710129320621\n",
      "Epoch: 336, loss: 1.04460692406, acc: 0.706896543503\n",
      "Val: 0.0188577584922\n",
      "Epoch: 337, loss: 1.08900654316, acc: 0.696659505367\n",
      "Epoch: 338, loss: 1.07519316673, acc: 0.688038766384\n",
      "Epoch: 339, loss: 1.0778349638, acc: 0.69288790226\n",
      "Epoch: 340, loss: 1.08704197407, acc: 0.695043087006\n",
      "Val: 0.0220905169845\n",
      "Epoch: 341, loss: 0.965288639069, acc: 0.726293087006\n",
      "Epoch: 342, loss: 0.990117609501, acc: 0.71875\n",
      "Epoch: 343, loss: 0.97762387991, acc: 0.720905184746\n",
      "Epoch: 344, loss: 0.896876752377, acc: 0.730603456497\n",
      "Val: 0.0231681037694\n",
      "Epoch: 345, loss: 0.904682993889, acc: 0.726831912994\n",
      "Epoch: 346, loss: 0.957584381104, acc: 0.72413790226\n",
      "Epoch: 347, loss: 0.885882735252, acc: 0.746228456497\n",
      "Epoch: 348, loss: 0.971700191498, acc: 0.725754320621\n",
      "Val: 0.0199353452772\n",
      "Epoch: 349, loss: 0.972422122955, acc: 0.72144395113\n",
      "Epoch: 350, loss: 1.00535225868, acc: 0.719288766384\n",
      "Epoch: 351, loss: 0.947741091251, acc: 0.727370679379\n",
      "Epoch: 352, loss: 0.877526462078, acc: 0.754849135876\n",
      "Val: 0.0199353452772\n",
      "Epoch: 353, loss: 0.901524424553, acc: 0.742456912994\n",
      "Epoch: 354, loss: 0.813060581684, acc: 0.765625\n",
      "Epoch: 355, loss: 0.910115361214, acc: 0.730603456497\n",
      "Epoch: 356, loss: 0.764009296894, acc: 0.783405184746\n",
      "Val: 0.0199353452772\n",
      "Epoch: 357, loss: 0.896193802357, acc: 0.748383641243\n",
      "Epoch: 358, loss: 0.859422087669, acc: 0.756465494633\n",
      "Epoch: 359, loss: 0.80556589365, acc: 0.766163766384\n",
      "Epoch: 360, loss: 0.771682262421, acc: 0.784482777119\n",
      "Val: 0.0247844830155\n",
      "Epoch: 361, loss: 0.86494833231, acc: 0.752155184746\n",
      "Epoch: 362, loss: 0.797206878662, acc: 0.779633641243\n",
      "Epoch: 363, loss: 0.746696472168, acc: 0.785021543503\n",
      "Epoch: 364, loss: 0.774942576885, acc: 0.788793087006\n",
      "Val: 0.0269396547228\n",
      "Epoch: 365, loss: 0.784756302834, acc: 0.773706912994\n",
      "Epoch: 366, loss: 0.821097493172, acc: 0.773168087006\n",
      "Epoch: 367, loss: 0.750518321991, acc: 0.780711233616\n",
      "Epoch: 368, loss: 0.833705067635, acc: 0.764547407627\n",
      "Val: 0.0172413792461\n",
      "Epoch: 369, loss: 0.850380778313, acc: 0.765625\n",
      "Epoch: 370, loss: 0.751735270023, acc: 0.790409505367\n",
      "Epoch: 371, loss: 0.716890752316, acc: 0.795258641243\n",
      "Epoch: 372, loss: 0.672825992107, acc: 0.802801728249\n",
      "Val: 0.0193965509534\n",
      "Epoch: 373, loss: 0.729177832603, acc: 0.798491358757\n",
      "Epoch: 374, loss: 0.645613729954, acc: 0.81788790226\n",
      "Epoch: 375, loss: 0.692741572857, acc: 0.809267222881\n",
      "Epoch: 376, loss: 0.750147938728, acc: 0.782866358757\n",
      "Val: 0.0226293094456\n",
      "Epoch: 377, loss: 0.687945306301, acc: 0.80980604887\n",
      "Epoch: 378, loss: 0.689647495747, acc: 0.801724135876\n",
      "Epoch: 379, loss: 0.652658581734, acc: 0.808728456497\n",
      "Epoch: 380, loss: 0.643201649189, acc: 0.807650864124\n",
      "Val: 0.0183189660311\n",
      "Epoch: 381, loss: 0.640445291996, acc: 0.808728456497\n",
      "Epoch: 382, loss: 0.69672703743, acc: 0.804956912994\n",
      "Epoch: 383, loss: 0.633814692497, acc: 0.818426728249\n",
      "Epoch: 384, loss: 0.704759597778, acc: 0.790948271751\n",
      "Val: 0.0264008622617\n",
      "Epoch: 385, loss: 0.641879200935, acc: 0.82273709774\n",
      "Epoch: 386, loss: 0.711552083492, acc: 0.795258641243\n",
      "Epoch: 387, loss: 0.585899353027, acc: 0.823814630508\n",
      "Epoch: 388, loss: 0.64003932476, acc: 0.814116358757\n",
      "Val: 0.0177801717073\n",
      "Epoch: 389, loss: 0.62886339426, acc: 0.817349135876\n",
      "Epoch: 390, loss: 0.652057766914, acc: 0.814116358757\n",
      "Epoch: 391, loss: 0.574439466, acc: 0.826508641243\n",
      "Epoch: 392, loss: 0.661216795444, acc: 0.808728456497\n",
      "Val: 0.0247844830155\n",
      "Epoch: 393, loss: 0.58737385273, acc: 0.826508641243\n",
      "Epoch: 394, loss: 0.544045090675, acc: 0.83081895113\n",
      "Epoch: 395, loss: 0.51716375351, acc: 0.841594815254\n",
      "Epoch: 396, loss: 0.510478556156, acc: 0.84375\n",
      "Val: 0.0183189660311\n",
      "Epoch: 397, loss: 0.604648590088, acc: 0.814655184746\n",
      "Epoch: 398, loss: 0.56856328249, acc: 0.832974135876\n",
      "Epoch: 399, loss: 0.542949318886, acc: 0.83351290226\n",
      "Epoch: 400, loss: 0.546795487404, acc: 0.84375\n",
      "Val: 0.0285560339689\n",
      "Epoch: 401, loss: 0.608392059803, acc: 0.831357777119\n",
      "Epoch: 402, loss: 0.589291512966, acc: 0.81788790226\n",
      "Epoch: 403, loss: 0.523142576218, acc: 0.845905184746\n",
      "Epoch: 404, loss: 0.524727642536, acc: 0.838900864124\n",
      "Val: 0.0220905169845\n",
      "Epoch: 405, loss: 0.529850780964, acc: 0.84913790226\n",
      "Epoch: 406, loss: 0.534319758415, acc: 0.837823271751\n",
      "Epoch: 407, loss: 0.533451974392, acc: 0.838900864124\n",
      "Epoch: 408, loss: 0.510732352734, acc: 0.853448271751\n",
      "Val: 0.0188577584922\n",
      "Epoch: 409, loss: 0.495776176453, acc: 0.855603456497\n",
      "Epoch: 410, loss: 0.478666037321, acc: 0.867995679379\n",
      "Epoch: 411, loss: 0.503442168236, acc: 0.848599135876\n",
      "Epoch: 412, loss: 0.479039549828, acc: 0.84913790226\n",
      "Val: 0.0220905169845\n",
      "Epoch: 413, loss: 0.424494713545, acc: 0.87769395113\n",
      "Epoch: 414, loss: 0.568124890327, acc: 0.831896543503\n",
      "Epoch: 415, loss: 0.546636998653, acc: 0.840517222881\n",
      "Epoch: 416, loss: 0.505274534225, acc: 0.858836233616\n",
      "Val: 0.0220905169845\n",
      "Epoch: 417, loss: 0.468287587166, acc: 0.867995679379\n",
      "Epoch: 418, loss: 0.449701279402, acc: 0.867995679379\n",
      "Epoch: 419, loss: 0.482830733061, acc: 0.85398709774\n",
      "Epoch: 420, loss: 0.445387542248, acc: 0.865301728249\n",
      "Val: 0.0247844830155\n",
      "Epoch: 421, loss: 0.475623726845, acc: 0.851293087006\n",
      "Epoch: 422, loss: 0.462135910988, acc: 0.866379320621\n",
      "Epoch: 423, loss: 0.44893848896, acc: 0.868534505367\n",
      "Epoch: 424, loss: 0.533450007439, acc: 0.851831912994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.0242456905544\n",
      "Epoch: 425, loss: 0.465223670006, acc: 0.863685369492\n",
      "Epoch: 426, loss: 0.409205853939, acc: 0.886853456497\n",
      "Epoch: 427, loss: 0.47184458375, acc: 0.856142222881\n",
      "Epoch: 428, loss: 0.456301808357, acc: 0.864224135876\n",
      "Val: 0.0264008622617\n",
      "Epoch: 429, loss: 0.391043484211, acc: 0.882004320621\n",
      "Epoch: 430, loss: 0.495088934898, acc: 0.860452592373\n",
      "Epoch: 431, loss: 0.376690357924, acc: 0.88793104887\n",
      "Epoch: 432, loss: 0.402766466141, acc: 0.878771543503\n",
      "Val: 0.0296336207539\n",
      "Epoch: 433, loss: 0.357551604509, acc: 0.887392222881\n",
      "Epoch: 434, loss: 0.429997414351, acc: 0.870150864124\n",
      "Epoch: 435, loss: 0.407573997974, acc: 0.875538766384\n",
      "Epoch: 436, loss: 0.367391765118, acc: 0.892780184746\n",
      "Val: 0.0220905169845\n",
      "Epoch: 437, loss: 0.414235651493, acc: 0.870689630508\n",
      "Epoch: 438, loss: 0.476875752211, acc: 0.863685369492\n",
      "Epoch: 439, loss: 0.39524024725, acc: 0.889008641243\n",
      "Epoch: 440, loss: 0.386170059443, acc: 0.889008641243\n",
      "Val: 0.0220905169845\n",
      "Epoch: 441, loss: 0.326834768057, acc: 0.902478456497\n",
      "Epoch: 442, loss: 0.380901694298, acc: 0.891163766384\n",
      "Epoch: 443, loss: 0.35681912303, acc: 0.891702592373\n",
      "Epoch: 444, loss: 0.368012100458, acc: 0.892241358757\n",
      "Val: 0.0258620698005\n",
      "Epoch: 445, loss: 0.422500580549, acc: 0.876616358757\n",
      "Epoch: 446, loss: 0.337928950787, acc: 0.89601290226\n",
      "Epoch: 447, loss: 0.376493215561, acc: 0.877155184746\n",
      "Epoch: 448, loss: 0.351868212223, acc: 0.892241358757\n",
      "Val: 0.0210129301995\n",
      "Epoch: 449, loss: 0.426547378302, acc: 0.882004320621\n",
      "Epoch: 450, loss: 0.333309918642, acc: 0.894935369492\n",
      "Epoch: 451, loss: 0.351650446653, acc: 0.892780184746\n",
      "Epoch: 452, loss: 0.361158132553, acc: 0.89331895113\n",
      "Val: 0.0215517245233\n",
      "Epoch: 453, loss: 0.395459413528, acc: 0.878232777119\n",
      "Epoch: 454, loss: 0.355953872204, acc: 0.895474135876\n",
      "Epoch: 455, loss: 0.327479034662, acc: 0.90086209774\n",
      "Epoch: 456, loss: 0.366875529289, acc: 0.892241358757\n",
      "Val: 0.0264008622617\n",
      "Epoch: 457, loss: 0.356800913811, acc: 0.898168087006\n",
      "Epoch: 458, loss: 0.327070534229, acc: 0.905172407627\n",
      "Epoch: 459, loss: 0.361565947533, acc: 0.89331895113\n",
      "Epoch: 460, loss: 0.303802251816, acc: 0.90894395113\n",
      "Val: 0.0231681037694\n",
      "Epoch: 461, loss: 0.329561263323, acc: 0.897629320621\n",
      "Epoch: 462, loss: 0.352027922869, acc: 0.886853456497\n",
      "Epoch: 463, loss: 0.290218681097, acc: 0.912176728249\n",
      "Epoch: 464, loss: 0.376026242971, acc: 0.892780184746\n",
      "Val: 0.0274784490466\n",
      "Epoch: 465, loss: 0.358258217573, acc: 0.891702592373\n",
      "Epoch: 466, loss: 0.310741633177, acc: 0.899784505367\n",
      "Epoch: 467, loss: 0.326636701822, acc: 0.905172407627\n",
      "Epoch: 468, loss: 0.329961270094, acc: 0.900323271751\n",
      "Val: 0.0172413792461\n",
      "Epoch: 469, loss: 0.349066972733, acc: 0.898706912994\n",
      "Epoch: 470, loss: 0.332923561335, acc: 0.902478456497\n",
      "Epoch: 471, loss: 0.345526754856, acc: 0.894396543503\n",
      "Epoch: 472, loss: 0.349100828171, acc: 0.89601290226\n",
      "Val: 0.0193965509534\n",
      "Epoch: 473, loss: 0.329866379499, acc: 0.891702592373\n",
      "Epoch: 474, loss: 0.472822219133, acc: 0.863146543503\n",
      "Epoch: 475, loss: 0.276308119297, acc: 0.909482777119\n",
      "Epoch: 476, loss: 0.297911763191, acc: 0.914331912994\n",
      "Val: 0.0237068962306\n",
      "Epoch: 477, loss: 0.305951565504, acc: 0.908405184746\n",
      "Epoch: 478, loss: 0.291084378958, acc: 0.910021543503\n",
      "Epoch: 479, loss: 0.323259174824, acc: 0.90894395113\n",
      "Epoch: 480, loss: 0.41352263093, acc: 0.884698271751\n",
      "Val: 0.0258620698005\n",
      "Epoch: 481, loss: 0.301311314106, acc: 0.903017222881\n",
      "Epoch: 482, loss: 0.291504621506, acc: 0.910560369492\n",
      "Epoch: 483, loss: 0.224681586027, acc: 0.934267222881\n",
      "Epoch: 484, loss: 0.342811763287, acc: 0.902478456497\n",
      "Val: 0.015625\n",
      "Epoch: 485, loss: 0.312249153852, acc: 0.91163790226\n",
      "Epoch: 486, loss: 0.307727724314, acc: 0.90625\n",
      "Epoch: 487, loss: 0.295528054237, acc: 0.909482777119\n",
      "Epoch: 488, loss: 0.268776684999, acc: 0.91163790226\n",
      "Val: 0.0247844830155\n",
      "Epoch: 489, loss: 0.262500107288, acc: 0.917564630508\n",
      "Epoch: 490, loss: 0.336246401072, acc: 0.899784505367\n",
      "Epoch: 491, loss: 0.288676559925, acc: 0.913793087006\n",
      "Epoch: 492, loss: 0.310143887997, acc: 0.907866358757\n",
      "Val: 0.0242456905544\n",
      "Epoch: 493, loss: 0.332894951105, acc: 0.899245679379\n",
      "Epoch: 494, loss: 0.27338513732, acc: 0.910021543503\n",
      "Epoch: 495, loss: 0.321792572737, acc: 0.90625\n",
      "Epoch: 496, loss: 0.296106874943, acc: 0.91163790226\n",
      "Val: 0.0204741377383\n",
      "Epoch: 497, loss: 0.312521666288, acc: 0.902478456497\n",
      "Epoch: 498, loss: 0.266716390848, acc: 0.919719815254\n",
      "Epoch: 499, loss: 0.259542495012, acc: 0.923491358757\n",
      "Epoch: 500, loss: 0.303789585829, acc: 0.912715494633\n",
      "Val: 0.0199353452772\n",
      "Epoch: 501, loss: 0.285177022219, acc: 0.912176728249\n",
      "Epoch: 502, loss: 0.274661809206, acc: 0.920797407627\n",
      "Epoch: 503, loss: 0.280104279518, acc: 0.918642222881\n",
      "Epoch: 504, loss: 0.285039156675, acc: 0.91918104887\n",
      "Val: 0.0215517245233\n",
      "Epoch: 505, loss: 0.318230301142, acc: 0.901939630508\n",
      "Epoch: 506, loss: 0.286727160215, acc: 0.91648709774\n",
      "Epoch: 507, loss: 0.303208142519, acc: 0.912176728249\n",
      "Epoch: 508, loss: 0.289532870054, acc: 0.918103456497\n",
      "Val: 0.0242456905544\n",
      "Epoch: 509, loss: 0.273083180189, acc: 0.917025864124\n",
      "Epoch: 510, loss: 0.272337824106, acc: 0.915948271751\n",
      "Epoch: 511, loss: 0.206436008215, acc: 0.933728456497\n",
      "Epoch: 512, loss: 0.246602445841, acc: 0.925646543503\n",
      "Val: 0.0210129301995\n",
      "Epoch: 513, loss: 0.221259102225, acc: 0.93211209774\n",
      "Epoch: 514, loss: 0.253808796406, acc: 0.918103456497\n",
      "Epoch: 515, loss: 0.259934157133, acc: 0.920797407627\n",
      "Epoch: 516, loss: 0.267416089773, acc: 0.917025864124\n",
      "Val: 0.0231681037694\n",
      "Epoch: 517, loss: 0.259090542793, acc: 0.928879320621\n",
      "Epoch: 518, loss: 0.25265750289, acc: 0.927801728249\n",
      "Epoch: 519, loss: 0.265303999186, acc: 0.917564630508\n",
      "Epoch: 520, loss: 0.34226885438, acc: 0.898706912994\n",
      "Val: 0.0220905169845\n",
      "Epoch: 521, loss: 0.278303146362, acc: 0.923491358757\n",
      "Epoch: 522, loss: 0.241383418441, acc: 0.923491358757\n",
      "Epoch: 523, loss: 0.249171957374, acc: 0.926724135876\n",
      "Epoch: 524, loss: 0.259962320328, acc: 0.917564630508\n",
      "Val: 0.0215517245233\n",
      "Epoch: 525, loss: 0.263527214527, acc: 0.921336233616\n",
      "Epoch: 526, loss: 0.22074200213, acc: 0.936422407627\n",
      "Epoch: 527, loss: 0.241056591272, acc: 0.92726290226\n",
      "Epoch: 528, loss: 0.218904644251, acc: 0.929418087006\n",
      "Val: 0.0242456905544\n",
      "Epoch: 529, loss: 0.225981280208, acc: 0.933189630508\n",
      "Epoch: 530, loss: 0.240724042058, acc: 0.927801728249\n",
      "Epoch: 531, loss: 0.248427063227, acc: 0.929956912994\n",
      "Epoch: 532, loss: 0.229903385043, acc: 0.932650864124\n",
      "Val: 0.0231681037694\n",
      "Epoch: 533, loss: 0.247414246202, acc: 0.925646543503\n",
      "Epoch: 534, loss: 0.205552950501, acc: 0.939655184746\n",
      "Epoch: 535, loss: 0.242208257318, acc: 0.933189630508\n",
      "Epoch: 536, loss: 0.205164223909, acc: 0.936961233616\n",
      "Val: 0.0199353452772\n",
      "Epoch: 537, loss: 0.225374519825, acc: 0.925107777119\n",
      "Epoch: 538, loss: 0.2772744596, acc: 0.910021543503\n",
      "Epoch: 539, loss: 0.255634963512, acc: 0.922952592373\n",
      "Epoch: 540, loss: 0.24833509326, acc: 0.929956912994\n",
      "Val: 0.016702586785\n",
      "Epoch: 541, loss: 0.225415766239, acc: 0.936961233616\n",
      "Epoch: 542, loss: 0.220527693629, acc: 0.931034505367\n",
      "Epoch: 543, loss: 0.222517073154, acc: 0.927801728249\n",
      "Epoch: 544, loss: 0.171489700675, acc: 0.948275864124\n",
      "Val: 0.0247844830155\n",
      "Epoch: 545, loss: 0.219756618142, acc: 0.934267222881\n",
      "Epoch: 546, loss: 0.267575591803, acc: 0.912715494633\n",
      "Epoch: 547, loss: 0.195771366358, acc: 0.938577592373\n",
      "Epoch: 548, loss: 0.206906333566, acc: 0.9375\n",
      "Val: 0.0226293094456\n",
      "Epoch: 549, loss: 0.251794248819, acc: 0.914870679379\n",
      "Epoch: 550, loss: 0.191496163607, acc: 0.946120679379\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a13b6e3407c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#memorizing3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.pyc\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_save\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/training_emb_e{}_l{}_val_data'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                     \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mfid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "#memorizing3\n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])\n",
    "\n",
    "'''Train generator ready, time elapsed: 404.054769993\n",
    "Epoch: 0, loss: 4.22428750992, acc: 0.0188577584922\n",
    "...\n",
    "Epoch: 540, loss: 0.24833509326, acc: 0.929956912994'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 18.0721051693\n",
      "Epoch: 0, loss: 4.20792913437, acc: 0.0183189660311\n",
      "Val: 0.0183189660311\n",
      "Epoch: 1, loss: 4.55330848694, acc: 0.015625\n",
      "Epoch: 2, loss: 4.20648241043, acc: 0.0150862066075\n",
      "Val: 0.0215517245233\n",
      "Epoch: 3, loss: 4.22681236267, acc: 0.0183189660311\n",
      "Epoch: 4, loss: 4.15933656693, acc: 0.0172413792461\n",
      "Val: 0.0215517245233\n",
      "Epoch: 5, loss: 4.15010976791, acc: 0.0150862066075\n",
      "Epoch: 6, loss: 4.12266159058, acc: 0.0199353452772\n",
      "Val: 0.0199353452772\n",
      "Epoch: 7, loss: 4.14025831223, acc: 0.0177801717073\n",
      "Epoch: 8, loss: 4.12471199036, acc: 0.0210129301995\n",
      "Val: 0.0215517245233\n",
      "Epoch: 9, loss: 4.06814861298, acc: 0.016702586785\n",
      "Epoch: 10, loss: 4.06806898117, acc: 0.0215517245233\n",
      "Val: 0.0215517245233\n",
      "Epoch: 11, loss: 4.08185100555, acc: 0.016702586785\n",
      "Epoch: 12, loss: 4.05561637878, acc: 0.0177801717073\n",
      "Val: 0.0177801717073\n",
      "Epoch: 13, loss: 4.08010292053, acc: 0.0237068962306\n",
      "Epoch: 14, loss: 4.05409908295, acc: 0.0161637924612\n",
      "Val: 0.0188577584922\n",
      "Epoch: 15, loss: 4.08373260498, acc: 0.0199353452772\n",
      "Epoch: 16, loss: 4.0639257431, acc: 0.0134698273614\n",
      "Val: 0.0188577584922\n",
      "Epoch: 17, loss: 4.061439991, acc: 0.0140086207539\n",
      "Epoch: 18, loss: 4.04562282562, acc: 0.0210129301995\n",
      "Val: 0.0215517245233\n",
      "Epoch: 19, loss: 3.92608189583, acc: 0.0150862066075\n",
      "Epoch: 20, loss: 3.85578131676, acc: 0.0247844830155\n",
      "Val: 0.0193965509534\n",
      "Epoch: 21, loss: 3.8552942276, acc: 0.0161637924612\n",
      "Epoch: 22, loss: 3.85151457787, acc: 0.0161637924612\n",
      "Val: 0.0145474141464\n",
      "Epoch: 23, loss: 3.85049247742, acc: 0.0193965509534\n",
      "Epoch: 24, loss: 3.84643340111, acc: 0.0226293094456\n",
      "Val: 0.0210129301995\n",
      "Epoch: 25, loss: 3.84736728668, acc: 0.0237068962306\n",
      "Epoch: 26, loss: 3.84548687935, acc: 0.0226293094456\n",
      "Val: 0.0226293094456\n",
      "Epoch: 27, loss: 3.84297800064, acc: 0.0269396547228\n",
      "Epoch: 28, loss: 3.84020757675, acc: 0.0210129301995\n",
      "Val: 0.0220905169845\n",
      "Epoch: 29, loss: 3.84042978287, acc: 0.0280172415078\n",
      "Epoch: 30, loss: 3.84158277512, acc: 0.0247844830155\n",
      "Val: 0.0188577584922\n",
      "Epoch: 31, loss: 3.83848309517, acc: 0.0274784490466\n",
      "Epoch: 32, loss: 3.83667063713, acc: 0.0253232754767\n",
      "Val: 0.0296336207539\n",
      "Epoch: 33, loss: 3.8376891613, acc: 0.0280172415078\n",
      "Epoch: 34, loss: 3.83708024025, acc: 0.0183189660311\n",
      "Val: 0.0215517245233\n",
      "Epoch: 35, loss: 3.83961939812, acc: 0.0247844830155\n",
      "Epoch: 36, loss: 3.83892011642, acc: 0.0264008622617\n",
      "Val: 0.0237068962306\n",
      "Epoch: 37, loss: 3.83557271957, acc: 0.0258620698005\n",
      "Epoch: 38, loss: 3.83791637421, acc: 0.0247844830155\n",
      "Val: 0.0290948282927\n",
      "Epoch: 39, loss: 3.83438801765, acc: 0.0280172415078\n",
      "Epoch: 40, loss: 3.83200383186, acc: 0.0253232754767\n",
      "Val: 0.0204741377383\n",
      "Epoch: 41, loss: 3.83504962921, acc: 0.0258620698005\n",
      "Epoch: 42, loss: 3.8315308094, acc: 0.0285560339689\n",
      "Val: 0.0382543094456\n",
      "Epoch: 43, loss: 3.83244752884, acc: 0.0296336207539\n",
      "Epoch: 44, loss: 3.83388972282, acc: 0.0280172415078\n",
      "Val: 0.0177801717073\n",
      "Epoch: 45, loss: 3.83446073532, acc: 0.0307112075388\n",
      "Epoch: 46, loss: 3.83475852013, acc: 0.0215517245233\n",
      "Val: 0.0237068962306\n",
      "Epoch: 47, loss: 3.82827854156, acc: 0.030172413215\n",
      "Epoch: 48, loss: 3.82573342323, acc: 0.0307112075388\n",
      "Val: 0.0215517245233\n",
      "Epoch: 49, loss: 3.83379530907, acc: 0.0307112075388\n",
      "Epoch: 50, loss: 3.82539725304, acc: 0.0307112075388\n",
      "Val: 0.030172413215\n",
      "Epoch: 51, loss: 3.82332706451, acc: 0.0355603434145\n",
      "Epoch: 52, loss: 3.82604408264, acc: 0.0290948282927\n",
      "Val: 0.0285560339689\n",
      "Epoch: 53, loss: 3.82335186005, acc: 0.0307112075388\n",
      "Epoch: 54, loss: 3.82969045639, acc: 0.0328663811088\n",
      "Val: 0.0344827584922\n",
      "Epoch: 55, loss: 3.82106637955, acc: 0.0339439660311\n",
      "Epoch: 56, loss: 3.82175540924, acc: 0.0290948282927\n",
      "Val: 0.0210129301995\n",
      "Epoch: 57, loss: 3.82017421722, acc: 0.0350215509534\n",
      "Epoch: 58, loss: 3.81415081024, acc: 0.030172413215\n",
      "Val: 0.0398706905544\n",
      "Epoch: 59, loss: 3.81888055801, acc: 0.0323275849223\n",
      "Epoch: 60, loss: 3.81698703766, acc: 0.0339439660311\n",
      "Val: 0.0264008622617\n",
      "Epoch: 61, loss: 3.81304121017, acc: 0.0328663811088\n",
      "Epoch: 62, loss: 3.82247138023, acc: 0.0307112075388\n",
      "Val: 0.0350215509534\n",
      "Epoch: 63, loss: 3.82059288025, acc: 0.0296336207539\n",
      "Epoch: 64, loss: 3.81797575951, acc: 0.0307112075388\n",
      "Val: 0.0247844830155\n",
      "Epoch: 65, loss: 3.81630730629, acc: 0.0317887924612\n",
      "Epoch: 66, loss: 3.81181144714, acc: 0.0339439660311\n",
      "Val: 0.0237068962306\n",
      "Epoch: 67, loss: 3.81266307831, acc: 0.0317887924612\n",
      "Epoch: 68, loss: 3.81841039658, acc: 0.03125\n",
      "Val: 0.0237068962306\n",
      "Epoch: 69, loss: 3.81301522255, acc: 0.0323275849223\n",
      "Epoch: 70, loss: 3.81202697754, acc: 0.0328663811088\n",
      "Val: 0.0323275849223\n",
      "Epoch: 71, loss: 3.81344509125, acc: 0.0323275849223\n",
      "Epoch: 72, loss: 3.80381417274, acc: 0.0344827584922\n",
      "Val: 0.0290948282927\n",
      "Epoch: 73, loss: 3.80968570709, acc: 0.0350215509534\n",
      "Epoch: 74, loss: 3.81146836281, acc: 0.0328663811088\n",
      "Val: 0.0129310349002\n",
      "Epoch: 75, loss: 3.79966235161, acc: 0.0366379320621\n",
      "Epoch: 76, loss: 3.78578042984, acc: 0.0366379320621\n",
      "Val: 0.0188577584922\n",
      "Epoch: 77, loss: 3.80256223679, acc: 0.0344827584922\n",
      "Epoch: 78, loss: 3.80545759201, acc: 0.0328663811088\n",
      "Val: 0.015625\n",
      "Epoch: 79, loss: 3.80138707161, acc: 0.0334051735699\n",
      "Epoch: 80, loss: 3.8036262989, acc: 0.0296336207539\n",
      "Val: 0.0355603434145\n",
      "Epoch: 81, loss: 3.7921102047, acc: 0.0339439660311\n",
      "Epoch: 82, loss: 3.79002332687, acc: 0.0377155169845\n",
      "Val: 0.0231681037694\n",
      "Epoch: 83, loss: 3.81154322624, acc: 0.03125\n",
      "Epoch: 84, loss: 3.80120825768, acc: 0.0366379320621\n",
      "Val: 0.0129310349002\n",
      "Epoch: 85, loss: 3.78982043266, acc: 0.036099139601\n",
      "Epoch: 86, loss: 3.80179190636, acc: 0.0317887924612\n",
      "Val: 0.0145474141464\n",
      "Epoch: 87, loss: 3.77934813499, acc: 0.0382543094456\n",
      "Epoch: 88, loss: 3.78807687759, acc: 0.0371767245233\n",
      "Val: 0.0172413792461\n",
      "Epoch: 89, loss: 3.80488228798, acc: 0.0307112075388\n",
      "Epoch: 90, loss: 3.78867268562, acc: 0.0366379320621\n",
      "Val: 0.0183189660311\n",
      "Epoch: 91, loss: 3.79036307335, acc: 0.0323275849223\n",
      "Epoch: 92, loss: 3.79704046249, acc: 0.0409482754767\n",
      "Val: 0.0215517245233\n",
      "Epoch: 93, loss: 3.78676605225, acc: 0.0350215509534\n",
      "Epoch: 94, loss: 3.78166675568, acc: 0.0404094830155\n",
      "Val: 0.0242456905544\n",
      "Epoch: 95, loss: 3.78709769249, acc: 0.0328663811088\n",
      "Epoch: 96, loss: 3.78168225288, acc: 0.0339439660311\n",
      "Val: 0.0193965509534\n",
      "Epoch: 97, loss: 3.79002690315, acc: 0.0323275849223\n",
      "Epoch: 98, loss: 3.77981042862, acc: 0.0425646565855\n",
      "Val: 0.0269396547228\n",
      "Epoch: 99, loss: 3.78346204758, acc: 0.0355603434145\n",
      "Epoch: 100, loss: 3.78266215324, acc: 0.0371767245233\n",
      "Val: 0.0247844830155\n",
      "Epoch: 101, loss: 3.77811169624, acc: 0.0366379320621\n",
      "Epoch: 102, loss: 3.78457570076, acc: 0.0371767245233\n",
      "Val: 0.03125\n",
      "Epoch: 103, loss: 3.77413511276, acc: 0.0377155169845\n",
      "Epoch: 104, loss: 3.77133774757, acc: 0.0344827584922\n",
      "Val: 0.0371767245233\n",
      "Epoch: 105, loss: 3.76885223389, acc: 0.0344827584922\n",
      "Epoch: 106, loss: 3.7737364769, acc: 0.0366379320621\n",
      "Val: 0.0307112075388\n",
      "Epoch: 107, loss: 3.77530455589, acc: 0.0350215509534\n",
      "Epoch: 108, loss: 3.77492880821, acc: 0.0355603434145\n",
      "Val: 0.0307112075388\n",
      "Epoch: 109, loss: 3.77527070045, acc: 0.0334051735699\n",
      "Epoch: 110, loss: 3.76965761185, acc: 0.0366379320621\n",
      "Val: 0.0183189660311\n",
      "Epoch: 111, loss: 3.76258277893, acc: 0.036099139601\n",
      "Epoch: 112, loss: 3.77245473862, acc: 0.030172413215\n",
      "Val: 0.0258620698005\n",
      "Epoch: 113, loss: 3.76356720924, acc: 0.0323275849223\n",
      "Epoch: 114, loss: 3.7635178566, acc: 0.0387931019068\n",
      "Val: 0.03125\n",
      "Epoch: 115, loss: 3.76509284973, acc: 0.0409482754767\n",
      "Epoch: 116, loss: 3.76680064201, acc: 0.0344827584922\n",
      "Val: 0.0199353452772\n",
      "Epoch: 117, loss: 3.76199436188, acc: 0.0393318980932\n",
      "Epoch: 118, loss: 3.76152539253, acc: 0.0371767245233\n",
      "Val: 0.0398706905544\n",
      "Epoch: 119, loss: 3.75785899162, acc: 0.042025860399\n",
      "Epoch: 120, loss: 3.75780749321, acc: 0.042025860399\n",
      "Val: 0.0226293094456\n",
      "Epoch: 121, loss: 3.74671578407, acc: 0.0393318980932\n",
      "Epoch: 122, loss: 3.76648521423, acc: 0.0371767245233\n",
      "Val: 0.0253232754767\n",
      "Epoch: 123, loss: 3.74831318855, acc: 0.0431034490466\n",
      "Epoch: 124, loss: 3.74768853188, acc: 0.0447198264301\n",
      "Val: 0.015625\n",
      "Epoch: 125, loss: 3.7455368042, acc: 0.0404094830155\n",
      "Epoch: 126, loss: 3.74802112579, acc: 0.0452586188912\n",
      "Val: 0.0414870679379\n",
      "Epoch: 127, loss: 3.73542094231, acc: 0.0425646565855\n",
      "Epoch: 128, loss: 3.73849844933, acc: 0.0377155169845\n",
      "Val: 0.0258620698005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129, loss: 3.74903798103, acc: 0.0436422415078\n",
      "Epoch: 130, loss: 3.75867509842, acc: 0.0409482754767\n",
      "Val: 0.0231681037694\n",
      "Epoch: 131, loss: 3.73664069176, acc: 0.0387931019068\n",
      "Epoch: 132, loss: 3.74057865143, acc: 0.0436422415078\n",
      "Val: 0.0199353452772\n",
      "Epoch: 133, loss: 3.7403011322, acc: 0.0409482754767\n",
      "Epoch: 134, loss: 3.74379563332, acc: 0.042025860399\n",
      "Val: 0.030172413215\n",
      "Epoch: 135, loss: 3.73359370232, acc: 0.0457974150777\n",
      "Epoch: 136, loss: 3.74092340469, acc: 0.036099139601\n",
      "Val: 0.0237068962306\n",
      "Epoch: 137, loss: 3.72794151306, acc: 0.0463362075388\n",
      "Epoch: 138, loss: 3.73515319824, acc: 0.0409482754767\n",
      "Val: 0.0377155169845\n",
      "Epoch: 139, loss: 3.72564077377, acc: 0.0409482754767\n",
      "Epoch: 140, loss: 3.73587489128, acc: 0.042025860399\n",
      "Val: 0.0393318980932\n",
      "Epoch: 141, loss: 3.73407077789, acc: 0.0463362075388\n",
      "Epoch: 142, loss: 3.73939156532, acc: 0.0350215509534\n",
      "Val: 0.0220905169845\n",
      "Epoch: 143, loss: 3.73675084114, acc: 0.0463362075388\n",
      "Epoch: 144, loss: 3.7372367382, acc: 0.042025860399\n",
      "Val: 0.0237068962306\n",
      "Epoch: 145, loss: 3.71815562248, acc: 0.0431034490466\n",
      "Epoch: 146, loss: 3.72847127914, acc: 0.0377155169845\n",
      "Val: 0.0231681037694\n",
      "Epoch: 147, loss: 3.71353983879, acc: 0.0398706905544\n",
      "Epoch: 148, loss: 3.71571564674, acc: 0.0447198264301\n",
      "Val: 0.0237068962306\n",
      "Epoch: 149, loss: 3.7167134285, acc: 0.0441810339689\n",
      "Epoch: 150, loss: 3.71346640587, acc: 0.0409482754767\n",
      "Val: 0.0215517245233\n",
      "Epoch: 151, loss: 3.72006392479, acc: 0.046875\n",
      "Epoch: 152, loss: 3.70659470558, acc: 0.0447198264301\n",
      "Val: 0.03125\n",
      "Epoch: 153, loss: 3.71998906136, acc: 0.0436422415078\n",
      "Epoch: 154, loss: 3.70895338058, acc: 0.0528017245233\n",
      "Val: 0.0193965509534\n",
      "Epoch: 155, loss: 3.71849775314, acc: 0.0425646565855\n",
      "Epoch: 156, loss: 3.70465040207, acc: 0.0366379320621\n",
      "Val: 0.0237068962306\n",
      "Epoch: 157, loss: 3.70264649391, acc: 0.0457974150777\n",
      "Epoch: 158, loss: 3.70717954636, acc: 0.042025860399\n",
      "Val: 0.036099139601\n",
      "Epoch: 159, loss: 3.69497513771, acc: 0.0447198264301\n",
      "Epoch: 160, loss: 3.6883995533, acc: 0.0452586188912\n",
      "Val: 0.0231681037694\n",
      "Epoch: 161, loss: 3.7055952549, acc: 0.0452586188912\n",
      "Epoch: 162, loss: 3.68520402908, acc: 0.0528017245233\n",
      "Val: 0.0237068962306\n",
      "Epoch: 163, loss: 3.71061968803, acc: 0.0474137924612\n",
      "Epoch: 164, loss: 3.69436883926, acc: 0.0501077584922\n",
      "Val: 0.036099139601\n",
      "Epoch: 165, loss: 3.69120836258, acc: 0.0463362075388\n",
      "Epoch: 166, loss: 3.68466591835, acc: 0.0511853434145\n",
      "Val: 0.03125\n",
      "Epoch: 167, loss: 3.69060015678, acc: 0.0490301735699\n",
      "Epoch: 168, loss: 3.68547320366, acc: 0.0511853434145\n",
      "Val: 0.0247844830155\n",
      "Epoch: 169, loss: 3.68623399734, acc: 0.0463362075388\n",
      "Epoch: 170, loss: 3.67375922203, acc: 0.0641163811088\n",
      "Val: 0.03125\n",
      "Epoch: 171, loss: 3.68911719322, acc: 0.051724139601\n",
      "Epoch: 172, loss: 3.68523335457, acc: 0.0474137924612\n",
      "Val: 0.0274784490466\n",
      "Epoch: 173, loss: 3.69017648697, acc: 0.0436422415078\n",
      "Epoch: 174, loss: 3.67034721375, acc: 0.0484913811088\n",
      "Val: 0.0317887924612\n",
      "Epoch: 175, loss: 3.67688846588, acc: 0.0592672415078\n",
      "Epoch: 176, loss: 3.67230629921, acc: 0.051724139601\n",
      "Val: 0.0253232754767\n",
      "Epoch: 177, loss: 3.6591668129, acc: 0.0560344830155\n",
      "Epoch: 178, loss: 3.66415929794, acc: 0.0533405169845\n",
      "Val: 0.0231681037694\n",
      "Epoch: 179, loss: 3.66485095024, acc: 0.0560344830155\n",
      "Epoch: 180, loss: 3.67008137703, acc: 0.057650860399\n",
      "Val: 0.030172413215\n",
      "Epoch: 181, loss: 3.65126442909, acc: 0.0554956905544\n",
      "Epoch: 182, loss: 3.66491889954, acc: 0.0560344830155\n",
      "Val: 0.030172413215\n",
      "Epoch: 183, loss: 3.66597652435, acc: 0.0549568980932\n",
      "Epoch: 184, loss: 3.63918209076, acc: 0.0522629320621\n",
      "Val: 0.03125\n",
      "Epoch: 185, loss: 3.63592505455, acc: 0.0608836188912\n",
      "Epoch: 186, loss: 3.63897705078, acc: 0.0560344830155\n",
      "Val: 0.0355603434145\n",
      "Epoch: 187, loss: 3.67118120193, acc: 0.0646551698446\n",
      "Epoch: 188, loss: 3.65904927254, acc: 0.0603448264301\n",
      "Val: 0.0220905169845\n",
      "Epoch: 189, loss: 3.64615726471, acc: 0.057650860399\n",
      "Epoch: 190, loss: 3.6365032196, acc: 0.0511853434145\n",
      "Val: 0.0253232754767\n",
      "Epoch: 191, loss: 3.65356087685, acc: 0.0560344830155\n",
      "Epoch: 192, loss: 3.6442232132, acc: 0.0549568980932\n",
      "Val: 0.0193965509534\n",
      "Epoch: 193, loss: 3.63715624809, acc: 0.0619612075388\n",
      "Epoch: 194, loss: 3.64213132858, acc: 0.0592672415078\n",
      "Val: 0.0253232754767\n",
      "Epoch: 195, loss: 3.6089193821, acc: 0.0630387961864\n",
      "Epoch: 196, loss: 3.63472175598, acc: 0.057650860399\n",
      "Val: 0.0258620698005\n",
      "Epoch: 197, loss: 3.64174461365, acc: 0.0554956905544\n",
      "Epoch: 198, loss: 3.62305092812, acc: 0.0608836188912\n",
      "Val: 0.0280172415078\n",
      "Epoch: 199, loss: 3.61925244331, acc: 0.0630387961864\n",
      "Epoch: 200, loss: 3.64796710014, acc: 0.0533405169845\n",
      "Val: 0.0280172415078\n",
      "Epoch: 201, loss: 3.61595153809, acc: 0.0608836188912\n",
      "Epoch: 202, loss: 3.62604117393, acc: 0.0657327622175\n",
      "Val: 0.0247844830155\n",
      "Epoch: 203, loss: 3.60059261322, acc: 0.0711206868291\n",
      "Epoch: 204, loss: 3.61859130859, acc: 0.0662715509534\n",
      "Val: 0.0253232754767\n",
      "Epoch: 205, loss: 3.61245679855, acc: 0.0619612075388\n",
      "Epoch: 206, loss: 3.59088635445, acc: 0.0727370679379\n",
      "Val: 0.0264008622617\n",
      "Epoch: 207, loss: 3.60797834396, acc: 0.0619612075388\n",
      "Epoch: 208, loss: 3.61972904205, acc: 0.0695043131709\n",
      "Val: 0.0253232754767\n",
      "Epoch: 209, loss: 3.61057543755, acc: 0.0598060339689\n",
      "Epoch: 210, loss: 3.60611963272, acc: 0.0651939660311\n",
      "Val: 0.0328663811088\n",
      "Epoch: 211, loss: 3.58692574501, acc: 0.0695043131709\n",
      "Epoch: 212, loss: 3.59893631935, acc: 0.0695043131709\n",
      "Val: 0.0247844830155\n",
      "Epoch: 213, loss: 3.58729624748, acc: 0.0738146528602\n",
      "Epoch: 214, loss: 3.5932431221, acc: 0.0668103471398\n",
      "Val: 0.0334051735699\n",
      "Epoch: 215, loss: 3.58417963982, acc: 0.0754310339689\n",
      "Epoch: 216, loss: 3.56478285789, acc: 0.0705818980932\n",
      "Val: 0.0339439660311\n",
      "Epoch: 217, loss: 3.59859251976, acc: 0.0678879320621\n",
      "Epoch: 218, loss: 3.56671118736, acc: 0.0738146528602\n",
      "Val: 0.0350215509534\n",
      "Epoch: 219, loss: 3.5800383091, acc: 0.0727370679379\n",
      "Epoch: 220, loss: 3.57173514366, acc: 0.068426720798\n",
      "Val: 0.0226293094456\n",
      "Epoch: 221, loss: 3.57141089439, acc: 0.0743534490466\n",
      "Epoch: 222, loss: 3.55657958984, acc: 0.0808189660311\n",
      "Val: 0.0274784490466\n",
      "Epoch: 223, loss: 3.55326795578, acc: 0.0754310339689\n",
      "Epoch: 224, loss: 3.53675317764, acc: 0.0813577622175\n",
      "Val: 0.0323275849223\n",
      "Epoch: 225, loss: 3.58437108994, acc: 0.0705818980932\n",
      "Epoch: 226, loss: 3.54186868668, acc: 0.0792025849223\n",
      "Val: 0.0366379320621\n",
      "Epoch: 227, loss: 3.5648920536, acc: 0.0792025849223\n",
      "Epoch: 228, loss: 3.5390355587, acc: 0.0775862038136\n",
      "Val: 0.0253232754767\n",
      "Epoch: 229, loss: 3.52269649506, acc: 0.0835129320621\n",
      "Epoch: 230, loss: 3.54562973976, acc: 0.0845905169845\n",
      "Val: 0.0253232754767\n",
      "Epoch: 231, loss: 3.54070091248, acc: 0.0792025849223\n",
      "Epoch: 232, loss: 3.52467918396, acc: 0.0775862038136\n",
      "Val: 0.0280172415078\n",
      "Epoch: 233, loss: 3.50732183456, acc: 0.0899784490466\n",
      "Epoch: 234, loss: 3.51147723198, acc: 0.0856681019068\n",
      "Val: 0.0344827584922\n",
      "Epoch: 235, loss: 3.51997542381, acc: 0.0894396528602\n",
      "Epoch: 236, loss: 3.51564049721, acc: 0.0808189660311\n",
      "Val: 0.0285560339689\n",
      "Epoch: 237, loss: 3.51777386665, acc: 0.0792025849223\n",
      "Epoch: 238, loss: 3.51044154167, acc: 0.0883620679379\n",
      "Val: 0.03125\n",
      "Epoch: 239, loss: 3.50933170319, acc: 0.0808189660311\n",
      "Epoch: 240, loss: 3.50263953209, acc: 0.0915948301554\n",
      "Val: 0.0307112075388\n",
      "Epoch: 241, loss: 3.51368713379, acc: 0.0894396528602\n",
      "Epoch: 242, loss: 3.5161254406, acc: 0.0851293131709\n",
      "Val: 0.03125\n",
      "Epoch: 243, loss: 3.506929636, acc: 0.0948275849223\n",
      "Epoch: 244, loss: 3.46217060089, acc: 0.0845905169845\n",
      "Val: 0.0290948282927\n",
      "Epoch: 245, loss: 3.46673083305, acc: 0.100754313171\n",
      "Epoch: 246, loss: 3.46081113815, acc: 0.0953663811088\n",
      "Val: 0.0307112075388\n",
      "Epoch: 247, loss: 3.4575855732, acc: 0.09375\n",
      "Epoch: 248, loss: 3.49787735939, acc: 0.0915948301554\n",
      "Val: 0.0269396547228\n",
      "Epoch: 249, loss: 3.48885393143, acc: 0.0969827622175\n",
      "Epoch: 250, loss: 3.45425748825, acc: 0.108836203814\n",
      "Val: 0.0377155169845\n",
      "Epoch: 251, loss: 3.45611786842, acc: 0.099676720798\n",
      "Epoch: 252, loss: 3.44511246681, acc: 0.0905172377825\n",
      "Val: 0.0371767245233\n",
      "Epoch: 253, loss: 3.41995787621, acc: 0.103448279202\n",
      "Epoch: 254, loss: 3.43959379196, acc: 0.099676720798\n",
      "Val: 0.0355603434145\n",
      "Epoch: 255, loss: 3.46892285347, acc: 0.103987067938\n",
      "Epoch: 256, loss: 3.46572422981, acc: 0.0926724150777\n",
      "Val: 0.0237068962306\n",
      "Epoch: 257, loss: 3.41241812706, acc: 0.107758618891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 258, loss: 3.43354964256, acc: 0.0969827622175\n",
      "Val: 0.0323275849223\n",
      "Epoch: 259, loss: 3.40730023384, acc: 0.109913796186\n",
      "Epoch: 260, loss: 3.39905619621, acc: 0.104525864124\n",
      "Val: 0.0323275849223\n",
      "Epoch: 261, loss: 3.41499018669, acc: 0.0959051698446\n",
      "Epoch: 262, loss: 3.40691900253, acc: 0.117995686829\n",
      "Val: 0.03125\n",
      "Epoch: 263, loss: 3.41123151779, acc: 0.108297415078\n",
      "Epoch: 264, loss: 3.38324594498, acc: 0.110452584922\n",
      "Val: 0.0231681037694\n",
      "Epoch: 265, loss: 3.35888743401, acc: 0.122844830155\n",
      "Epoch: 266, loss: 3.39874577522, acc: 0.111530169845\n",
      "Val: 0.0328663811088\n",
      "Epoch: 267, loss: 3.36113715172, acc: 0.110991381109\n",
      "Epoch: 268, loss: 3.39433860779, acc: 0.109913796186\n",
      "Val: 0.0339439660311\n",
      "Epoch: 269, loss: 3.38002753258, acc: 0.106142237782\n",
      "Epoch: 270, loss: 3.3374516964, acc: 0.116379313171\n",
      "Val: 0.0307112075388\n",
      "Epoch: 271, loss: 3.39272499084, acc: 0.106142237782\n",
      "Epoch: 272, loss: 3.36012673378, acc: 0.113146550953\n",
      "Val: 0.03125\n",
      "Epoch: 273, loss: 3.32616257668, acc: 0.132543101907\n",
      "Epoch: 274, loss: 3.3265337944, acc: 0.118534483016\n",
      "Val: 0.0350215509534\n",
      "Epoch: 275, loss: 3.35444378853, acc: 0.117456898093\n",
      "Epoch: 276, loss: 3.38285374641, acc: 0.121228449047\n",
      "Val: 0.0382543094456\n",
      "Epoch: 277, loss: 3.33924651146, acc: 0.117995686829\n",
      "Epoch: 278, loss: 3.32309317589, acc: 0.126616373658\n",
      "Val: 0.0323275849223\n",
      "Epoch: 279, loss: 3.331158638, acc: 0.122306033969\n",
      "Epoch: 280, loss: 3.31661367416, acc: 0.12068965286\n",
      "Val: 0.0371767245233\n",
      "Epoch: 281, loss: 3.30608081818, acc: 0.128232762218\n",
      "Epoch: 282, loss: 3.28268933296, acc: 0.133081898093\n",
      "Val: 0.030172413215\n",
      "Epoch: 283, loss: 3.34112453461, acc: 0.129310339689\n",
      "Epoch: 284, loss: 3.30922102928, acc: 0.132543101907\n",
      "Val: 0.0253232754767\n",
      "Epoch: 285, loss: 3.30607461929, acc: 0.129310339689\n",
      "Epoch: 286, loss: 3.27184057236, acc: 0.129310339689\n",
      "Val: 0.03125\n",
      "Epoch: 287, loss: 3.23324370384, acc: 0.144935339689\n",
      "Epoch: 288, loss: 3.27623176575, acc: 0.13362069428\n",
      "Val: 0.0317887924612\n",
      "Epoch: 289, loss: 3.24112343788, acc: 0.137392237782\n",
      "Epoch: 290, loss: 3.22523093224, acc: 0.153017237782\n",
      "Val: 0.0393318980932\n",
      "Epoch: 291, loss: 3.22291564941, acc: 0.154633626342\n",
      "Epoch: 292, loss: 3.19971370697, acc: 0.146551728249\n",
      "Val: 0.0355603434145\n",
      "Epoch: 293, loss: 3.22937250137, acc: 0.135775864124\n",
      "Epoch: 294, loss: 3.246073246, acc: 0.148168101907\n",
      "Val: 0.0371767245233\n",
      "Epoch: 295, loss: 3.2441637516, acc: 0.149784475565\n",
      "Epoch: 296, loss: 3.16838145256, acc: 0.169719830155\n",
      "Val: 0.0328663811088\n",
      "Epoch: 297, loss: 3.17943072319, acc: 0.167564660311\n",
      "Epoch: 298, loss: 3.19635820389, acc: 0.164331898093\n",
      "Val: 0.0285560339689\n",
      "Epoch: 299, loss: 3.16771101952, acc: 0.16325430572\n",
      "Epoch: 300, loss: 3.21113562584, acc: 0.150862067938\n",
      "Val: 0.0269396547228\n",
      "Epoch: 301, loss: 3.19003367424, acc: 0.14924569428\n",
      "Epoch: 302, loss: 3.15639543533, acc: 0.163793101907\n",
      "Val: 0.0264008622617\n",
      "Epoch: 303, loss: 3.19814801216, acc: 0.165948271751\n",
      "Epoch: 304, loss: 3.18603372574, acc: 0.158943966031\n",
      "Val: 0.0204741377383\n",
      "Epoch: 305, loss: 3.11409807205, acc: 0.167564660311\n",
      "Epoch: 306, loss: 3.12717437744, acc: 0.157327592373\n",
      "Val: 0.0237068962306\n",
      "Epoch: 307, loss: 3.14078783989, acc: 0.158405169845\n",
      "Epoch: 308, loss: 3.10821127892, acc: 0.176724135876\n",
      "Val: 0.030172413215\n",
      "Epoch: 309, loss: 3.08497023582, acc: 0.170797407627\n",
      "Epoch: 310, loss: 3.13420605659, acc: 0.161637932062\n",
      "Val: 0.030172413215\n",
      "Epoch: 311, loss: 3.12631964684, acc: 0.182112067938\n",
      "Epoch: 312, loss: 3.11666965485, acc: 0.181034475565\n",
      "Val: 0.0264008622617\n",
      "Epoch: 313, loss: 3.07688164711, acc: 0.170258626342\n",
      "Epoch: 314, loss: 3.02223825455, acc: 0.19612069428\n",
      "Val: 0.0242456905544\n",
      "Epoch: 315, loss: 3.04263496399, acc: 0.1875\n",
      "Epoch: 316, loss: 3.04702043533, acc: 0.189116373658\n",
      "Val: 0.0247844830155\n",
      "Epoch: 317, loss: 3.06074357033, acc: 0.177262932062\n",
      "Epoch: 318, loss: 3.04580640793, acc: 0.183728441596\n",
      "Val: 0.0242456905544\n",
      "Epoch: 319, loss: 3.05357837677, acc: 0.189116373658\n",
      "Epoch: 320, loss: 3.03504633904, acc: 0.19450430572\n",
      "Val: 0.030172413215\n",
      "Epoch: 321, loss: 3.02782082558, acc: 0.189655169845\n",
      "Epoch: 322, loss: 3.01798033714, acc: 0.192887932062\n",
      "Val: 0.0253232754767\n",
      "Epoch: 323, loss: 3.0209543705, acc: 0.202047407627\n",
      "Epoch: 324, loss: 3.00123500824, acc: 0.202047407627\n",
      "Val: 0.030172413215\n",
      "Epoch: 325, loss: 2.97319126129, acc: 0.204741373658\n",
      "Epoch: 326, loss: 2.99905920029, acc: 0.207435339689\n",
      "Val: 0.0231681037694\n",
      "Epoch: 327, loss: 2.96016073227, acc: 0.206357762218\n",
      "Epoch: 328, loss: 2.87953948975, acc: 0.224676728249\n",
      "Val: 0.0253232754767\n",
      "Epoch: 329, loss: 2.89406728745, acc: 0.22575430572\n",
      "Epoch: 330, loss: 2.94377875328, acc: 0.205280169845\n",
      "Val: 0.0296336207539\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "417269760 requested and 297750000 written",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f16616b0c48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.pyc\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_save\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/training_emb_e{}_l{}_val_data'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                     \u001b[0mc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         format.write_array(fid, arr, allow_pickle=allow_pickle,\n\u001b[0;32m--> 511\u001b[0;31m                            pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    512\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mown_fid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/format.pyc\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             for chunk in numpy.nditer(\n",
      "\u001b[0;31mIOError\u001b[0m: 417269760 requested and 297750000 written"
     ]
    }
   ],
   "source": [
    "#memorizing2 \n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train generator ready, time elapsed: 18.3066239357\n",
      "Epoch: 0, loss: 4.07017421722, acc: 0.0457974150777\n",
      "Val: 0.0215517245233\n",
      "Epoch: 1, loss: 3.95395255089, acc: 0.0765086188912\n",
      "Epoch: 2, loss: 3.81776404381, acc: 0.0856681019068\n",
      "Val: 0.0274784490466\n",
      "Epoch: 3, loss: 3.79651331902, acc: 0.0921336188912\n",
      "Epoch: 4, loss: 3.84251570702, acc: 0.0829741358757\n",
      "Val: 0.0183189660311\n",
      "Epoch: 5, loss: 3.71569108963, acc: 0.104525864124\n",
      "Epoch: 6, loss: 3.59529614449, acc: 0.114762932062\n",
      "Val: 0.030172413215\n",
      "Epoch: 7, loss: 3.53423976898, acc: 0.126616373658\n",
      "Epoch: 8, loss: 3.48844432831, acc: 0.141163796186\n",
      "Val: 0.0226293094456\n",
      "Epoch: 9, loss: 3.38816738129, acc: 0.138469830155\n",
      "Epoch: 10, loss: 3.21254682541, acc: 0.154633626342\n",
      "Val: 0.0258620698005\n",
      "Epoch: 11, loss: 3.11616444588, acc: 0.167025864124\n",
      "Epoch: 12, loss: 3.02064108849, acc: 0.193426728249\n",
      "Val: 0.0344827584922\n",
      "Epoch: 13, loss: 2.90528607368, acc: 0.206357762218\n",
      "Epoch: 14, loss: 2.87765598297, acc: 0.219827592373\n",
      "Val: 0.0296336207539\n",
      "Epoch: 15, loss: 2.80477046967, acc: 0.228987067938\n",
      "Epoch: 16, loss: 2.74202394485, acc: 0.253232747316\n",
      "Val: 0.0425646565855\n",
      "Epoch: 17, loss: 2.69982671738, acc: 0.241918101907\n",
      "Epoch: 18, loss: 2.58123517036, acc: 0.273168116808\n",
      "Val: 0.0350215509534\n",
      "Epoch: 19, loss: 2.53297066689, acc: 0.278017252684\n",
      "Epoch: 20, loss: 2.44793272018, acc: 0.306034475565\n",
      "Val: 0.03125\n",
      "Epoch: 21, loss: 2.3318669796, acc: 0.334590524435\n",
      "Epoch: 22, loss: 2.30771636963, acc: 0.331357747316\n",
      "Val: 0.0393318980932\n",
      "Epoch: 23, loss: 2.28303360939, acc: 0.331896543503\n",
      "Epoch: 24, loss: 2.24059510231, acc: 0.348599135876\n",
      "Val: 0.0258620698005\n",
      "Epoch: 25, loss: 2.20841240883, acc: 0.354525864124\n",
      "Epoch: 26, loss: 2.12860512733, acc: 0.379310339689\n",
      "Val: 0.0425646565855\n",
      "Epoch: 27, loss: 2.00472092628, acc: 0.407327592373\n",
      "Epoch: 28, loss: 1.97909629345, acc: 0.425107747316\n",
      "Val: 0.0425646565855\n",
      "Epoch: 29, loss: 1.86121964455, acc: 0.441810339689\n",
      "Epoch: 30, loss: 1.87376749516, acc: 0.439655184746\n",
      "Val: 0.030172413215\n",
      "Epoch: 31, loss: 1.78510594368, acc: 0.473060339689\n",
      "Epoch: 32, loss: 1.65017056465, acc: 0.496228456497\n",
      "Val: 0.036099139601\n",
      "Epoch: 33, loss: 1.6109855175, acc: 0.527478456497\n",
      "Epoch: 34, loss: 1.56975722313, acc: 0.53125\n",
      "Val: 0.0463362075388\n",
      "Epoch: 35, loss: 1.58982515335, acc: 0.529633641243\n",
      "Epoch: 36, loss: 1.43133795261, acc: 0.567349135876\n",
      "Val: 0.0414870679379\n",
      "Epoch: 37, loss: 1.34912371635, acc: 0.60398709774\n",
      "Epoch: 38, loss: 1.26572537422, acc: 0.614224135876\n",
      "Val: 0.0355603434145\n",
      "Epoch: 39, loss: 1.35293376446, acc: 0.59375\n",
      "Epoch: 40, loss: 1.3665112257, acc: 0.589439630508\n",
      "Val: 0.0436422415078\n",
      "Epoch: 41, loss: 1.31247615814, acc: 0.605603456497\n",
      "Epoch: 42, loss: 1.09560120106, acc: 0.672952592373\n",
      "Val: 0.042025860399\n",
      "Epoch: 43, loss: 1.15098154545, acc: 0.640086233616\n",
      "Epoch: 44, loss: 1.01334226131, acc: 0.686422407627\n",
      "Val: 0.046875\n",
      "Epoch: 45, loss: 1.0169159174, acc: 0.689116358757\n",
      "Epoch: 46, loss: 1.06656301022, acc: 0.678340494633\n",
      "Val: 0.0393318980932\n",
      "Epoch: 47, loss: 1.03595876694, acc: 0.68211209774\n",
      "Epoch: 48, loss: 0.919128954411, acc: 0.70851290226\n",
      "Val: 0.0533405169845\n",
      "Epoch: 49, loss: 0.921836495399, acc: 0.703663766384\n",
      "Epoch: 50, loss: 0.870309233665, acc: 0.732219815254\n",
      "Val: 0.051724139601\n",
      "Epoch: 51, loss: 0.866361260414, acc: 0.729525864124\n",
      "Epoch: 52, loss: 0.845541596413, acc: 0.737607777119\n",
      "Val: 0.0511853434145\n",
      "Epoch: 53, loss: 0.749795794487, acc: 0.764547407627\n",
      "Epoch: 54, loss: 0.782228708267, acc: 0.758081912994\n",
      "Val: 0.0479525849223\n",
      "Epoch: 55, loss: 0.766693949699, acc: 0.775323271751\n",
      "Epoch: 56, loss: 0.716109454632, acc: 0.787176728249\n",
      "Val: 0.0528017245233\n",
      "Epoch: 57, loss: 0.646749079227, acc: 0.790948271751\n",
      "Epoch: 58, loss: 0.615631580353, acc: 0.806573271751\n",
      "Val: 0.0668103471398\n",
      "Epoch: 59, loss: 0.598230183125, acc: 0.820581912994\n",
      "Epoch: 60, loss: 0.687720060349, acc: 0.793103456497\n",
      "Val: 0.0398706905544\n",
      "Epoch: 61, loss: 0.717991471291, acc: 0.782866358757\n",
      "Epoch: 62, loss: 0.577518105507, acc: 0.82273709774\n",
      "Val: 0.0533405169845\n",
      "Epoch: 63, loss: 0.574884951115, acc: 0.825969815254\n",
      "Epoch: 64, loss: 0.481783688068, acc: 0.847521543503\n",
      "Val: 0.0339439660311\n",
      "Epoch: 65, loss: 0.518820106983, acc: 0.83836209774\n",
      "Epoch: 66, loss: 0.594518661499, acc: 0.828125\n",
      "Val: 0.0387931019068\n",
      "Epoch: 67, loss: 0.589390695095, acc: 0.813577592373\n",
      "Epoch: 68, loss: 0.522729635239, acc: 0.83351290226\n",
      "Val: 0.0490301735699\n",
      "Epoch: 69, loss: 0.534192681313, acc: 0.840517222881\n",
      "Epoch: 70, loss: 0.507173418999, acc: 0.845905184746\n",
      "Val: 0.0463362075388\n",
      "Epoch: 71, loss: 0.423003703356, acc: 0.863146543503\n",
      "Epoch: 72, loss: 0.445526123047, acc: 0.864224135876\n",
      "Val: 0.0506465509534\n",
      "Epoch: 73, loss: 0.379818946123, acc: 0.879310369492\n",
      "Epoch: 74, loss: 0.356112778187, acc: 0.898168087006\n",
      "Val: 0.0457974150777\n",
      "Epoch: 75, loss: 0.33451539278, acc: 0.892241358757\n",
      "Epoch: 76, loss: 0.406274586916, acc: 0.878232777119\n",
      "Val: 0.0501077584922\n",
      "Epoch: 77, loss: 0.471475601196, acc: 0.854525864124\n",
      "Epoch: 78, loss: 0.467664778233, acc: 0.852909505367\n",
      "Val: 0.0479525849223\n",
      "Epoch: 79, loss: 0.457175374031, acc: 0.859375\n",
      "Epoch: 80, loss: 0.392379701138, acc: 0.873383641243\n",
      "Val: 0.0495689660311\n",
      "Epoch: 81, loss: 0.370447695255, acc: 0.889008641243\n",
      "Epoch: 82, loss: 0.308652877808, acc: 0.899784505367\n",
      "Val: 0.0447198264301\n",
      "Epoch: 83, loss: 0.308018118143, acc: 0.907866358757\n",
      "Epoch: 84, loss: 0.339367687702, acc: 0.897629320621\n",
      "Val: 0.0641163811088\n",
      "Epoch: 85, loss: 0.330917686224, acc: 0.903017222881\n",
      "Epoch: 86, loss: 0.249068111181, acc: 0.920258641243\n",
      "Val: 0.0490301735699\n",
      "Epoch: 87, loss: 0.330307334661, acc: 0.891163766384\n",
      "Epoch: 88, loss: 0.251084655523, acc: 0.923491358757\n",
      "Val: 0.0436422415078\n",
      "Epoch: 89, loss: 0.340830892324, acc: 0.897090494633\n",
      "Epoch: 90, loss: 0.297774910927, acc: 0.907327592373\n",
      "Val: 0.0549568980932\n",
      "Epoch: 91, loss: 0.263196766376, acc: 0.915409505367\n",
      "Epoch: 92, loss: 0.333602279425, acc: 0.894935369492\n",
      "Val: 0.0474137924612\n",
      "Epoch: 93, loss: 0.304879188538, acc: 0.908405184746\n",
      "Epoch: 94, loss: 0.281128108501, acc: 0.912176728249\n",
      "Val: 0.0560344830155\n",
      "Epoch: 95, loss: 0.281470835209, acc: 0.908405184746\n",
      "Epoch: 96, loss: 0.286657094955, acc: 0.912176728249\n",
      "Val: 0.0598060339689\n",
      "Epoch: 97, loss: 0.247035250068, acc: 0.926724135876\n",
      "Epoch: 98, loss: 0.264416158199, acc: 0.924030184746\n",
      "Val: 0.0506465509534\n",
      "Epoch: 99, loss: 0.220123291016, acc: 0.9375\n",
      "Epoch: 100, loss: 0.222924992442, acc: 0.934267222881\n",
      "Val: 0.0538793094456\n",
      "Epoch: 101, loss: 0.226536020637, acc: 0.924030184746\n",
      "Epoch: 102, loss: 0.245136007667, acc: 0.917025864124\n",
      "Val: 0.0614224150777\n",
      "Epoch: 103, loss: 0.261396557093, acc: 0.920258641243\n",
      "Epoch: 104, loss: 0.22484613955, acc: 0.93480604887\n",
      "Val: 0.0641163811088\n",
      "Epoch: 105, loss: 0.201443508267, acc: 0.935883641243\n",
      "Epoch: 106, loss: 0.238793671131, acc: 0.92726290226\n",
      "Val: 0.0436422415078\n",
      "Epoch: 107, loss: 0.210751414299, acc: 0.938038766384\n",
      "Epoch: 108, loss: 0.189715191722, acc: 0.946659505367\n",
      "Val: 0.0565732754767\n",
      "Epoch: 109, loss: 0.165040686727, acc: 0.956896543503\n",
      "Epoch: 110, loss: 0.189123347402, acc: 0.946120679379\n",
      "Val: 0.0436422415078\n",
      "Epoch: 111, loss: 0.243847057223, acc: 0.925646543503\n",
      "Epoch: 112, loss: 0.277260899544, acc: 0.918103456497\n",
      "Val: 0.0549568980932\n",
      "Epoch: 113, loss: 0.19264651835, acc: 0.940732777119\n",
      "Epoch: 114, loss: 0.209295630455, acc: 0.941810369492\n",
      "Val: 0.0528017245233\n",
      "Epoch: 115, loss: 0.195600047708, acc: 0.94019395113\n",
      "Epoch: 116, loss: 0.207768931985, acc: 0.942349135876\n",
      "Val: 0.0463362075388\n",
      "Epoch: 117, loss: 0.198616296053, acc: 0.9375\n",
      "Epoch: 118, loss: 0.142653241754, acc: 0.955280184746\n",
      "Val: 0.0441810339689\n",
      "Epoch: 119, loss: 0.124584451318, acc: 0.959590494633\n",
      "Epoch: 120, loss: 0.151120945811, acc: 0.95581895113\n",
      "Val: 0.0463362075388\n",
      "Epoch: 121, loss: 0.229676946998, acc: 0.932650864124\n",
      "Epoch: 122, loss: 0.164883837104, acc: 0.949353456497\n",
      "Val: 0.0560344830155\n",
      "Epoch: 123, loss: 0.205744743347, acc: 0.936961233616\n",
      "Epoch: 124, loss: 0.174823760986, acc: 0.94288790226\n",
      "Val: 0.0533405169845\n",
      "Epoch: 125, loss: 0.153903186321, acc: 0.949353456497\n",
      "Epoch: 126, loss: 0.136175766587, acc: 0.95581895113\n",
      "Val: 0.0511853434145\n",
      "Epoch: 127, loss: 0.122503437102, acc: 0.962823271751\n",
      "Epoch: 128, loss: 0.108304195106, acc: 0.96875\n",
      "Val: 0.0495689660311\n",
      "Epoch: 129, loss: 0.123079508543, acc: 0.963900864124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130, loss: 0.0969701111317, acc: 0.973599135876\n",
      "Val: 0.0592672415078\n",
      "Epoch: 131, loss: 0.109592102468, acc: 0.971982777119\n",
      "Epoch: 132, loss: 0.121964998543, acc: 0.963900864124\n",
      "Val: 0.0474137924612\n",
      "Epoch: 133, loss: 0.121895372868, acc: 0.96605604887\n",
      "Epoch: 134, loss: 0.104574009776, acc: 0.962823271751\n",
      "Val: 0.0592672415078\n",
      "Epoch: 135, loss: 0.141100764275, acc: 0.954202592373\n",
      "Epoch: 136, loss: 0.12562571466, acc: 0.95851290226\n",
      "Val: 0.0598060339689\n",
      "Epoch: 137, loss: 0.132362395525, acc: 0.957974135876\n",
      "Epoch: 138, loss: 0.126098334789, acc: 0.964439630508\n",
      "Val: 0.0431034490466\n",
      "Epoch: 139, loss: 0.138479202986, acc: 0.95581895113\n",
      "Epoch: 140, loss: 0.119030423462, acc: 0.96336209774\n",
      "Val: 0.0554956905544\n",
      "Epoch: 141, loss: 0.135146275163, acc: 0.959051728249\n",
      "Epoch: 142, loss: 0.0895030200481, acc: 0.973060369492\n",
      "Val: 0.0587284490466\n",
      "Epoch: 143, loss: 0.0932655110955, acc: 0.969288766384\n",
      "Epoch: 144, loss: 0.11921646446, acc: 0.962284505367\n",
      "Val: 0.0592672415078\n",
      "Epoch: 145, loss: 0.12206889689, acc: 0.960668087006\n",
      "Epoch: 146, loss: 0.119007736444, acc: 0.96605604887\n",
      "Val: 0.0651939660311\n",
      "Epoch: 147, loss: 0.141304343939, acc: 0.956896543503\n",
      "Epoch: 148, loss: 0.129709124565, acc: 0.961745679379\n",
      "Val: 0.0625\n",
      "Epoch: 149, loss: 0.136371657252, acc: 0.959051728249\n",
      "Epoch: 150, loss: 0.129637226462, acc: 0.962823271751\n",
      "Val: 0.0662715509534\n",
      "Epoch: 151, loss: 0.0752455070615, acc: 0.983836233616\n",
      "Epoch: 152, loss: 0.0791440233588, acc: 0.977370679379\n",
      "Val: 0.0452586188912\n",
      "Epoch: 153, loss: 0.0994886755943, acc: 0.97413790226\n",
      "Epoch: 154, loss: 0.0937856137753, acc: 0.97144395113\n",
      "Val: 0.0554956905544\n",
      "Epoch: 155, loss: 0.132647335529, acc: 0.960129320621\n",
      "Epoch: 156, loss: 0.110906362534, acc: 0.972521543503\n",
      "Val: 0.0565732754767\n",
      "Epoch: 157, loss: 0.0940156131983, acc: 0.972521543503\n",
      "Epoch: 158, loss: 0.0888581201434, acc: 0.977909505367\n",
      "Val: 0.0673491358757\n",
      "Epoch: 159, loss: 0.0965629294515, acc: 0.970905184746\n",
      "Epoch: 160, loss: 0.0953541100025, acc: 0.971982777119\n",
      "Val: 0.0673491358757\n",
      "Epoch: 161, loss: 0.10531912744, acc: 0.967672407627\n",
      "Epoch: 162, loss: 0.112497024238, acc: 0.969288766384\n",
      "Val: 0.0474137924612\n",
      "Epoch: 163, loss: 0.127085655928, acc: 0.96336209774\n",
      "Epoch: 164, loss: 0.123708680272, acc: 0.960668087006\n",
      "Val: 0.0587284490466\n",
      "Epoch: 165, loss: 0.11241863668, acc: 0.964978456497\n",
      "Epoch: 166, loss: 0.108428940177, acc: 0.967133641243\n",
      "Val: 0.057650860399\n",
      "Epoch: 167, loss: 0.0863302350044, acc: 0.971982777119\n",
      "Epoch: 168, loss: 0.0904479026794, acc: 0.974676728249\n",
      "Val: 0.0538793094456\n",
      "Epoch: 169, loss: 0.085276812315, acc: 0.97144395113\n",
      "Epoch: 170, loss: 0.0988183021545, acc: 0.970905184746\n",
      "Val: 0.0484913811088\n",
      "Epoch: 171, loss: 0.097703114152, acc: 0.966594815254\n",
      "Epoch: 172, loss: 0.0540733747184, acc: 0.983297407627\n",
      "Val: 0.0587284490466\n",
      "Epoch: 173, loss: 0.0813890248537, acc: 0.97413790226\n",
      "Epoch: 174, loss: 0.11175262183, acc: 0.96875\n",
      "Val: 0.0598060339689\n",
      "Epoch: 175, loss: 0.116058260202, acc: 0.96605604887\n",
      "Epoch: 176, loss: 0.0887575000525, acc: 0.975215494633\n",
      "Val: 0.0571120679379\n",
      "Epoch: 177, loss: 0.0751515179873, acc: 0.979525864124\n",
      "Epoch: 178, loss: 0.114085778594, acc: 0.960668087006\n",
      "Val: 0.0501077584922\n",
      "Epoch: 179, loss: 0.139810174704, acc: 0.959051728249\n",
      "Epoch: 180, loss: 0.123228535056, acc: 0.970905184746\n",
      "Val: 0.0743534490466\n",
      "Epoch: 181, loss: 0.0780482962728, acc: 0.980603456497\n",
      "Epoch: 182, loss: 0.0715341120958, acc: 0.982219815254\n",
      "Val: 0.0614224150777\n",
      "Epoch: 183, loss: 0.0611092932522, acc: 0.983297407627\n",
      "Epoch: 184, loss: 0.0855258479714, acc: 0.970905184746\n",
      "Val: 0.0571120679379\n",
      "Epoch: 185, loss: 0.0654098168015, acc: 0.976831912994\n",
      "Epoch: 186, loss: 0.0792872086167, acc: 0.973060369492\n",
      "Val: 0.0560344830155\n",
      "Epoch: 187, loss: 0.0722514763474, acc: 0.97413790226\n",
      "Epoch: 188, loss: 0.0642502829432, acc: 0.981142222881\n",
      "Val: 0.0522629320621\n",
      "Epoch: 189, loss: 0.0878202393651, acc: 0.976293087006\n",
      "Epoch: 190, loss: 0.0741269290447, acc: 0.977909505367\n",
      "Val: 0.0603448264301\n",
      "Epoch: 191, loss: 0.0944351628423, acc: 0.972521543503\n",
      "Epoch: 192, loss: 0.0594280324876, acc: 0.97898709774\n",
      "Val: 0.0668103471398\n",
      "Epoch: 193, loss: 0.0565313659608, acc: 0.97898709774\n",
      "Epoch: 194, loss: 0.0781955644488, acc: 0.977370679379\n",
      "Val: 0.0770474150777\n",
      "Epoch: 195, loss: 0.0669212788343, acc: 0.979525864124\n",
      "Epoch: 196, loss: 0.065916210413, acc: 0.980064630508\n",
      "Val: 0.0571120679379\n",
      "Epoch: 197, loss: 0.0812461152673, acc: 0.975754320621\n",
      "Epoch: 198, loss: 0.0582545176148, acc: 0.983836233616\n",
      "Val: 0.0554956905544\n",
      "Epoch: 199, loss: 0.0391939356923, acc: 0.987607777119\n",
      "Epoch: 200, loss: 0.043255712837, acc: 0.985991358757\n",
      "Val: 0.0657327622175\n",
      "Epoch: 201, loss: 0.0473870486021, acc: 0.988685369492\n",
      "Epoch: 202, loss: 0.0794229581952, acc: 0.981142222881\n",
      "Val: 0.0651939660311\n",
      "Epoch: 203, loss: 0.0677309930325, acc: 0.978448271751\n",
      "Epoch: 204, loss: 0.088600538671, acc: 0.974676728249\n",
      "Val: 0.0560344830155\n",
      "Epoch: 205, loss: 0.0724213421345, acc: 0.979525864124\n",
      "Epoch: 206, loss: 0.0528499409556, acc: 0.984375\n",
      "Val: 0.0678879320621\n",
      "Epoch: 207, loss: 0.0417420640588, acc: 0.988146543503\n",
      "Epoch: 208, loss: 0.0300139077008, acc: 0.992456912994\n",
      "Val: 0.0549568980932\n",
      "Epoch: 209, loss: 0.0384692065418, acc: 0.986530184746\n",
      "Epoch: 210, loss: 0.0434870757163, acc: 0.987607777119\n",
      "Val: 0.0592672415078\n",
      "Epoch: 211, loss: 0.046893581748, acc: 0.985991358757\n",
      "Epoch: 212, loss: 0.0459605157375, acc: 0.985991358757\n",
      "Val: 0.057650860399\n",
      "Epoch: 213, loss: 0.0361486449838, acc: 0.987607777119\n",
      "Epoch: 214, loss: 0.0323396958411, acc: 0.991379320621\n",
      "Val: 0.0614224150777\n",
      "Epoch: 215, loss: 0.0310197360814, acc: 0.992456912994\n",
      "Epoch: 216, loss: 0.0309802517295, acc: 0.991918087006\n",
      "Val: 0.0678879320621\n",
      "Epoch: 217, loss: 0.0270587354898, acc: 0.991379320621\n",
      "Epoch: 218, loss: 0.0401712730527, acc: 0.98976290226\n",
      "Val: 0.0700431019068\n",
      "Epoch: 219, loss: 0.0265363380313, acc: 0.991379320621\n",
      "Epoch: 220, loss: 0.0515496991575, acc: 0.982758641243\n",
      "Val: 0.0614224150777\n",
      "Epoch: 221, loss: 0.0520309358835, acc: 0.985991358757\n",
      "Epoch: 222, loss: 0.0518503934145, acc: 0.98706895113\n",
      "Val: 0.0646551698446\n",
      "Epoch: 223, loss: 0.0403904467821, acc: 0.988146543503\n",
      "Epoch: 224, loss: 0.072794996202, acc: 0.981142222881\n",
      "Val: 0.0538793094456\n",
      "Epoch: 225, loss: 0.109396159649, acc: 0.971982777119\n",
      "Epoch: 226, loss: 0.0960158258677, acc: 0.971982777119\n",
      "Val: 0.0528017245233\n",
      "Epoch: 227, loss: 0.0912760198116, acc: 0.971982777119\n",
      "Epoch: 228, loss: 0.0585288777947, acc: 0.982219815254\n",
      "Val: 0.0625\n",
      "Epoch: 229, loss: 0.0557722598314, acc: 0.984913766384\n",
      "Epoch: 230, loss: 0.0475960671902, acc: 0.987607777119\n",
      "Val: 0.0646551698446\n",
      "Epoch: 231, loss: 0.0432718470693, acc: 0.989224135876\n",
      "Epoch: 232, loss: 0.0488498285413, acc: 0.985991358757\n",
      "Val: 0.0657327622175\n",
      "Epoch: 233, loss: 0.0550021156669, acc: 0.982758641243\n",
      "Epoch: 234, loss: 0.0391991436481, acc: 0.98976290226\n",
      "Val: 0.0641163811088\n",
      "Epoch: 235, loss: 0.0303934775293, acc: 0.992456912994\n",
      "Epoch: 236, loss: 0.0251583326608, acc: 0.992995679379\n",
      "Val: 0.0716594830155\n",
      "Epoch: 237, loss: 0.0438356734812, acc: 0.987607777119\n",
      "Epoch: 238, loss: 0.0424463264644, acc: 0.98706895113\n",
      "Val: 0.0678879320621\n",
      "Epoch: 239, loss: 0.0505354069173, acc: 0.985452592373\n",
      "Epoch: 240, loss: 0.0587046220899, acc: 0.984913766384\n",
      "Val: 0.0608836188912\n",
      "Epoch: 241, loss: 0.0634166821837, acc: 0.982758641243\n",
      "Epoch: 242, loss: 0.0342309400439, acc: 0.990840494633\n",
      "Val: 0.0538793094456\n",
      "Epoch: 243, loss: 0.0418619662523, acc: 0.991918087006\n",
      "Epoch: 244, loss: 0.042697712779, acc: 0.986530184746\n",
      "Val: 0.0484913811088\n",
      "Epoch: 245, loss: 0.041352853179, acc: 0.990301728249\n",
      "Epoch: 246, loss: 0.0300389267504, acc: 0.993534505367\n",
      "Val: 0.0657327622175\n",
      "Epoch: 247, loss: 0.0321959406137, acc: 0.987607777119\n",
      "Epoch: 248, loss: 0.0473701059818, acc: 0.988146543503\n",
      "Val: 0.051724139601\n",
      "Epoch: 249, loss: 0.0274747051299, acc: 0.991379320621\n",
      "Epoch: 250, loss: 0.0291898380965, acc: 0.990840494633\n",
      "Val: 0.0625\n",
      "Epoch: 251, loss: 0.0295310802758, acc: 0.991918087006\n",
      "Epoch: 252, loss: 0.0456996187568, acc: 0.990301728249\n",
      "Val: 0.0792025849223\n",
      "Epoch: 253, loss: 0.0382834784687, acc: 0.991379320621\n",
      "Epoch: 254, loss: 0.0246912278235, acc: 0.995150864124\n",
      "Val: 0.0770474150777\n",
      "Epoch: 255, loss: 0.0317818038166, acc: 0.991918087006\n",
      "Epoch: 256, loss: 0.0271492749453, acc: 0.992995679379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: 0.0662715509534\n",
      "Epoch: 257, loss: 0.0350261367857, acc: 0.990301728249\n",
      "Epoch: 258, loss: 0.0421565249562, acc: 0.989224135876\n",
      "Val: 0.0463362075388\n",
      "Epoch: 259, loss: 0.0378952696919, acc: 0.98976290226\n",
      "Epoch: 260, loss: 0.0258518010378, acc: 0.992456912994\n",
      "Val: 0.0657327622175\n",
      "Epoch: 261, loss: 0.0244504082948, acc: 0.991918087006\n",
      "Epoch: 262, loss: 0.0289608705789, acc: 0.992995679379\n",
      "Val: 0.0824353471398\n",
      "Epoch: 263, loss: 0.0347021892667, acc: 0.990301728249\n",
      "Epoch: 264, loss: 0.0387923084199, acc: 0.988685369492\n",
      "Val: 0.0673491358757\n",
      "Epoch: 265, loss: 0.0345307178795, acc: 0.993534505367\n",
      "Epoch: 266, loss: 0.0238083545119, acc: 0.992995679379\n",
      "Val: 0.0727370679379\n",
      "Epoch: 267, loss: 0.0198188293725, acc: 0.995689630508\n",
      "Epoch: 268, loss: 0.022757243365, acc: 0.992456912994\n",
      "Val: 0.0754310339689\n",
      "Epoch: 269, loss: 0.0391412526369, acc: 0.990301728249\n",
      "Epoch: 270, loss: 0.032679785043, acc: 0.992456912994\n",
      "Val: 0.0802801698446\n",
      "Epoch: 271, loss: 0.0382509753108, acc: 0.990301728249\n",
      "Epoch: 272, loss: 0.0366667062044, acc: 0.989224135876\n",
      "Val: 0.0738146528602\n",
      "Epoch: 273, loss: 0.0428837649524, acc: 0.98976290226\n",
      "Epoch: 274, loss: 0.0371947400272, acc: 0.988685369492\n",
      "Val: 0.0587284490466\n",
      "Epoch: 275, loss: 0.0329145118594, acc: 0.990840494633\n",
      "Epoch: 276, loss: 0.0321572050452, acc: 0.990840494633\n",
      "Val: 0.0668103471398\n",
      "Epoch: 277, loss: 0.0215328168124, acc: 0.994073271751\n",
      "Epoch: 278, loss: 0.0152851343155, acc: 0.995689630508\n",
      "Val: 0.0554956905544\n",
      "Epoch: 279, loss: 0.0254320241511, acc: 0.992456912994\n",
      "Epoch: 280, loss: 0.0288577675819, acc: 0.990840494633\n",
      "Val: 0.0646551698446\n",
      "Epoch: 281, loss: 0.0363926962018, acc: 0.990840494633\n",
      "Epoch: 282, loss: 0.0357768423855, acc: 0.988146543503\n",
      "Val: 0.0625\n",
      "Epoch: 283, loss: 0.0260575450957, acc: 0.991918087006\n",
      "Epoch: 284, loss: 0.0257232040167, acc: 0.992456912994\n",
      "Val: 0.0695043131709\n",
      "Epoch: 285, loss: 0.0343588367105, acc: 0.986530184746\n",
      "Epoch: 286, loss: 0.0357505045831, acc: 0.991379320621\n",
      "Val: 0.0705818980932\n",
      "Epoch: 287, loss: 0.0476142987609, acc: 0.985991358757\n",
      "Epoch: 288, loss: 0.0416756384075, acc: 0.991918087006\n",
      "Val: 0.057650860399\n",
      "Epoch: 289, loss: 0.0386486873031, acc: 0.985452592373\n",
      "Epoch: 290, loss: 0.0382330492139, acc: 0.989224135876\n",
      "Val: 0.0511853434145\n",
      "Epoch: 291, loss: 0.0333059951663, acc: 0.98976290226\n",
      "Epoch: 292, loss: 0.0416085012257, acc: 0.98976290226\n",
      "Val: 0.051724139601\n",
      "Epoch: 293, loss: 0.0440266504884, acc: 0.990301728249\n",
      "Epoch: 294, loss: 0.0411547087133, acc: 0.988146543503\n",
      "Val: 0.0565732754767\n",
      "Epoch: 295, loss: 0.0671374052763, acc: 0.982219815254\n",
      "Epoch: 296, loss: 0.0391126647592, acc: 0.989224135876\n",
      "Val: 0.0678879320621\n",
      "Epoch: 297, loss: 0.0351215340197, acc: 0.991379320621\n",
      "Epoch: 298, loss: 0.06622261554, acc: 0.977909505367\n",
      "Val: 0.0641163811088\n",
      "Epoch: 299, loss: 0.0549797303975, acc: 0.983836233616\n",
      "Epoch: 300, loss: 0.0522965528071, acc: 0.984375\n",
      "Val: 0.0646551698446\n",
      "Epoch: 301, loss: 0.0596515499055, acc: 0.983297407627\n",
      "Epoch: 302, loss: 0.0435825437307, acc: 0.984913766384\n",
      "Val: 0.0625\n",
      "Epoch: 303, loss: 0.0313146784902, acc: 0.990840494633\n",
      "Epoch: 304, loss: 0.034652736038, acc: 0.991379320621\n",
      "Val: 0.0549568980932\n",
      "Epoch: 305, loss: 0.0322187505662, acc: 0.98976290226\n",
      "Epoch: 306, loss: 0.0377917252481, acc: 0.98976290226\n",
      "Val: 0.0673491358757\n",
      "Epoch: 307, loss: 0.0420599840581, acc: 0.985452592373\n",
      "Epoch: 308, loss: 0.0389581173658, acc: 0.989224135876\n",
      "Val: 0.0608836188912\n",
      "Epoch: 309, loss: 0.030921312049, acc: 0.98976290226\n",
      "Epoch: 310, loss: 0.0214708391577, acc: 0.99461209774\n",
      "Val: 0.0657327622175\n",
      "Epoch: 311, loss: 0.0107560576871, acc: 0.997844815254\n",
      "Epoch: 312, loss: 0.026253093034, acc: 0.99461209774\n",
      "Val: 0.0592672415078\n",
      "Epoch: 313, loss: 0.0392851345241, acc: 0.988146543503\n",
      "Epoch: 314, loss: 0.0283207371831, acc: 0.992995679379\n",
      "Val: 0.0711206868291\n",
      "Epoch: 315, loss: 0.0212169829756, acc: 0.996228456497\n",
      "Epoch: 316, loss: 0.0200864616781, acc: 0.99461209774\n",
      "Val: 0.0614224150777\n",
      "Epoch: 317, loss: 0.0198710728437, acc: 0.99461209774\n",
      "Epoch: 318, loss: 0.0301187522709, acc: 0.990301728249\n",
      "Val: 0.0695043131709\n",
      "Epoch: 319, loss: 0.030960265547, acc: 0.990840494633\n",
      "Epoch: 320, loss: 0.0118502750993, acc: 0.99730604887\n",
      "Val: 0.072198279202\n",
      "Epoch: 321, loss: 0.00834455806762, acc: 0.99730604887\n",
      "Epoch: 322, loss: 0.013059290126, acc: 0.99730604887\n",
      "Val: 0.0711206868291\n",
      "Epoch: 323, loss: 0.0120532149449, acc: 0.996767222881\n",
      "Epoch: 324, loss: 0.0261785071343, acc: 0.992995679379\n",
      "Val: 0.0716594830155\n",
      "Epoch: 325, loss: 0.0180534236133, acc: 0.996228456497\n",
      "Epoch: 326, loss: 0.0171742867678, acc: 0.993534505367\n",
      "Val: 0.0689655169845\n",
      "Epoch: 327, loss: 0.0231397375464, acc: 0.99461209774\n",
      "Epoch: 328, loss: 0.020721161738, acc: 0.99730604887\n",
      "Val: 0.0625\n",
      "Epoch: 329, loss: 0.0152011997998, acc: 0.99730604887\n",
      "Epoch: 330, loss: 0.0126673616469, acc: 0.997844815254\n",
      "Val: 0.0673491358757\n",
      "Epoch: 331, loss: 0.0126679698005, acc: 0.996228456497\n",
      "Epoch: 332, loss: 0.00982912629843, acc: 0.997844815254\n",
      "Val: 0.0657327622175\n",
      "Epoch: 333, loss: 0.0144257135689, acc: 0.996767222881\n",
      "Epoch: 334, loss: 0.0166425462812, acc: 0.994073271751\n",
      "Val: 0.0651939660311\n",
      "Epoch: 335, loss: 0.0199840553105, acc: 0.99461209774\n",
      "Epoch: 336, loss: 0.0244293436408, acc: 0.995689630508\n",
      "Val: 0.0668103471398\n",
      "Epoch: 337, loss: 0.0287239290774, acc: 0.99461209774\n",
      "Epoch: 338, loss: 0.0175364855677, acc: 0.996228456497\n",
      "Val: 0.068426720798\n",
      "Epoch: 339, loss: 0.0291678179055, acc: 0.98976290226\n",
      "Epoch: 340, loss: 0.0294075217098, acc: 0.991918087006\n",
      "Val: 0.0700431019068\n",
      "Epoch: 341, loss: 0.0290092006326, acc: 0.990840494633\n",
      "Epoch: 342, loss: 0.0178177822381, acc: 0.996767222881\n",
      "Val: 0.0689655169845\n",
      "Epoch: 343, loss: 0.00725577678531, acc: 0.997844815254\n",
      "Epoch: 344, loss: 0.013507434167, acc: 0.995689630508\n",
      "Val: 0.072198279202\n",
      "Epoch: 345, loss: 0.0116251669824, acc: 0.996228456497\n",
      "Epoch: 346, loss: 0.0239420738071, acc: 0.992995679379\n",
      "Val: 0.0716594830155\n",
      "Epoch: 347, loss: 0.024455036968, acc: 0.993534505367\n",
      "Epoch: 348, loss: 0.0326217971742, acc: 0.990840494633\n",
      "Val: 0.0732758641243\n",
      "Epoch: 349, loss: 0.0286904461682, acc: 0.992995679379\n",
      "Epoch: 350, loss: 0.0251913778484, acc: 0.994073271751\n",
      "Val: 0.0651939660311\n",
      "Epoch: 351, loss: 0.0194733701646, acc: 0.996228456497\n",
      "Epoch: 352, loss: 0.0140932425857, acc: 0.996228456497\n",
      "Val: 0.0695043131709\n",
      "Epoch: 353, loss: 0.0118349613622, acc: 0.995689630508\n",
      "Epoch: 354, loss: 0.0104628214613, acc: 0.997844815254\n",
      "Val: 0.0797413811088\n",
      "Epoch: 355, loss: 0.0295780189335, acc: 0.993534505367\n",
      "Epoch: 356, loss: 0.037162207067, acc: 0.992995679379\n",
      "Val: 0.0705818980932\n",
      "Epoch: 357, loss: 0.0271330066025, acc: 0.995150864124\n",
      "Epoch: 358, loss: 0.0152396028861, acc: 0.996228456497\n",
      "Val: 0.0689655169845\n",
      "Epoch: 359, loss: 0.0263862796128, acc: 0.991918087006\n",
      "Epoch: 360, loss: 0.0259479768574, acc: 0.993534505367\n",
      "Val: 0.0673491358757\n",
      "Epoch: 361, loss: 0.0348283499479, acc: 0.991918087006\n",
      "Epoch: 362, loss: 0.0286051686853, acc: 0.989224135876\n",
      "Val: 0.0565732754767\n",
      "Epoch: 363, loss: 0.0178440194577, acc: 0.996767222881\n",
      "Epoch: 364, loss: 0.0166223179549, acc: 0.996767222881\n",
      "Val: 0.0571120679379\n",
      "Epoch: 365, loss: 0.0365389734507, acc: 0.992456912994\n",
      "Epoch: 366, loss: 0.0333761014044, acc: 0.991918087006\n",
      "Val: 0.0565732754767\n",
      "Epoch: 367, loss: 0.0192216094583, acc: 0.995689630508\n",
      "Epoch: 368, loss: 0.0136353876442, acc: 0.99730604887\n",
      "Val: 0.0668103471398\n",
      "Epoch: 369, loss: 0.015393092297, acc: 0.996228456497\n",
      "Epoch: 370, loss: 0.0193921178579, acc: 0.996228456497\n",
      "Val: 0.0651939660311\n",
      "Epoch: 371, loss: 0.0268106777221, acc: 0.995689630508\n",
      "Epoch: 372, loss: 0.0134550174698, acc: 0.995150864124\n",
      "Val: 0.0829741358757\n",
      "Epoch: 373, loss: 0.00953256431967, acc: 0.996767222881\n",
      "Epoch: 374, loss: 0.010652798228, acc: 0.996228456497\n",
      "Val: 0.0732758641243\n",
      "Epoch: 375, loss: 0.0101215075701, acc: 0.996767222881\n",
      "Epoch: 376, loss: 0.00610190769657, acc: 0.999461233616\n",
      "Val: 0.0711206868291\n",
      "Epoch: 377, loss: 0.0103078447282, acc: 0.997844815254\n",
      "Epoch: 378, loss: 0.0212769676, acc: 0.994073271751\n",
      "Val: 0.0651939660311\n",
      "Epoch: 379, loss: 0.0154849225655, acc: 0.996228456497\n",
      "Epoch: 380, loss: 0.0115704266354, acc: 0.996228456497\n",
      "Val: 0.0668103471398\n",
      "Epoch: 381, loss: 0.020150391385, acc: 0.99461209774\n",
      "Epoch: 382, loss: 0.0277378745377, acc: 0.991918087006\n",
      "Val: 0.072198279202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 383, loss: 0.0357750728726, acc: 0.990840494633\n",
      "Epoch: 384, loss: 0.0124042537063, acc: 0.995150864124\n",
      "Val: 0.0711206868291\n",
      "Epoch: 385, loss: 0.0351561941206, acc: 0.991379320621\n",
      "Epoch: 386, loss: 0.0455415397882, acc: 0.989224135876\n",
      "Val: 0.0695043131709\n",
      "Epoch: 387, loss: 0.0385570786893, acc: 0.991379320621\n",
      "Epoch: 388, loss: 0.0308506041765, acc: 0.990840494633\n",
      "Val: 0.0603448264301\n",
      "Epoch: 389, loss: 0.0203839335591, acc: 0.995150864124\n",
      "Epoch: 390, loss: 0.0148763190955, acc: 0.996228456497\n",
      "Val: 0.0727370679379\n",
      "Epoch: 391, loss: 0.0283623654395, acc: 0.99461209774\n",
      "Epoch: 392, loss: 0.0137972962111, acc: 0.994073271751\n",
      "Val: 0.0657327622175\n",
      "Epoch: 393, loss: 0.0130704753101, acc: 0.99730604887\n",
      "Epoch: 394, loss: 0.0214960798621, acc: 0.995150864124\n",
      "Val: 0.0862068980932\n",
      "Epoch: 395, loss: 0.0182736068964, acc: 0.996767222881\n",
      "Epoch: 396, loss: 0.0153674380854, acc: 0.99461209774\n",
      "Val: 0.0759698301554\n",
      "Epoch: 397, loss: 0.0281450636685, acc: 0.992995679379\n",
      "Epoch: 398, loss: 0.0149420965463, acc: 0.995689630508\n",
      "Val: 0.0689655169845\n",
      "Epoch: 399, loss: 0.0165766235441, acc: 0.995689630508\n",
      "Epoch: 400, loss: 0.0128044495359, acc: 0.99461209774\n",
      "Val: 0.0732758641243\n",
      "Epoch: 401, loss: 0.015119000338, acc: 0.996767222881\n",
      "Epoch: 402, loss: 0.0161852166057, acc: 0.996767222881\n",
      "Val: 0.072198279202\n",
      "Epoch: 403, loss: 0.0118748480454, acc: 0.99730604887\n",
      "Epoch: 404, loss: 0.0178516358137, acc: 0.993534505367\n",
      "Val: 0.0571120679379\n",
      "Epoch: 405, loss: 0.0120289810002, acc: 0.99730604887\n",
      "Epoch: 406, loss: 0.0168210454285, acc: 0.995689630508\n",
      "Val: 0.0587284490466\n",
      "Epoch: 407, loss: 0.0194365177304, acc: 0.993534505367\n",
      "Epoch: 408, loss: 0.022052930668, acc: 0.993534505367\n",
      "Val: 0.0727370679379\n",
      "Epoch: 409, loss: 0.0750617906451, acc: 0.980064630508\n",
      "Epoch: 410, loss: 0.0350890420377, acc: 0.988146543503\n",
      "Val: 0.0743534490466\n",
      "Epoch: 411, loss: 0.0235446039587, acc: 0.99461209774\n",
      "Epoch: 412, loss: 0.0195044726133, acc: 0.996228456497\n",
      "Val: 0.0748922377825\n",
      "Epoch: 413, loss: 0.0242254808545, acc: 0.99461209774\n",
      "Epoch: 414, loss: 0.0239446908236, acc: 0.995689630508\n",
      "Val: 0.0748922377825\n",
      "Epoch: 415, loss: 0.028281621635, acc: 0.993534505367\n",
      "Epoch: 416, loss: 0.0169165749103, acc: 0.996767222881\n",
      "Val: 0.0614224150777\n",
      "Epoch: 417, loss: 0.0162532776594, acc: 0.995150864124\n",
      "Epoch: 418, loss: 0.0118472781032, acc: 0.996767222881\n",
      "Val: 0.0625\n",
      "Epoch: 419, loss: 0.017062170431, acc: 0.99461209774\n",
      "Epoch: 420, loss: 0.0252076797187, acc: 0.99461209774\n",
      "Val: 0.0619612075388\n",
      "Epoch: 421, loss: 0.0160625129938, acc: 0.996228456497\n",
      "Epoch: 422, loss: 0.00885069277138, acc: 0.996228456497\n",
      "Val: 0.0630387961864\n",
      "Epoch: 423, loss: 0.012039763853, acc: 0.99730604887\n",
      "Epoch: 424, loss: 0.0208135992289, acc: 0.995150864124\n",
      "Val: 0.068426720798\n",
      "Epoch: 425, loss: 0.0193118564785, acc: 0.994073271751\n",
      "Epoch: 426, loss: 0.0169370472431, acc: 0.99461209774\n",
      "Val: 0.0662715509534\n",
      "Epoch: 427, loss: 0.0242653097957, acc: 0.995150864124\n",
      "Epoch: 428, loss: 0.0125635759905, acc: 0.996767222881\n",
      "Val: 0.0770474150777\n",
      "Epoch: 429, loss: 0.0108662033454, acc: 0.997844815254\n",
      "Epoch: 430, loss: 0.00985069014132, acc: 0.998383641243\n",
      "Val: 0.072198279202\n",
      "Epoch: 431, loss: 0.0126813240349, acc: 0.996767222881\n",
      "Epoch: 432, loss: 0.0197092853487, acc: 0.995150864124\n",
      "Val: 0.0700431019068\n",
      "Epoch: 433, loss: 0.0145948557183, acc: 0.995150864124\n",
      "Epoch: 434, loss: 0.0157209895551, acc: 0.995150864124\n",
      "Val: 0.0625\n",
      "Epoch: 435, loss: 0.0113290436566, acc: 0.995150864124\n",
      "Epoch: 436, loss: 0.00966395344585, acc: 0.997844815254\n",
      "Val: 0.0835129320621\n",
      "Epoch: 437, loss: 0.0137796504423, acc: 0.995150864124\n",
      "Epoch: 438, loss: 0.00836662016809, acc: 0.997844815254\n",
      "Val: 0.0700431019068\n",
      "Epoch: 439, loss: 0.0109724011272, acc: 0.998383641243\n",
      "Epoch: 440, loss: 0.00964001193643, acc: 0.996767222881\n",
      "Val: 0.0808189660311\n",
      "Epoch: 441, loss: 0.0075250165537, acc: 0.99730604887\n",
      "Epoch: 442, loss: 0.00539079727605, acc: 0.999461233616\n",
      "Val: 0.0851293131709\n",
      "Epoch: 443, loss: 0.00399328675121, acc: 0.999461233616\n",
      "Epoch: 444, loss: 0.00491066742688, acc: 0.998922407627\n",
      "Val: 0.0872844830155\n",
      "Epoch: 445, loss: 0.00670640310273, acc: 0.996767222881\n",
      "Epoch: 446, loss: 0.00398862641305, acc: 0.998922407627\n",
      "Val: 0.0765086188912\n",
      "Epoch: 447, loss: 0.006793434266, acc: 0.99730604887\n",
      "Epoch: 448, loss: 0.00555175542831, acc: 0.998922407627\n",
      "Val: 0.0792025849223\n",
      "Epoch: 449, loss: 0.00258508720435, acc: 1.0\n",
      "Epoch: 450, loss: 0.00739404838532, acc: 0.99730604887\n",
      "Val: 0.0738146528602\n",
      "Epoch: 451, loss: 0.00898509379476, acc: 0.999461233616\n",
      "Epoch: 452, loss: 0.00891891028732, acc: 0.997844815254\n",
      "Val: 0.0738146528602\n",
      "Epoch: 453, loss: 0.00447823759168, acc: 0.998383641243\n",
      "Epoch: 454, loss: 0.00849068257958, acc: 0.997844815254\n",
      "Val: 0.0765086188912\n",
      "Epoch: 455, loss: 0.00441425899044, acc: 0.998922407627\n",
      "Epoch: 456, loss: 0.0115300724283, acc: 0.996228456497\n",
      "Val: 0.072198279202\n",
      "Epoch: 457, loss: 0.0130623308942, acc: 0.997844815254\n",
      "Epoch: 458, loss: 0.0139388199896, acc: 0.99730604887\n",
      "Val: 0.0678879320621\n",
      "Epoch: 459, loss: 0.0122945290059, acc: 0.996767222881\n",
      "Epoch: 460, loss: 0.0139998979867, acc: 0.996767222881\n",
      "Val: 0.0544181019068\n",
      "Epoch: 461, loss: 0.0171964168549, acc: 0.99461209774\n",
      "Epoch: 462, loss: 0.0274889282882, acc: 0.993534505367\n",
      "Val: 0.0797413811088\n",
      "Epoch: 463, loss: 0.0105970567092, acc: 0.996767222881\n",
      "Epoch: 464, loss: 0.0152775356546, acc: 0.996767222881\n",
      "Val: 0.0727370679379\n",
      "Epoch: 465, loss: 0.00583863584325, acc: 0.998922407627\n",
      "Epoch: 466, loss: 0.0127687985078, acc: 0.996767222881\n",
      "Val: 0.0635775849223\n",
      "Epoch: 467, loss: 0.00813922099769, acc: 0.998383641243\n",
      "Epoch: 468, loss: 0.020219611004, acc: 0.994073271751\n",
      "Val: 0.0743534490466\n",
      "Epoch: 469, loss: 0.00918296258897, acc: 0.998383641243\n",
      "Epoch: 470, loss: 0.0137147400528, acc: 0.996228456497\n",
      "Val: 0.0630387961864\n",
      "Epoch: 471, loss: 0.0153175368905, acc: 0.997844815254\n",
      "Epoch: 472, loss: 0.024602279067, acc: 0.99461209774\n",
      "Val: 0.0603448264301\n",
      "Epoch: 473, loss: 0.0107413325459, acc: 0.997844815254\n",
      "Epoch: 474, loss: 0.00675668101758, acc: 0.998922407627\n",
      "Val: 0.068426720798\n",
      "Epoch: 475, loss: 0.00443471455947, acc: 0.999461233616\n",
      "Epoch: 476, loss: 0.00840200297534, acc: 0.99730604887\n",
      "Val: 0.0608836188912\n",
      "Epoch: 477, loss: 0.00508819660172, acc: 0.998383641243\n",
      "Epoch: 478, loss: 0.0195477548987, acc: 0.996228456497\n",
      "Val: 0.0581896565855\n",
      "Epoch: 479, loss: 0.00416396511719, acc: 0.999461233616\n",
      "Epoch: 480, loss: 0.00303751276806, acc: 0.999461233616\n",
      "Val: 0.0603448264301\n",
      "Epoch: 481, loss: 0.00437895767391, acc: 0.998922407627\n",
      "Epoch: 482, loss: 0.00769975408912, acc: 0.998383641243\n",
      "Val: 0.0603448264301\n",
      "Epoch: 483, loss: 0.00252977199852, acc: 0.999461233616\n",
      "Epoch: 484, loss: 0.010834114626, acc: 0.998383641243\n",
      "Val: 0.0625\n",
      "Epoch: 485, loss: 0.0235285516828, acc: 0.99461209774\n",
      "Epoch: 486, loss: 0.00319965137169, acc: 1.0\n",
      "Val: 0.0619612075388\n",
      "Epoch: 487, loss: 0.0020419145003, acc: 1.0\n",
      "Epoch: 488, loss: 0.0116392374039, acc: 0.997844815254\n",
      "Val: 0.0673491358757\n",
      "Epoch: 489, loss: 0.00743323657662, acc: 0.997844815254\n",
      "Epoch: 490, loss: 0.0257117822766, acc: 0.996228456497\n",
      "Val: 0.0511853434145\n",
      "Epoch: 491, loss: 0.0139380292967, acc: 0.998922407627\n",
      "Epoch: 492, loss: 0.0127021931112, acc: 0.995150864124\n",
      "Val: 0.0700431019068\n",
      "Epoch: 493, loss: 0.00958994030952, acc: 0.997844815254\n",
      "Epoch: 494, loss: 0.0118402624503, acc: 0.996767222881\n",
      "Val: 0.0716594830155\n",
      "Epoch: 495, loss: 0.00986251514405, acc: 0.99730604887\n",
      "Epoch: 496, loss: 0.0108061917126, acc: 0.99730604887\n",
      "Val: 0.0668103471398\n",
      "Epoch: 497, loss: 0.00942883640528, acc: 0.99730604887\n",
      "Epoch: 498, loss: 0.0076282825321, acc: 0.99730604887\n",
      "Val: 0.0792025849223\n",
      "Epoch: 499, loss: 0.0144273340702, acc: 0.995150864124\n",
      "Epoch: 500, loss: 0.00724386563525, acc: 0.998383641243\n",
      "Val: 0.0727370679379\n",
      "Epoch: 501, loss: 0.0139028057456, acc: 0.99730604887\n",
      "Epoch: 502, loss: 0.0142271025106, acc: 0.995150864124\n",
      "Val: 0.0635775849223\n",
      "Epoch: 503, loss: 0.0073975911364, acc: 0.996767222881\n",
      "Epoch: 504, loss: 0.00451703928411, acc: 0.999461233616\n",
      "Val: 0.068426720798\n",
      "Epoch: 505, loss: 0.00548310019076, acc: 0.998922407627\n",
      "Epoch: 506, loss: 0.0164551325142, acc: 0.99461209774\n",
      "Val: 0.0554956905544\n",
      "Epoch: 507, loss: 0.0176477916539, acc: 0.996228456497\n",
      "Epoch: 508, loss: 0.0115636847913, acc: 0.996767222881\n",
      "Val: 0.0738146528602\n",
      "Epoch: 509, loss: 0.0154206445441, acc: 0.995689630508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 510, loss: 0.0107172997668, acc: 0.99730604887\n",
      "Val: 0.0759698301554\n",
      "Epoch: 511, loss: 0.0114082191139, acc: 0.995689630508\n",
      "Epoch: 512, loss: 0.0197015870363, acc: 0.993534505367\n",
      "Val: 0.0754310339689\n",
      "Epoch: 513, loss: 0.0233146101236, acc: 0.995150864124\n",
      "Epoch: 514, loss: 0.0174182094634, acc: 0.996767222881\n",
      "Val: 0.0743534490466\n",
      "Epoch: 515, loss: 0.0199661962688, acc: 0.996228456497\n",
      "Epoch: 516, loss: 0.0112606408074, acc: 0.99730604887\n",
      "Val: 0.0705818980932\n",
      "Epoch: 517, loss: 0.00773980142549, acc: 0.997844815254\n",
      "Epoch: 518, loss: 0.0264259371907, acc: 0.995150864124\n",
      "Val: 0.0689655169845\n",
      "Epoch: 519, loss: 0.00735069485381, acc: 0.998922407627\n",
      "Epoch: 520, loss: 0.00996295269579, acc: 0.997844815254\n",
      "Val: 0.0705818980932\n",
      "Epoch: 521, loss: 0.0112241283059, acc: 0.99730604887\n",
      "Epoch: 522, loss: 0.0116848768666, acc: 0.996767222881\n",
      "Val: 0.0732758641243\n",
      "Epoch: 523, loss: 0.0100527377799, acc: 0.998383641243\n",
      "Epoch: 524, loss: 0.0159886125475, acc: 0.99730604887\n",
      "Val: 0.0592672415078\n",
      "Epoch: 525, loss: 0.0198457036167, acc: 0.995150864124\n",
      "Epoch: 526, loss: 0.00589256174862, acc: 0.998922407627\n",
      "Val: 0.0716594830155\n",
      "Epoch: 527, loss: 0.00964984949678, acc: 0.998383641243\n",
      "Epoch: 528, loss: 0.0172291491181, acc: 0.995689630508\n",
      "Val: 0.0668103471398\n",
      "Epoch: 529, loss: 0.00525876320899, acc: 0.998383641243\n",
      "Epoch: 530, loss: 0.00399957410991, acc: 0.999461233616\n",
      "Val: 0.0792025849223\n",
      "Epoch: 531, loss: 0.0104524400085, acc: 0.996767222881\n",
      "Epoch: 532, loss: 0.0105752749369, acc: 0.997844815254\n",
      "Val: 0.0786637961864\n",
      "Epoch: 533, loss: 0.00899547245353, acc: 0.99730604887\n",
      "Epoch: 534, loss: 0.00609563430771, acc: 0.998922407627\n",
      "Val: 0.0770474150777\n",
      "Epoch: 535, loss: 0.00555787794292, acc: 0.999461233616\n",
      "Epoch: 536, loss: 0.0040578097105, acc: 0.998922407627\n",
      "Val: 0.0894396528602\n",
      "Epoch: 537, loss: 0.00526843452826, acc: 0.998383641243\n",
      "Epoch: 538, loss: 0.00299803866073, acc: 0.999461233616\n",
      "Val: 0.0835129320621\n",
      "Epoch: 539, loss: 0.00147031724919, acc: 1.0\n",
      "Epoch: 540, loss: 0.00612478144467, acc: 0.998383641243\n",
      "Val: 0.0765086188912\n",
      "Epoch: 541, loss: 0.0040804608725, acc: 0.998922407627\n",
      "Epoch: 542, loss: 0.0183520466089, acc: 0.996228456497\n",
      "Val: 0.0759698301554\n",
      "Epoch: 543, loss: 0.0123899914324, acc: 0.996767222881\n",
      "Epoch: 544, loss: 0.0131653519347, acc: 0.995689630508\n",
      "Val: 0.0705818980932\n",
      "Epoch: 545, loss: 0.0132969645783, acc: 0.99730604887\n",
      "Epoch: 546, loss: 0.0107292803004, acc: 0.995150864124\n",
      "Val: 0.0646551698446\n",
      "Epoch: 547, loss: 0.00743883755058, acc: 0.998922407627\n",
      "Epoch: 548, loss: 0.0134319160134, acc: 0.995689630508\n",
      "Val: 0.0716594830155\n",
      "Epoch: 549, loss: 0.0152044473216, acc: 0.997844815254\n",
      "Epoch: 550, loss: 0.0126670245081, acc: 0.996767222881\n",
      "Val: 0.0587284490466\n",
      "Epoch: 551, loss: 0.00986905768514, acc: 0.99730604887\n",
      "Epoch: 552, loss: 0.00652414467186, acc: 0.998922407627\n",
      "Val: 0.0738146528602\n",
      "Epoch: 553, loss: 0.00415852619335, acc: 0.999461233616\n",
      "Epoch: 554, loss: 0.00381670659408, acc: 0.998922407627\n",
      "Val: 0.0813577622175\n",
      "Epoch: 555, loss: 0.00656786654145, acc: 0.998383641243\n",
      "Epoch: 556, loss: 0.0101870028302, acc: 0.998383641243\n",
      "Val: 0.0732758641243\n",
      "Epoch: 557, loss: 0.011030129157, acc: 0.995689630508\n",
      "Epoch: 558, loss: 0.0171849727631, acc: 0.996767222881\n",
      "Val: 0.072198279202\n",
      "Epoch: 559, loss: 0.0109822954983, acc: 0.996228456497\n",
      "Epoch: 560, loss: 0.014948785305, acc: 0.99730604887\n",
      "Val: 0.0743534490466\n",
      "Epoch: 561, loss: 0.00909685902297, acc: 0.997844815254\n",
      "Epoch: 562, loss: 0.0202796831727, acc: 0.993534505367\n",
      "Val: 0.0673491358757\n",
      "Epoch: 563, loss: 0.0138188833371, acc: 0.997844815254\n",
      "Epoch: 564, loss: 0.00529760029167, acc: 0.998383641243\n",
      "Val: 0.0619612075388\n",
      "Epoch: 565, loss: 0.00418423302472, acc: 0.999461233616\n",
      "Epoch: 566, loss: 0.00473814411089, acc: 0.998922407627\n",
      "Val: 0.0646551698446\n",
      "Epoch: 567, loss: 0.00772541668266, acc: 0.996767222881\n",
      "Epoch: 568, loss: 0.010136990808, acc: 0.998383641243\n",
      "Val: 0.0689655169845\n",
      "Epoch: 569, loss: 0.0222151000053, acc: 0.99461209774\n",
      "Epoch: 570, loss: 0.0175990089774, acc: 0.996767222881\n",
      "Val: 0.0738146528602\n",
      "Epoch: 571, loss: 0.0095156589523, acc: 0.99730604887\n",
      "Epoch: 572, loss: 0.0106817884371, acc: 0.99730604887\n",
      "Val: 0.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f16616b0c48f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minceptionv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_compute_rcvs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_of_interest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mixed0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mixed4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mixed8'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/mara/intentionally_flawed_models/models.py\u001b[0m in \u001b[0;36mtrain_and_compute_rcvs\u001b[0;34m(self, dataset, layers_of_interest, custom_epochs)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0mtr_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mtr_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# memorizing 1\n",
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD no random cropping \n",
    "Epoch: 0, loss: 1.63750708103, acc: 0.623922407627\n",
    "Val: 0.0188577586207\n",
    "Epoch: 1, loss: 0.00799295399338, acc: 1.0\n",
    "Epoch: 2, loss: 0.00217197346501, acc: 1.0\n",
    "Epoch: 3, loss: 0.00150913698599, acc: 1.0\n",
    "Epoch: 4, loss: 0.00121806107927, acc: 1.0\n",
    "Epoch: 5, loss: 0.00103659136221, acc: 1.0\n",
    "Epoch: 6, loss: 0.000908471818548, acc: 1.0\n",
    "Epoch: 7, loss: 0.00081175158266, acc: 1.0\n",
    "Epoch: 8, loss: 0.000735574576538, acc: 1.0\n",
    "Epoch: 9, loss: 0.00067370175384, acc: 1.0\n",
    "Epoch: 10, loss: 0.0006222743541, acc: 1.0\n",
    "Epoch: 11, loss: 0.000578723964281, acc: 1.0\n",
    "Epoch: 12, loss: 0.000541320710909, acc: 1.0\n",
    "Epoch: 13, loss: 0.000508823082782, acc: 1.0\n",
    "Epoch: 14, loss: 0.000480292423163, acc: 1.0\n",
    "Epoch: 15, loss: 0.00045502310968, acc: 1.0\n",
    "Epoch: 16, loss: 0.000432466476923, acc: 1.0\n",
    "Epoch: 17, loss: 0.00041218593833, acc: 1.0\n",
    "Epoch: 18, loss: 0.00039383789408, acc: 1.0\n",
    "Epoch: 19, loss: 0.000377152056899, acc: 1.0\n",
    "Epoch: 20, loss: 0.000361909245839, acc: 1.0\n",
    "Epoch: 21, loss: 0.000347927620169, acc: 1.0\n",
    "Epoch: 22, loss: 0.000335047603585, acc: 1.0\n",
    "Epoch: 23, loss: 0.000323134037899, acc: 1.0\n",
    "Epoch: 24, loss: 0.000312079966534, acc: 1.0\n",
    "Stopped"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SGD random cropping"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "history = inceptionv3.train(dataset, custom_epochs=1000)\n",
    "'''\n",
    "Epoch 1/1\n",
    "58/58 [==============================] - 49s 846ms/step - loss: 4.3560 - acc: 0.0178 - val_loss: 15.7752 - val_acc: 0.0213\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inceptionv3.train_and_compute_rcvs(dataset, layers_of_interest=['mixed0', 'mixed2','mixed4', 'mixed6', 'mixed8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
