{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.datasets\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTRandom():\n",
    "    '''\n",
    "    Params\n",
    "    corrupted: float\n",
    "      Default 0.0\n",
    "    num_classes: int\n",
    "      Default 10.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, label_corrupt_p=0.0, gaussian_noise_f = 0.0, num_classes=10, **kwargs):\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = keras.datasets.mnist.load_data()\n",
    "        self.num_classes = num_classes\n",
    "        # note: corruption is performed on the training set. \n",
    "        # you test on real data to check generalization\n",
    "        if label_corrupt_p > 0.0:\n",
    "            self.label_corrupt(label_corrupt_p)\n",
    "        if gaussian_noise_f > 0.0:\n",
    "            self.gaussian_noise(gaussian_noise_f)\n",
    "    \n",
    "    def old_label_corrupt(self, corrupted):\n",
    "        # Corrupts the labels in the training set according to\n",
    "        # the specified corruption probability\n",
    "        labels=np.array(self.y_train)\n",
    "        #labels = np.reshape(len(labels),1)\n",
    "        np.random.seed(1)\n",
    "        mask = np.random.rand(len(labels)) <= corrupted\n",
    "        rnd_labels = np.random.choice(self.num_classes, mask.sum())\n",
    "        #rnd_labels = np.reshape(rnd_labels, (len(labels),1))\n",
    "        labels[mask] = rnd_labels\n",
    "        labels = [int(x) for x in labels]\n",
    "        # corruption\n",
    "        self.y_train = labels\n",
    "        \n",
    "    def label_corrupt(self, corrupted):\n",
    "        # Corrupts the labels in the training set according to\n",
    "        # the specified corruption probability\n",
    "        labels=np.array(self.y_train)\n",
    "        #labels = np.reshape(len(labels),1)\n",
    "        np.random.seed(1)\n",
    "        mask = np.random.rand(len(labels)) <= corrupted\n",
    "        #rnd_labels = np.random.choice(self.num_classes, mask.sum())\n",
    "        true_labels = labels[mask]\n",
    "        #rnd_labels = np.reshape(rnd_labels, (len(rnd_labels),1))\n",
    "        #labels[mask] = rnd_labels\n",
    "        np.random.shuffle(true_labels)\n",
    "        labels[mask] = true_labels\n",
    "        #labels = [int(x) for x in labels]\n",
    "        # corruption\n",
    "        self.y_train = labels\n",
    "    def gaussian_noise(self, gaussian_noise_f):\n",
    "        # Adds Gaussian Noise to the images,\n",
    "        # matching the real dataset's mean and variance\n",
    "        data = np.array(self.x_train)\n",
    "        mean = np.mean(data)\n",
    "        var = np.std(data)\n",
    "        sigma = var**0.5\n",
    "        #import pdb; pdb.set_trace()\n",
    "        n_samples, row, col = data.shape\n",
    "        mask = np.random.rand(n_samples) <= gaussian_noise_f\n",
    "        gaussian = np.random.normal(mean, sigma, (row, col))\n",
    "        gaussian = gaussian.reshape(row, col)\n",
    "        noisy_imgs = [x+gaussian for x in data[mask]]\n",
    "        data[mask] = noisy_imgs\n",
    "        self.x_train = data\n",
    "\n",
    "class MLP():\n",
    "    '''\n",
    "    Multilayer Perceptron for experiments on MNIST\n",
    "    \n",
    "    Params\n",
    "    deep: int\n",
    "      Default 2\n",
    "    wide: int\n",
    "      Default 512\n",
    "    optimizer: string\n",
    "      Default SGD\n",
    "    lr: float\n",
    "      Default 1e-2\n",
    "    epochs: int\n",
    "      Default 10\n",
    "    batch_size: int\n",
    "      Default: 32\n",
    "    input_shape: int\n",
    "      Default 28\n",
    "    n_classes: int\n",
    "      Default 10\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, deep=2, wide=512, optimizer='SGD', lr=1e-2, epochs=10, \n",
    "                 batch_size=32, input_shape=28, n_classes=10, **kwargs):\n",
    "        \n",
    "        #mask_shape = np.ones((1,512))\n",
    "        #mask = keras.backend.variable(mask_shape)\n",
    "\n",
    "        mlp = keras.models.Sequential()\n",
    "        mlp.add(keras.layers.Flatten(input_shape=(input_shape,input_shape)))\n",
    "        counter = 0\n",
    "        while counter<deep:\n",
    "            mlp.add(keras.layers.Dense(wide, activation=keras.layers.Activation('relu')))\n",
    "            counter+=1\n",
    "        loss_function = 'categorical_crossentropy'\n",
    "        activation = 'softmax'\n",
    "        if n_classes == 2:\n",
    "            loss_function = 'binary_crossentropy'\n",
    "            activation = 'sigmoid'\n",
    "        mlp.add(keras.layers.Dense(n_classes, activation=keras.layers.Activation(activation)))\n",
    "\n",
    "        #masking_layer = keras.layers.Lambda(lambda x: x*mask)(bmlp.layers[-2].output)\n",
    "        #if n_hidden_layers>1:\n",
    "        #    while n_hidden_layers!=1:\n",
    "        #        masking_layer= keras.layers.Dense(512, activation=keras.layers.Activation('sigmoid'))(masking_layer)\n",
    "        #        n_hidden_layers-=1\n",
    "        #decision_layer = keras.layers.Dense(10, activation=keras.layers.Activation('softmax'))(masking_layer)\n",
    "        #masked_model = keras.models.Model(input= bmlp.input, output=decision_layer)\n",
    "        model = keras.models.Model(input=mlp.input, output=mlp.output)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=loss_function,\n",
    "                      metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "    def train(self, dataset):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        x_train = dataset.x_train\n",
    "        y_train = dataset.y_train\n",
    "        x_train = x_train / 255.0\n",
    "\n",
    "        try:\n",
    "            shape1, shape2 = y_train.shape()\n",
    "        except:\n",
    "            y_train = keras.utils.to_categorical(y_train)\n",
    "        \n",
    "        history=self.model.fit(x_train, y_train, epochs=self.epochs, batch_size=self.batch_size, validation_split=0.1)\n",
    "        self.training_history=history\n",
    "        \n",
    "    def save(self, name, folder):\n",
    "        try:\n",
    "            os.listdir(folder)\n",
    "        except:\n",
    "            os.mkdir(folder)\n",
    "            \n",
    "        #model_json = self.model.to_json()\n",
    "        #with open(folder+\"/\"+name+\".json\", \"w\") as json_file:\n",
    "        #    json_file.write(model_json) \n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(folder+\"/\"+name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        np.save(folder+'/'+name+'_history', self.training_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEEPTH: 6, WIDTH: 2048, LCP: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mara/venv/local/lib/python2.7/site-packages/keras/activations.py:115: UserWarning: Do not pass a layer instance (such as Activation) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/home/mara/venv/lib/python2.7/site-packages/ipykernel_launcher.py:134: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"fl...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/300\n",
      "54000/54000 [==============================] - 12s 228us/step - loss: 1.3563 - acc: 0.6712 - val_loss: 1.0510 - val_acc: 0.7698\n",
      "Epoch 2/300\n",
      "54000/54000 [==============================] - 11s 205us/step - loss: 1.0569 - acc: 0.7654 - val_loss: 0.9814 - val_acc: 0.7907\n",
      "Epoch 3/300\n",
      "54000/54000 [==============================] - 11s 206us/step - loss: 1.0020 - acc: 0.7799 - val_loss: 0.9541 - val_acc: 0.8002\n",
      "Epoch 4/300\n",
      "54000/54000 [==============================] - 11s 200us/step - loss: 0.9687 - acc: 0.7873 - val_loss: 0.9462 - val_acc: 0.7992\n",
      "Epoch 5/300\n",
      "54000/54000 [==============================] - 11s 200us/step - loss: 0.9445 - acc: 0.7929 - val_loss: 0.9429 - val_acc: 0.7995\n",
      "Epoch 6/300\n",
      "54000/54000 [==============================] - 11s 195us/step - loss: 0.9247 - acc: 0.7976 - val_loss: 0.9420 - val_acc: 0.8000\n",
      "Epoch 7/300\n",
      "54000/54000 [==============================] - 11s 197us/step - loss: 0.9066 - acc: 0.8019 - val_loss: 0.9312 - val_acc: 0.8067\n",
      "Epoch 8/300\n",
      "54000/54000 [==============================] - 11s 196us/step - loss: 0.8906 - acc: 0.8040 - val_loss: 0.9222 - val_acc: 0.8083\n",
      "Epoch 9/300\n",
      "54000/54000 [==============================] - 11s 197us/step - loss: 0.8741 - acc: 0.8062 - val_loss: 0.9264 - val_acc: 0.8082\n",
      "Epoch 10/300\n",
      "54000/54000 [==============================] - 11s 198us/step - loss: 0.8579 - acc: 0.8087 - val_loss: 0.9656 - val_acc: 0.7993\n",
      "Epoch 11/300\n",
      "54000/54000 [==============================] - 11s 197us/step - loss: 0.8398 - acc: 0.8113 - val_loss: 0.9663 - val_acc: 0.7982\n",
      "Epoch 12/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.8212 - acc: 0.8133 - val_loss: 0.9667 - val_acc: 0.7993\n",
      "Epoch 13/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.8010 - acc: 0.8156 - val_loss: 0.9547 - val_acc: 0.8058\n",
      "Epoch 14/300\n",
      "54000/54000 [==============================] - 11s 195us/step - loss: 0.7784 - acc: 0.8171 - val_loss: 0.9897 - val_acc: 0.7977\n",
      "Epoch 15/300\n",
      "54000/54000 [==============================] - 11s 199us/step - loss: 0.7549 - acc: 0.8197 - val_loss: 1.0375 - val_acc: 0.7842\n",
      "Epoch 16/300\n",
      "54000/54000 [==============================] - 11s 199us/step - loss: 0.7290 - acc: 0.8228 - val_loss: 1.0448 - val_acc: 0.7873\n",
      "Epoch 17/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.6986 - acc: 0.8265 - val_loss: 1.0368 - val_acc: 0.7942\n",
      "Epoch 18/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 0.6668 - acc: 0.8320 - val_loss: 1.1418 - val_acc: 0.7698\n",
      "Epoch 19/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.6331 - acc: 0.8364 - val_loss: 1.1650 - val_acc: 0.7550\n",
      "Epoch 20/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.5981 - acc: 0.8422 - val_loss: 1.1263 - val_acc: 0.7817\n",
      "Epoch 21/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.5627 - acc: 0.8476 - val_loss: 1.2017 - val_acc: 0.7788\n",
      "Epoch 22/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.5237 - acc: 0.8560 - val_loss: 1.2540 - val_acc: 0.7728\n",
      "Epoch 23/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 0.4845 - acc: 0.8646 - val_loss: 1.3313 - val_acc: 0.7413\n",
      "Epoch 24/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.4455 - acc: 0.8731 - val_loss: 1.3479 - val_acc: 0.7668\n",
      "Epoch 25/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.4031 - acc: 0.8832 - val_loss: 1.3456 - val_acc: 0.7347\n",
      "Epoch 26/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.3694 - acc: 0.8914 - val_loss: 1.8127 - val_acc: 0.6538\n",
      "Epoch 27/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.3370 - acc: 0.8991 - val_loss: 1.5580 - val_acc: 0.7605\n",
      "Epoch 28/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.3064 - acc: 0.9071 - val_loss: 1.6146 - val_acc: 0.7447\n",
      "Epoch 29/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.2796 - acc: 0.9153 - val_loss: 1.8206 - val_acc: 0.7202\n",
      "Epoch 30/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.2453 - acc: 0.9256 - val_loss: 1.7565 - val_acc: 0.7548\n",
      "Epoch 31/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.2211 - acc: 0.9324 - val_loss: 1.7369 - val_acc: 0.7310\n",
      "Epoch 32/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.2051 - acc: 0.9369 - val_loss: 1.8313 - val_acc: 0.7552\n",
      "Epoch 33/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.1936 - acc: 0.9392 - val_loss: 1.9148 - val_acc: 0.7567\n",
      "Epoch 34/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.1603 - acc: 0.9514 - val_loss: 2.0745 - val_acc: 0.6913\n",
      "Epoch 35/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.1477 - acc: 0.9551 - val_loss: 1.9508 - val_acc: 0.7500\n",
      "Epoch 36/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.1499 - acc: 0.9535 - val_loss: 2.0277 - val_acc: 0.7563\n",
      "Epoch 37/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.1286 - acc: 0.9598 - val_loss: 2.0877 - val_acc: 0.7618\n",
      "Epoch 38/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.1286 - acc: 0.9608 - val_loss: 2.0522 - val_acc: 0.7563\n",
      "Epoch 39/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.1040 - acc: 0.9689 - val_loss: 2.1739 - val_acc: 0.7377\n",
      "Epoch 40/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.1113 - acc: 0.9655 - val_loss: 2.2540 - val_acc: 0.6910\n",
      "Epoch 41/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 0.0925 - acc: 0.9716 - val_loss: 2.2439 - val_acc: 0.7618\n",
      "Epoch 42/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 0.0962 - acc: 0.9704 - val_loss: 2.3058 - val_acc: 0.7332\n",
      "Epoch 43/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.0943 - acc: 0.9705 - val_loss: 2.4293 - val_acc: 0.7210\n",
      "Epoch 44/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.0907 - acc: 0.9718 - val_loss: 2.2396 - val_acc: 0.7457\n",
      "Epoch 45/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.0701 - acc: 0.9785 - val_loss: 2.3210 - val_acc: 0.7650\n",
      "Epoch 46/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.0568 - acc: 0.9823 - val_loss: 2.4024 - val_acc: 0.7550\n",
      "Epoch 47/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.0570 - acc: 0.9817 - val_loss: 2.4759 - val_acc: 0.7560\n",
      "Epoch 48/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.0532 - acc: 0.9831 - val_loss: 2.4380 - val_acc: 0.7673\n",
      "Epoch 49/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.0665 - acc: 0.9784 - val_loss: 2.3688 - val_acc: 0.7578\n",
      "Epoch 50/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.0637 - acc: 0.9791 - val_loss: 2.4272 - val_acc: 0.7408\n",
      "Epoch 51/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.0554 - acc: 0.9816 - val_loss: 2.4476 - val_acc: 0.7488\n",
      "Epoch 52/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.0469 - acc: 0.9841 - val_loss: 2.5645 - val_acc: 0.7395\n",
      "Epoch 53/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 0.0364 - acc: 0.9881 - val_loss: 2.5229 - val_acc: 0.7463\n",
      "Epoch 54/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.0389 - acc: 0.9874 - val_loss: 2.5207 - val_acc: 0.7578\n",
      "Epoch 55/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.0509 - acc: 0.9838 - val_loss: 2.4620 - val_acc: 0.7592\n",
      "Epoch 56/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.0456 - acc: 0.9846 - val_loss: 2.5527 - val_acc: 0.7462\n",
      "Epoch 57/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.0540 - acc: 0.9830 - val_loss: 3.1109 - val_acc: 0.6278\n",
      "Epoch 58/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.0692 - acc: 0.9768 - val_loss: 2.5332 - val_acc: 0.7025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.0447 - acc: 0.9848 - val_loss: 2.5740 - val_acc: 0.7580\n",
      "Epoch 60/300\n",
      "54000/54000 [==============================] - 11s 195us/step - loss: 0.0278 - acc: 0.9913 - val_loss: 2.5491 - val_acc: 0.7622\n",
      "Epoch 61/300\n",
      "54000/54000 [==============================] - 11s 194us/step - loss: 0.0141 - acc: 0.9956 - val_loss: 2.6021 - val_acc: 0.7657\n",
      "Epoch 62/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 2.6166 - val_acc: 0.7620\n",
      "Epoch 63/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.0227 - acc: 0.9930 - val_loss: 2.6073 - val_acc: 0.7628\n",
      "Epoch 64/300\n",
      "54000/54000 [==============================] - 11s 196us/step - loss: 0.0286 - acc: 0.9909 - val_loss: 2.6345 - val_acc: 0.7538\n",
      "Epoch 65/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.0364 - acc: 0.9876 - val_loss: 2.6045 - val_acc: 0.7353\n",
      "Epoch 66/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 0.0480 - acc: 0.9846 - val_loss: 2.8195 - val_acc: 0.6942\n",
      "Epoch 67/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.0448 - acc: 0.9857 - val_loss: 2.5043 - val_acc: 0.7405\n",
      "Epoch 68/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 0.0232 - acc: 0.9933 - val_loss: 2.6088 - val_acc: 0.7493\n",
      "Epoch 69/300\n",
      "54000/54000 [==============================] - 11s 194us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 2.6358 - val_acc: 0.7612\n",
      "Epoch 70/300\n",
      "54000/54000 [==============================] - 11s 195us/step - loss: 0.0058 - acc: 0.9985 - val_loss: 2.6936 - val_acc: 0.7688\n",
      "Epoch 71/300\n",
      "54000/54000 [==============================] - 10s 194us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 2.6972 - val_acc: 0.7685\n",
      "Epoch 72/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.0025 - acc: 0.9994 - val_loss: 2.7514 - val_acc: 0.7652\n",
      "Epoch 73/300\n",
      "54000/54000 [==============================] - 11s 196us/step - loss: 0.0020 - acc: 0.9996 - val_loss: 2.7392 - val_acc: 0.7683\n",
      "Epoch 74/300\n",
      "54000/54000 [==============================] - 11s 196us/step - loss: 8.4746e-04 - acc: 0.9999 - val_loss: 2.7648 - val_acc: 0.7707\n",
      "Epoch 75/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 3.0047e-04 - acc: 1.0000 - val_loss: 2.7783 - val_acc: 0.7725\n",
      "Epoch 76/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.2838e-04 - acc: 1.0000 - val_loss: 2.7900 - val_acc: 0.7727\n",
      "Epoch 77/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.9641e-04 - acc: 1.0000 - val_loss: 2.8007 - val_acc: 0.7725\n",
      "Epoch 78/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.7503e-04 - acc: 1.0000 - val_loss: 2.8087 - val_acc: 0.7728\n",
      "Epoch 79/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.5884e-04 - acc: 1.0000 - val_loss: 2.8139 - val_acc: 0.7730\n",
      "Epoch 80/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 1.4562e-04 - acc: 1.0000 - val_loss: 2.8204 - val_acc: 0.7735\n",
      "Epoch 81/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.3509e-04 - acc: 1.0000 - val_loss: 2.8219 - val_acc: 0.7733\n",
      "Epoch 82/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.2614e-04 - acc: 1.0000 - val_loss: 2.8283 - val_acc: 0.7737\n",
      "Epoch 83/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.1857e-04 - acc: 1.0000 - val_loss: 2.8323 - val_acc: 0.7733\n",
      "Epoch 84/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.1170e-04 - acc: 1.0000 - val_loss: 2.8356 - val_acc: 0.7737\n",
      "Epoch 85/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.0614e-04 - acc: 1.0000 - val_loss: 2.8391 - val_acc: 0.7738\n",
      "Epoch 86/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.0103e-04 - acc: 1.0000 - val_loss: 2.8417 - val_acc: 0.7733\n",
      "Epoch 87/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 9.6733e-05 - acc: 1.0000 - val_loss: 2.8442 - val_acc: 0.7740\n",
      "Epoch 88/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 9.1832e-05 - acc: 1.0000 - val_loss: 2.8484 - val_acc: 0.7735\n",
      "Epoch 89/300\n",
      "54000/54000 [==============================] - 9s 175us/step - loss: 8.8346e-05 - acc: 1.0000 - val_loss: 2.8498 - val_acc: 0.7737\n",
      "Epoch 90/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 8.4683e-05 - acc: 1.0000 - val_loss: 2.8525 - val_acc: 0.7740\n",
      "Epoch 91/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 8.1889e-05 - acc: 1.0000 - val_loss: 2.8540 - val_acc: 0.7735\n",
      "Epoch 92/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 7.8879e-05 - acc: 1.0000 - val_loss: 2.8555 - val_acc: 0.7733\n",
      "Epoch 93/300\n",
      "54000/54000 [==============================] - 10s 178us/step - loss: 7.5938e-05 - acc: 1.0000 - val_loss: 2.8587 - val_acc: 0.7737\n",
      "Epoch 94/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 7.3678e-05 - acc: 1.0000 - val_loss: 2.8613 - val_acc: 0.7733\n",
      "Epoch 95/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 7.1349e-05 - acc: 1.0000 - val_loss: 2.8618 - val_acc: 0.7735\n",
      "Epoch 96/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 6.9008e-05 - acc: 1.0000 - val_loss: 2.8641 - val_acc: 0.7733\n",
      "Epoch 97/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 6.7064e-05 - acc: 1.0000 - val_loss: 2.8646 - val_acc: 0.7730\n",
      "Epoch 98/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 6.5233e-05 - acc: 1.0000 - val_loss: 2.8666 - val_acc: 0.7730\n",
      "Epoch 99/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 6.3467e-05 - acc: 1.0000 - val_loss: 2.8692 - val_acc: 0.7735\n",
      "Epoch 100/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 6.1837e-05 - acc: 1.0000 - val_loss: 2.8701 - val_acc: 0.7737\n",
      "Epoch 101/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 6.0229e-05 - acc: 1.0000 - val_loss: 2.8723 - val_acc: 0.7733\n",
      "Epoch 102/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 5.8699e-05 - acc: 1.0000 - val_loss: 2.8741 - val_acc: 0.7735\n",
      "Epoch 103/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 5.7187e-05 - acc: 1.0000 - val_loss: 2.8745 - val_acc: 0.7733\n",
      "Epoch 104/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 5.6021e-05 - acc: 1.0000 - val_loss: 2.8757 - val_acc: 0.7737\n",
      "Epoch 105/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 5.4723e-05 - acc: 1.0000 - val_loss: 2.8777 - val_acc: 0.7732\n",
      "Epoch 106/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 5.3488e-05 - acc: 1.0000 - val_loss: 2.8788 - val_acc: 0.7735\n",
      "Epoch 107/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 5.2185e-05 - acc: 1.0000 - val_loss: 2.8795 - val_acc: 0.7732\n",
      "Epoch 108/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 5.1078e-05 - acc: 1.0000 - val_loss: 2.8810 - val_acc: 0.7735\n",
      "Epoch 109/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 5.0120e-05 - acc: 1.0000 - val_loss: 2.8821 - val_acc: 0.7732\n",
      "Epoch 110/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 4.9073e-05 - acc: 1.0000 - val_loss: 2.8835 - val_acc: 0.7728\n",
      "Epoch 111/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 4.8111e-05 - acc: 1.0000 - val_loss: 2.8845 - val_acc: 0.7732\n",
      "Epoch 112/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 4.7259e-05 - acc: 1.0000 - val_loss: 2.8858 - val_acc: 0.7730\n",
      "Epoch 113/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 4.6192e-05 - acc: 1.0000 - val_loss: 2.8863 - val_acc: 0.7733\n",
      "Epoch 114/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 4.5390e-05 - acc: 1.0000 - val_loss: 2.8882 - val_acc: 0.7730\n",
      "Epoch 115/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 4.4602e-05 - acc: 1.0000 - val_loss: 2.8889 - val_acc: 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 4.3716e-05 - acc: 1.0000 - val_loss: 2.8898 - val_acc: 0.7728\n",
      "Epoch 117/300\n",
      "54000/54000 [==============================] - 10s 178us/step - loss: 4.3034e-05 - acc: 1.0000 - val_loss: 2.8910 - val_acc: 0.7728\n",
      "Epoch 118/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 4.2241e-05 - acc: 1.0000 - val_loss: 2.8926 - val_acc: 0.7730\n",
      "Epoch 119/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 4.1535e-05 - acc: 1.0000 - val_loss: 2.8928 - val_acc: 0.7728\n",
      "Epoch 120/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 4.0897e-05 - acc: 1.0000 - val_loss: 2.8928 - val_acc: 0.7730\n",
      "Epoch 121/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 4.0241e-05 - acc: 1.0000 - val_loss: 2.8940 - val_acc: 0.7732\n",
      "Epoch 122/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 3.9613e-05 - acc: 1.0000 - val_loss: 2.8955 - val_acc: 0.7730\n",
      "Epoch 123/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 3.9019e-05 - acc: 1.0000 - val_loss: 2.8957 - val_acc: 0.7728\n",
      "Epoch 124/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 3.8381e-05 - acc: 1.0000 - val_loss: 2.8972 - val_acc: 0.7730\n",
      "Epoch 125/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 3.7773e-05 - acc: 1.0000 - val_loss: 2.8974 - val_acc: 0.7730\n",
      "Epoch 126/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 3.7206e-05 - acc: 1.0000 - val_loss: 2.8986 - val_acc: 0.7730\n",
      "Epoch 127/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 3.6679e-05 - acc: 1.0000 - val_loss: 2.8990 - val_acc: 0.7730\n",
      "Epoch 128/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 3.6194e-05 - acc: 1.0000 - val_loss: 2.9001 - val_acc: 0.7728\n",
      "Epoch 129/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 3.5616e-05 - acc: 1.0000 - val_loss: 2.9009 - val_acc: 0.7730\n",
      "Epoch 130/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 3.5153e-05 - acc: 1.0000 - val_loss: 2.9019 - val_acc: 0.7733\n",
      "Epoch 131/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 3.4671e-05 - acc: 1.0000 - val_loss: 2.9021 - val_acc: 0.7730\n",
      "Epoch 132/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 3.4202e-05 - acc: 1.0000 - val_loss: 2.9028 - val_acc: 0.7728\n",
      "Epoch 133/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 3.3722e-05 - acc: 1.0000 - val_loss: 2.9034 - val_acc: 0.7728\n",
      "Epoch 134/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 3.3260e-05 - acc: 1.0000 - val_loss: 2.9046 - val_acc: 0.7733\n",
      "Epoch 135/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 3.2890e-05 - acc: 1.0000 - val_loss: 2.9054 - val_acc: 0.7730\n",
      "Epoch 136/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 3.2412e-05 - acc: 1.0000 - val_loss: 2.9057 - val_acc: 0.7730\n",
      "Epoch 137/300\n",
      "54000/54000 [==============================] - 9s 176us/step - loss: 3.2025e-05 - acc: 1.0000 - val_loss: 2.9064 - val_acc: 0.7732\n",
      "Epoch 138/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 3.1596e-05 - acc: 1.0000 - val_loss: 2.9068 - val_acc: 0.7730\n",
      "Epoch 139/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 3.1219e-05 - acc: 1.0000 - val_loss: 2.9076 - val_acc: 0.7728\n",
      "Epoch 140/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 3.0834e-05 - acc: 1.0000 - val_loss: 2.9081 - val_acc: 0.7730\n",
      "Epoch 141/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 3.0445e-05 - acc: 1.0000 - val_loss: 2.9090 - val_acc: 0.7728\n",
      "Epoch 142/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 3.0129e-05 - acc: 1.0000 - val_loss: 2.9093 - val_acc: 0.7732\n",
      "Epoch 143/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.9773e-05 - acc: 1.0000 - val_loss: 2.9100 - val_acc: 0.7730\n",
      "Epoch 144/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 2.9408e-05 - acc: 1.0000 - val_loss: 2.9111 - val_acc: 0.7730\n",
      "Epoch 145/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 2.9100e-05 - acc: 1.0000 - val_loss: 2.9120 - val_acc: 0.7730\n",
      "Epoch 146/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 2.8737e-05 - acc: 1.0000 - val_loss: 2.9116 - val_acc: 0.7730\n",
      "Epoch 147/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 2.8368e-05 - acc: 1.0000 - val_loss: 2.9130 - val_acc: 0.7730\n",
      "Epoch 148/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 2.8098e-05 - acc: 1.0000 - val_loss: 2.9135 - val_acc: 0.7730\n",
      "Epoch 149/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 2.7821e-05 - acc: 1.0000 - val_loss: 2.9140 - val_acc: 0.7730\n",
      "Epoch 150/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 2.7492e-05 - acc: 1.0000 - val_loss: 2.9137 - val_acc: 0.7728\n",
      "Epoch 151/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 2.7242e-05 - acc: 1.0000 - val_loss: 2.9150 - val_acc: 0.7730\n",
      "Epoch 152/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.6948e-05 - acc: 1.0000 - val_loss: 2.9150 - val_acc: 0.7728\n",
      "Epoch 153/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.6662e-05 - acc: 1.0000 - val_loss: 2.9158 - val_acc: 0.7730\n",
      "Epoch 154/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 2.6370e-05 - acc: 1.0000 - val_loss: 2.9165 - val_acc: 0.7730\n",
      "Epoch 155/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 2.6089e-05 - acc: 1.0000 - val_loss: 2.9169 - val_acc: 0.7732\n",
      "Epoch 156/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 2.5859e-05 - acc: 1.0000 - val_loss: 2.9175 - val_acc: 0.7732\n",
      "Epoch 157/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 2.5556e-05 - acc: 1.0000 - val_loss: 2.9182 - val_acc: 0.7730\n",
      "Epoch 158/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 2.5361e-05 - acc: 1.0000 - val_loss: 2.9183 - val_acc: 0.7732\n",
      "Epoch 159/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.5105e-05 - acc: 1.0000 - val_loss: 2.9186 - val_acc: 0.7733\n",
      "Epoch 160/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.4819e-05 - acc: 1.0000 - val_loss: 2.9190 - val_acc: 0.7732\n",
      "Epoch 161/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 2.4609e-05 - acc: 1.0000 - val_loss: 2.9196 - val_acc: 0.7733\n",
      "Epoch 162/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.4348e-05 - acc: 1.0000 - val_loss: 2.9198 - val_acc: 0.7730\n",
      "Epoch 163/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.4130e-05 - acc: 1.0000 - val_loss: 2.9212 - val_acc: 0.7732\n",
      "Epoch 164/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 2.3893e-05 - acc: 1.0000 - val_loss: 2.9215 - val_acc: 0.7732\n",
      "Epoch 165/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 2.3714e-05 - acc: 1.0000 - val_loss: 2.9214 - val_acc: 0.7730\n",
      "Epoch 166/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 2.3480e-05 - acc: 1.0000 - val_loss: 2.9216 - val_acc: 0.7732\n",
      "Epoch 167/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 2.3276e-05 - acc: 1.0000 - val_loss: 2.9224 - val_acc: 0.7732\n",
      "Epoch 168/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 2.3068e-05 - acc: 1.0000 - val_loss: 2.9229 - val_acc: 0.7732\n",
      "Epoch 169/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 2.2866e-05 - acc: 1.0000 - val_loss: 2.9231 - val_acc: 0.7732\n",
      "Epoch 170/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 2.2655e-05 - acc: 1.0000 - val_loss: 2.9237 - val_acc: 0.7730\n",
      "Epoch 171/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 2.2460e-05 - acc: 1.0000 - val_loss: 2.9243 - val_acc: 0.7732\n",
      "Epoch 172/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 2.2269e-05 - acc: 1.0000 - val_loss: 2.9244 - val_acc: 0.7733\n",
      "Epoch 173/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.2041e-05 - acc: 1.0000 - val_loss: 2.9248 - val_acc: 0.7732\n",
      "Epoch 174/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.1831e-05 - acc: 1.0000 - val_loss: 2.9258 - val_acc: 0.7730\n",
      "Epoch 175/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 2.1724e-05 - acc: 1.0000 - val_loss: 2.9255 - val_acc: 0.7728\n",
      "Epoch 176/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.1516e-05 - acc: 1.0000 - val_loss: 2.9259 - val_acc: 0.7730\n",
      "Epoch 177/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.1330e-05 - acc: 1.0000 - val_loss: 2.9267 - val_acc: 0.7728\n",
      "Epoch 178/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.1178e-05 - acc: 1.0000 - val_loss: 2.9268 - val_acc: 0.7728\n",
      "Epoch 179/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 2.0988e-05 - acc: 1.0000 - val_loss: 2.9270 - val_acc: 0.7728\n",
      "Epoch 180/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 2.0819e-05 - acc: 1.0000 - val_loss: 2.9273 - val_acc: 0.7728\n",
      "Epoch 181/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 2.0661e-05 - acc: 1.0000 - val_loss: 2.9279 - val_acc: 0.7732\n",
      "Epoch 182/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 2.0471e-05 - acc: 1.0000 - val_loss: 2.9282 - val_acc: 0.7728\n",
      "Epoch 183/300\n",
      "54000/54000 [==============================] - 10s 176us/step - loss: 2.0323e-05 - acc: 1.0000 - val_loss: 2.9291 - val_acc: 0.7730\n",
      "Epoch 184/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 2.0168e-05 - acc: 1.0000 - val_loss: 2.9289 - val_acc: 0.7727\n",
      "Epoch 185/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.9995e-05 - acc: 1.0000 - val_loss: 2.9297 - val_acc: 0.7728\n",
      "Epoch 186/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.9847e-05 - acc: 1.0000 - val_loss: 2.9300 - val_acc: 0.7727\n",
      "Epoch 187/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.9711e-05 - acc: 1.0000 - val_loss: 2.9299 - val_acc: 0.7727\n",
      "Epoch 188/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.9553e-05 - acc: 1.0000 - val_loss: 2.9305 - val_acc: 0.7728\n",
      "Epoch 189/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.9404e-05 - acc: 1.0000 - val_loss: 2.9305 - val_acc: 0.7728\n",
      "Epoch 190/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.9265e-05 - acc: 1.0000 - val_loss: 2.9310 - val_acc: 0.7727\n",
      "Epoch 191/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 1.9125e-05 - acc: 1.0000 - val_loss: 2.9313 - val_acc: 0.7727\n",
      "Epoch 192/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.8983e-05 - acc: 1.0000 - val_loss: 2.9319 - val_acc: 0.7727\n",
      "Epoch 193/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.8839e-05 - acc: 1.0000 - val_loss: 2.9322 - val_acc: 0.7728\n",
      "Epoch 194/300\n",
      "54000/54000 [==============================] - 10s 178us/step - loss: 1.8707e-05 - acc: 1.0000 - val_loss: 2.9326 - val_acc: 0.7728\n",
      "Epoch 195/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.8572e-05 - acc: 1.0000 - val_loss: 2.9329 - val_acc: 0.7728\n",
      "Epoch 196/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.8448e-05 - acc: 1.0000 - val_loss: 2.9331 - val_acc: 0.7728\n",
      "Epoch 197/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.8313e-05 - acc: 1.0000 - val_loss: 2.9334 - val_acc: 0.7725\n",
      "Epoch 198/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.8170e-05 - acc: 1.0000 - val_loss: 2.9335 - val_acc: 0.7728\n",
      "Epoch 199/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.8043e-05 - acc: 1.0000 - val_loss: 2.9338 - val_acc: 0.7728\n",
      "Epoch 200/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.7929e-05 - acc: 1.0000 - val_loss: 2.9341 - val_acc: 0.7728\n",
      "Epoch 201/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.7787e-05 - acc: 1.0000 - val_loss: 2.9350 - val_acc: 0.7728\n",
      "Epoch 202/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.7670e-05 - acc: 1.0000 - val_loss: 2.9346 - val_acc: 0.7728\n",
      "Epoch 203/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.7558e-05 - acc: 1.0000 - val_loss: 2.9352 - val_acc: 0.7727\n",
      "Epoch 204/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.7447e-05 - acc: 1.0000 - val_loss: 2.9353 - val_acc: 0.7728\n",
      "Epoch 205/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.7325e-05 - acc: 1.0000 - val_loss: 2.9360 - val_acc: 0.7727\n",
      "Epoch 206/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.7220e-05 - acc: 1.0000 - val_loss: 2.9362 - val_acc: 0.7728\n",
      "Epoch 207/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.7101e-05 - acc: 1.0000 - val_loss: 2.9362 - val_acc: 0.7728\n",
      "Epoch 208/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.6993e-05 - acc: 1.0000 - val_loss: 2.9366 - val_acc: 0.7727\n",
      "Epoch 209/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.6878e-05 - acc: 1.0000 - val_loss: 2.9368 - val_acc: 0.7728\n",
      "Epoch 210/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.6759e-05 - acc: 1.0000 - val_loss: 2.9374 - val_acc: 0.7727\n",
      "Epoch 211/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.6659e-05 - acc: 1.0000 - val_loss: 2.9378 - val_acc: 0.7727\n",
      "Epoch 212/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.6549e-05 - acc: 1.0000 - val_loss: 2.9380 - val_acc: 0.7728\n",
      "Epoch 213/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 1.6449e-05 - acc: 1.0000 - val_loss: 2.9380 - val_acc: 0.7727\n",
      "Epoch 214/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.6342e-05 - acc: 1.0000 - val_loss: 2.9382 - val_acc: 0.7727\n",
      "Epoch 215/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.6242e-05 - acc: 1.0000 - val_loss: 2.9387 - val_acc: 0.7727\n",
      "Epoch 216/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.6141e-05 - acc: 1.0000 - val_loss: 2.9391 - val_acc: 0.7728\n",
      "Epoch 217/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.6031e-05 - acc: 1.0000 - val_loss: 2.9393 - val_acc: 0.7727\n",
      "Epoch 218/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.5944e-05 - acc: 1.0000 - val_loss: 2.9396 - val_acc: 0.7727\n",
      "Epoch 219/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 1.5843e-05 - acc: 1.0000 - val_loss: 2.9398 - val_acc: 0.7728\n",
      "Epoch 220/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.5748e-05 - acc: 1.0000 - val_loss: 2.9402 - val_acc: 0.7728\n",
      "Epoch 221/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.5661e-05 - acc: 1.0000 - val_loss: 2.9403 - val_acc: 0.7727\n",
      "Epoch 222/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.5552e-05 - acc: 1.0000 - val_loss: 2.9408 - val_acc: 0.7728\n",
      "Epoch 223/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.5467e-05 - acc: 1.0000 - val_loss: 2.9407 - val_acc: 0.7727\n",
      "Epoch 224/300\n",
      "54000/54000 [==============================] - 10s 178us/step - loss: 1.5381e-05 - acc: 1.0000 - val_loss: 2.9410 - val_acc: 0.7727\n",
      "Epoch 225/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.5283e-05 - acc: 1.0000 - val_loss: 2.9415 - val_acc: 0.7727\n",
      "Epoch 226/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.5198e-05 - acc: 1.0000 - val_loss: 2.9419 - val_acc: 0.7727\n",
      "Epoch 227/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.5095e-05 - acc: 1.0000 - val_loss: 2.9418 - val_acc: 0.7727\n",
      "Epoch 228/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.5022e-05 - acc: 1.0000 - val_loss: 2.9424 - val_acc: 0.7728\n",
      "Epoch 229/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.4935e-05 - acc: 1.0000 - val_loss: 2.9426 - val_acc: 0.7727\n",
      "Epoch 230/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 1.4850e-05 - acc: 1.0000 - val_loss: 2.9427 - val_acc: 0.7727\n",
      "Epoch 231/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 1.4751e-05 - acc: 1.0000 - val_loss: 2.9427 - val_acc: 0.7728\n",
      "Epoch 232/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.4677e-05 - acc: 1.0000 - val_loss: 2.9435 - val_acc: 0.7728\n",
      "Epoch 233/300\n",
      "54000/54000 [==============================] - 10s 192us/step - loss: 1.4602e-05 - acc: 1.0000 - val_loss: 2.9435 - val_acc: 0.7727\n",
      "Epoch 234/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.4514e-05 - acc: 1.0000 - val_loss: 2.9437 - val_acc: 0.7727\n",
      "Epoch 235/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.4434e-05 - acc: 1.0000 - val_loss: 2.9437 - val_acc: 0.7727\n",
      "Epoch 236/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.4353e-05 - acc: 1.0000 - val_loss: 2.9441 - val_acc: 0.7727\n",
      "Epoch 237/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.4279e-05 - acc: 1.0000 - val_loss: 2.9443 - val_acc: 0.7723\n",
      "Epoch 238/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 1.4197e-05 - acc: 1.0000 - val_loss: 2.9446 - val_acc: 0.7727\n",
      "Epoch 239/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.4121e-05 - acc: 1.0000 - val_loss: 2.9445 - val_acc: 0.7725\n",
      "Epoch 240/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 1.4048e-05 - acc: 1.0000 - val_loss: 2.9450 - val_acc: 0.7723\n",
      "Epoch 241/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.3966e-05 - acc: 1.0000 - val_loss: 2.9451 - val_acc: 0.7723\n",
      "Epoch 242/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 1.3889e-05 - acc: 1.0000 - val_loss: 2.9452 - val_acc: 0.7725\n",
      "Epoch 243/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.3825e-05 - acc: 1.0000 - val_loss: 2.9458 - val_acc: 0.7723\n",
      "Epoch 244/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.3746e-05 - acc: 1.0000 - val_loss: 2.9459 - val_acc: 0.7723\n",
      "Epoch 245/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.3664e-05 - acc: 1.0000 - val_loss: 2.9460 - val_acc: 0.7723\n",
      "Epoch 246/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.3599e-05 - acc: 1.0000 - val_loss: 2.9463 - val_acc: 0.7725\n",
      "Epoch 247/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.3522e-05 - acc: 1.0000 - val_loss: 2.9470 - val_acc: 0.7725\n",
      "Epoch 248/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.3475e-05 - acc: 1.0000 - val_loss: 2.9469 - val_acc: 0.7723\n",
      "Epoch 249/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.3392e-05 - acc: 1.0000 - val_loss: 2.9473 - val_acc: 0.7723\n",
      "Epoch 250/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.3330e-05 - acc: 1.0000 - val_loss: 2.9475 - val_acc: 0.7723\n",
      "Epoch 251/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.3256e-05 - acc: 1.0000 - val_loss: 2.9475 - val_acc: 0.7723\n",
      "Epoch 252/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.3184e-05 - acc: 1.0000 - val_loss: 2.9479 - val_acc: 0.7723\n",
      "Epoch 253/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.3128e-05 - acc: 1.0000 - val_loss: 2.9480 - val_acc: 0.7723\n",
      "Epoch 254/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.3048e-05 - acc: 1.0000 - val_loss: 2.9479 - val_acc: 0.7723\n",
      "Epoch 255/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 1.2994e-05 - acc: 1.0000 - val_loss: 2.9484 - val_acc: 0.7723\n",
      "Epoch 256/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 1.2929e-05 - acc: 1.0000 - val_loss: 2.9486 - val_acc: 0.7723\n",
      "Epoch 257/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.2863e-05 - acc: 1.0000 - val_loss: 2.9487 - val_acc: 0.7723\n",
      "Epoch 258/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 1.2797e-05 - acc: 1.0000 - val_loss: 2.9489 - val_acc: 0.7723\n",
      "Epoch 259/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 1.2743e-05 - acc: 1.0000 - val_loss: 2.9493 - val_acc: 0.7723\n",
      "Epoch 260/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 1.2670e-05 - acc: 1.0000 - val_loss: 2.9491 - val_acc: 0.7723\n",
      "Epoch 261/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.2617e-05 - acc: 1.0000 - val_loss: 2.9497 - val_acc: 0.7723\n",
      "Epoch 262/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.2551e-05 - acc: 1.0000 - val_loss: 2.9501 - val_acc: 0.7723\n",
      "Epoch 263/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.2496e-05 - acc: 1.0000 - val_loss: 2.9500 - val_acc: 0.7723\n",
      "Epoch 264/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.2440e-05 - acc: 1.0000 - val_loss: 2.9502 - val_acc: 0.7723\n",
      "Epoch 265/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.2372e-05 - acc: 1.0000 - val_loss: 2.9503 - val_acc: 0.7725\n",
      "Epoch 266/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.2312e-05 - acc: 1.0000 - val_loss: 2.9509 - val_acc: 0.7723\n",
      "Epoch 267/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.2256e-05 - acc: 1.0000 - val_loss: 2.9508 - val_acc: 0.7723\n",
      "Epoch 268/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.2201e-05 - acc: 1.0000 - val_loss: 2.9512 - val_acc: 0.7727\n",
      "Epoch 269/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.2138e-05 - acc: 1.0000 - val_loss: 2.9510 - val_acc: 0.7723\n",
      "Epoch 270/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.2091e-05 - acc: 1.0000 - val_loss: 2.9514 - val_acc: 0.7725\n",
      "Epoch 271/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.2024e-05 - acc: 1.0000 - val_loss: 2.9514 - val_acc: 0.7725\n",
      "Epoch 272/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.1975e-05 - acc: 1.0000 - val_loss: 2.9518 - val_acc: 0.7723\n",
      "Epoch 273/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.1915e-05 - acc: 1.0000 - val_loss: 2.9519 - val_acc: 0.7727\n",
      "Epoch 274/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.1870e-05 - acc: 1.0000 - val_loss: 2.9523 - val_acc: 0.7727\n",
      "Epoch 275/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.1815e-05 - acc: 1.0000 - val_loss: 2.9524 - val_acc: 0.7727\n",
      "Epoch 276/300\n",
      "54000/54000 [==============================] - 10s 191us/step - loss: 1.1762e-05 - acc: 1.0000 - val_loss: 2.9526 - val_acc: 0.7727\n",
      "Epoch 277/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 1.1712e-05 - acc: 1.0000 - val_loss: 2.9528 - val_acc: 0.7725\n",
      "Epoch 278/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.1655e-05 - acc: 1.0000 - val_loss: 2.9530 - val_acc: 0.7725\n",
      "Epoch 279/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.1601e-05 - acc: 1.0000 - val_loss: 2.9533 - val_acc: 0.7727\n",
      "Epoch 280/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.1554e-05 - acc: 1.0000 - val_loss: 2.9533 - val_acc: 0.7727\n",
      "Epoch 281/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.1503e-05 - acc: 1.0000 - val_loss: 2.9535 - val_acc: 0.7727\n",
      "Epoch 282/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.1451e-05 - acc: 1.0000 - val_loss: 2.9539 - val_acc: 0.7727\n",
      "Epoch 283/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.1400e-05 - acc: 1.0000 - val_loss: 2.9538 - val_acc: 0.7727\n",
      "Epoch 284/300\n",
      "54000/54000 [==============================] - 9s 175us/step - loss: 1.1351e-05 - acc: 1.0000 - val_loss: 2.9541 - val_acc: 0.7727\n",
      "Epoch 285/300\n",
      "54000/54000 [==============================] - 9s 172us/step - loss: 1.1304e-05 - acc: 1.0000 - val_loss: 2.9541 - val_acc: 0.7727\n",
      "Epoch 286/300\n",
      "54000/54000 [==============================] - 10s 176us/step - loss: 1.1255e-05 - acc: 1.0000 - val_loss: 2.9544 - val_acc: 0.7727\n",
      "Epoch 287/300\n",
      "54000/54000 [==============================] - 9s 176us/step - loss: 1.1201e-05 - acc: 1.0000 - val_loss: 2.9544 - val_acc: 0.7727\n",
      "Epoch 288/300\n",
      "54000/54000 [==============================] - 9s 173us/step - loss: 1.1156e-05 - acc: 1.0000 - val_loss: 2.9547 - val_acc: 0.7727\n",
      "Epoch 289/300\n",
      "54000/54000 [==============================] - 10s 176us/step - loss: 1.1107e-05 - acc: 1.0000 - val_loss: 2.9549 - val_acc: 0.7725\n",
      "Epoch 290/300\n",
      "54000/54000 [==============================] - 9s 176us/step - loss: 1.1064e-05 - acc: 1.0000 - val_loss: 2.9551 - val_acc: 0.7727\n",
      "Epoch 291/300\n",
      "54000/54000 [==============================] - 9s 175us/step - loss: 1.1014e-05 - acc: 1.0000 - val_loss: 2.9552 - val_acc: 0.7727\n",
      "Epoch 292/300\n",
      "54000/54000 [==============================] - 10s 178us/step - loss: 1.0968e-05 - acc: 1.0000 - val_loss: 2.9553 - val_acc: 0.7727\n",
      "Epoch 293/300\n",
      "54000/54000 [==============================] - 9s 173us/step - loss: 1.0927e-05 - acc: 1.0000 - val_loss: 2.9557 - val_acc: 0.7727\n",
      "Epoch 294/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.0877e-05 - acc: 1.0000 - val_loss: 2.9559 - val_acc: 0.7727\n",
      "Epoch 295/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.0835e-05 - acc: 1.0000 - val_loss: 2.9560 - val_acc: 0.7727\n",
      "Epoch 296/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.0784e-05 - acc: 1.0000 - val_loss: 2.9563 - val_acc: 0.7727\n",
      "Epoch 297/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 1.0744e-05 - acc: 1.0000 - val_loss: 2.9563 - val_acc: 0.7727\n",
      "Epoch 298/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.0700e-05 - acc: 1.0000 - val_loss: 2.9565 - val_acc: 0.7727\n",
      "Epoch 299/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.0657e-05 - acc: 1.0000 - val_loss: 2.9566 - val_acc: 0.7727\n",
      "Epoch 300/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.0612e-05 - acc: 1.0000 - val_loss: 2.9567 - val_acc: 0.7727\n",
      "Saved model to disk\n",
      "DEEPTH: 6, WIDTH: 2048, LCP: 0.4\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/300\n",
      "54000/54000 [==============================] - 11s 199us/step - loss: 1.8116 - acc: 0.5071 - val_loss: 1.6217 - val_acc: 0.5845\n",
      "Epoch 2/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.5936 - acc: 0.5897 - val_loss: 1.5589 - val_acc: 0.6032\n",
      "Epoch 3/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.5509 - acc: 0.6018 - val_loss: 1.5548 - val_acc: 0.6067\n",
      "Epoch 4/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 1.5237 - acc: 0.6090 - val_loss: 1.5368 - val_acc: 0.6153\n",
      "Epoch 5/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.5040 - acc: 0.6145 - val_loss: 1.5324 - val_acc: 0.6163\n",
      "Epoch 6/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 1.4862 - acc: 0.6185 - val_loss: 1.5282 - val_acc: 0.6150\n",
      "Epoch 7/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.4706 - acc: 0.6211 - val_loss: 1.5255 - val_acc: 0.6195\n",
      "Epoch 8/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.4555 - acc: 0.6247 - val_loss: 1.5270 - val_acc: 0.6172\n",
      "Epoch 9/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 1.4391 - acc: 0.6270 - val_loss: 1.5204 - val_acc: 0.6217\n",
      "Epoch 10/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 1.4222 - acc: 0.6287 - val_loss: 1.5458 - val_acc: 0.6162\n",
      "Epoch 11/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.4041 - acc: 0.6309 - val_loss: 1.5352 - val_acc: 0.6190\n",
      "Epoch 12/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.3826 - acc: 0.6333 - val_loss: 1.5607 - val_acc: 0.6152\n",
      "Epoch 13/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.3599 - acc: 0.6355 - val_loss: 1.5579 - val_acc: 0.6175\n",
      "Epoch 14/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 1.3348 - acc: 0.6388 - val_loss: 1.6076 - val_acc: 0.6012\n",
      "Epoch 15/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 1.3052 - acc: 0.6421 - val_loss: 1.6457 - val_acc: 0.5873\n",
      "Epoch 16/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.2715 - acc: 0.6480 - val_loss: 1.6202 - val_acc: 0.6060\n",
      "Epoch 17/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.2339 - acc: 0.6529 - val_loss: 1.6799 - val_acc: 0.5825\n",
      "Epoch 18/300\n",
      "54000/54000 [==============================] - 10s 180us/step - loss: 1.1898 - acc: 0.6615 - val_loss: 1.7152 - val_acc: 0.5800\n",
      "Epoch 19/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 1.1414 - acc: 0.6717 - val_loss: 1.7703 - val_acc: 0.5792\n",
      "Epoch 20/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 1.0913 - acc: 0.6810 - val_loss: 1.8770 - val_acc: 0.5428\n",
      "Epoch 21/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 1.0358 - acc: 0.6932 - val_loss: 1.8439 - val_acc: 0.5470\n",
      "Epoch 22/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 0.9760 - acc: 0.7077 - val_loss: 1.9329 - val_acc: 0.5367\n",
      "Epoch 23/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.9104 - acc: 0.7258 - val_loss: 2.1290 - val_acc: 0.5143\n",
      "Epoch 24/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.8481 - acc: 0.7410 - val_loss: 2.2599 - val_acc: 0.4775\n",
      "Epoch 25/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 0.7823 - acc: 0.7589 - val_loss: 2.1428 - val_acc: 0.5123\n",
      "Epoch 26/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.7235 - acc: 0.7730 - val_loss: 2.5013 - val_acc: 0.5178\n",
      "Epoch 27/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.6604 - acc: 0.7931 - val_loss: 2.4323 - val_acc: 0.5130\n",
      "Epoch 28/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.6023 - acc: 0.8099 - val_loss: 2.5235 - val_acc: 0.5028\n",
      "Epoch 29/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.5504 - acc: 0.8241 - val_loss: 2.5956 - val_acc: 0.5197\n",
      "Epoch 30/300\n",
      "54000/54000 [==============================] - 10s 181us/step - loss: 0.4968 - acc: 0.8411 - val_loss: 2.7579 - val_acc: 0.4893\n",
      "Epoch 31/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.4587 - acc: 0.8530 - val_loss: 2.9506 - val_acc: 0.5215\n",
      "Epoch 32/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.4159 - acc: 0.8667 - val_loss: 3.0864 - val_acc: 0.4713\n",
      "Epoch 33/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.3791 - acc: 0.8791 - val_loss: 3.3060 - val_acc: 0.4808\n",
      "Epoch 34/300\n",
      "54000/54000 [==============================] - 10s 182us/step - loss: 0.3503 - acc: 0.8878 - val_loss: 3.1993 - val_acc: 0.5035\n",
      "Epoch 35/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 0.3356 - acc: 0.8915 - val_loss: 3.3421 - val_acc: 0.4838\n",
      "Epoch 36/300\n",
      "54000/54000 [==============================] - 10s 185us/step - loss: 0.2824 - acc: 0.9100 - val_loss: 3.3398 - val_acc: 0.5053\n",
      "Epoch 37/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.2725 - acc: 0.9117 - val_loss: 3.5709 - val_acc: 0.4793\n",
      "Epoch 38/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 0.2405 - acc: 0.9222 - val_loss: 3.6163 - val_acc: 0.5023\n",
      "Epoch 39/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.2339 - acc: 0.9237 - val_loss: 3.6269 - val_acc: 0.4568\n",
      "Epoch 40/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.2158 - acc: 0.9307 - val_loss: 3.9390 - val_acc: 0.4923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.2025 - acc: 0.9344 - val_loss: 3.7615 - val_acc: 0.5200\n",
      "Epoch 42/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.1885 - acc: 0.9387 - val_loss: 4.0643 - val_acc: 0.4518\n",
      "Epoch 43/300\n",
      "54000/54000 [==============================] - 10s 187us/step - loss: 0.1839 - acc: 0.9410 - val_loss: 4.4804 - val_acc: 0.4180\n",
      "Epoch 44/300\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.1803 - acc: 0.9410 - val_loss: 4.0949 - val_acc: 0.4322\n",
      "Epoch 45/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.1710 - acc: 0.9435 - val_loss: 3.9947 - val_acc: 0.5123\n",
      "Epoch 46/300\n",
      "54000/54000 [==============================] - 10s 186us/step - loss: 0.1602 - acc: 0.9466 - val_loss: 4.1065 - val_acc: 0.4950\n",
      "Epoch 47/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.1244 - acc: 0.9592 - val_loss: 4.1448 - val_acc: 0.4813\n",
      "Epoch 48/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.1423 - acc: 0.9523 - val_loss: 4.2338 - val_acc: 0.4998\n",
      "Epoch 49/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.1132 - acc: 0.9625 - val_loss: 4.7071 - val_acc: 0.4633\n",
      "Epoch 50/300\n",
      "54000/54000 [==============================] - 10s 188us/step - loss: 0.1223 - acc: 0.9589 - val_loss: 4.4885 - val_acc: 0.4950\n",
      "Epoch 51/300\n",
      "54000/54000 [==============================] - 10s 190us/step - loss: 0.1149 - acc: 0.9611 - val_loss: 4.4134 - val_acc: 0.4878\n",
      "Epoch 52/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.1138 - acc: 0.9615 - val_loss: 4.2954 - val_acc: 0.4903\n",
      "Epoch 53/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.1054 - acc: 0.9644 - val_loss: 4.3878 - val_acc: 0.5073\n",
      "Epoch 54/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.0900 - acc: 0.9697 - val_loss: 4.5550 - val_acc: 0.5035\n",
      "Epoch 55/300\n",
      "54000/54000 [==============================] - 10s 184us/step - loss: 0.0767 - acc: 0.9749 - val_loss: 4.5456 - val_acc: 0.4958\n",
      "Epoch 56/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 0.0658 - acc: 0.9780 - val_loss: 4.6869 - val_acc: 0.4852\n",
      "Epoch 57/300\n",
      "54000/54000 [==============================] - 10s 189us/step - loss: 0.0815 - acc: 0.9723 - val_loss: 4.5278 - val_acc: 0.4973\n",
      "Epoch 58/300\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 0.0767 - acc: 0.9744 - val_loss: 4.5639 - val_acc: 0.4767\n",
      "Epoch 59/300\n",
      "54000/54000 [==============================] - 10s 179us/step - loss: 0.0793 - acc: 0.9735 - val_loss: 4.5700 - val_acc: 0.4997\n",
      "Epoch 60/300\n",
      "54000/54000 [==============================] - 10s 183us/step - loss: 0.0924 - acc: 0.9686 - val_loss: 5.2809 - val_acc: 0.4322\n",
      "Epoch 61/300\n",
      "52608/54000 [============================>.] - ETA: 0s - loss: 0.0956 - acc: 0.9676"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f6522547fcce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTRandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_corrupt_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_{}d_{}w_{}lcp'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mnist_exps_new_corruption'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-56fed6b3cea1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mara/venv/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mnist = MNISTRandom()\n",
    "depths = [6]\n",
    "widths = [4,16,64,256, 1024, 2048, 4096]\n",
    "#widths = [4096]\n",
    "#label_corrupt_pbs = [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "label_corrupt_pbs = [ 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "#gaussian_noise_fs = [0., 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "try:\n",
    "    del mlp\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for d in depths:\n",
    "    for w in widths:\n",
    "        for lcp in label_corrupt_pbs[:]:\n",
    "            try:\n",
    "                del mlp\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                del mnist\n",
    "            except:\n",
    "                pass\n",
    "            print 'DEEPTH: {}, WIDTH: {}, LCP: {}'.format(d,w,lcp)\n",
    "            mnist = MNISTRandom(label_corrupt_p = lcp)\n",
    "            mlp = MLP(deep=d, wide=w,epochs=300)\n",
    "            mlp.train(mnist)\n",
    "            mlp.save('mnist_{}d_{}w_{}lcp'.format(d,w,lcp), 'mnist_exps_new_corruption')\n",
    "\n",
    "for d in depths:\n",
    "    for w in widths:\n",
    "        for gnf in gaussian_noise_fs:\n",
    "            try:\n",
    "                del mlp\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                del mnist\n",
    "            except:\n",
    "                pass\n",
    "            print 'DEEPTH: {}, WIDTH: {}, GNF: {}'.format(d,w,gnf)\n",
    "            mnist = MNISTRandom(gaussian_noise_f=gnf)\n",
    "            mlp = MLP(deep=d, wide=w,epochs=300)\n",
    "            mlp.train(mnist)\n",
    "            mlp.save('mnist_{}d_{}w_{}gnf'.format(d,w,gnf), 'mnist_exps_gn_new_corruption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = memorization_utils.MNISTRandom()\n",
    "depths = [6]\n",
    "#depths = [6,]\n",
    "#widths = [4,16,64,256, 1024, 2048]\n",
    "widths = [2048]\n",
    "\n",
    "#label_corrupt_pbs = [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "label_corrupt_pbs = [ 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "gaussian_noise_fs = [0., 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "try:\n",
    "    del mlp\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for d in depths:\n",
    "    for w in widths:\n",
    "        for lcp in label_corrupt_pbs[:]:\n",
    "            try:\n",
    "                del mlp\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                del mnist\n",
    "            except:\n",
    "                pass\n",
    "            print 'DEEPTH: {}, WIDTH: {}, LCP: {}'.format(d,w,lcp)\n",
    "            mnist = memorization_utils.MNISTRandom(label_corrupt_p = lcp)\n",
    "            mlp = memorization_utils.MLP(deep=d, wide=w,epochs=300)\n",
    "            mlp.train(mnist)\n",
    "            mlp.save('mnist_{}d_{}w_{}lcp'.format(d,w,lcp), 'mnist_exps_new_corruption')\n",
    "\n",
    "for d in depths:\n",
    "    for w in widths:\n",
    "        for gnf in gaussian_noise_fs:\n",
    "            try:\n",
    "                del mlp\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                del mnist\n",
    "            except:\n",
    "                pass\n",
    "            print 'DEEPTH: {}, WIDTH: {}, GNF: {}'.format(d,w,gnf)\n",
    "            mnist = memorization_utils.MNISTRandom(gaussian_noise_f=gnf)\n",
    "            mlp = memorization_utils.MLP(deep=d, wide=w,epochs=300)\n",
    "            mlp.train(mnist)\n",
    "            mlp.save('mnist_{}d_{}w_{}gnf'.format(d,w,gnf), 'mnist_exps_gn_new_corruption')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
